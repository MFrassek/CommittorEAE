{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "#import IPython.display as display\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6899 images found.\n",
      "Existing classes: ['7' '5' '0' '2' '4' '3' '1' '6']\n"
     ]
    }
   ],
   "source": [
    "# a fancy way of setting AUTOTUNE to -1 so the maximum number of threads are run later\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "FOLDER_NAME = \"natural_images\"\n",
    "# set data_dir to be the path to the selected folder\n",
    "data_dir = pathlib.Path(str(FOLDER_NAME))\n",
    "# counts all JPG files in all subfolders of our selected folder\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "print(str(image_count) + \" images found.\")\n",
    "# makes a list of the subfolder names in our selected folder, which corresponds to the classes \n",
    "CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != \"LICENSE.txt\"])\n",
    "print(\"Existing classes: \" + str(CLASS_NAMES))\n",
    "#\n",
    "BATCH_SIZE = 1000\n",
    "# pick a size to which the images should be rescaled\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "# BUFFER_SIZE will be used for shuffling the dataset later on. \n",
    "# By selecting image_count as the size, the complete data set is guaranteed to be shuffled.\n",
    "BUFFER_SIZE = image_count\n",
    "# Sets a split size for train and test data set\n",
    "TRAIN_SIZE = int(image_count * 0.7)\n",
    "# choose whether the labels shoud be one hot encoded. This appears to be beneficial if labels aren't numeric.\n",
    "ONE_HOT = False\n",
    "# produces a time stamp to use for file naming\n",
    "TIME_STAMP = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffles the dataset an splits it into train and test test\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*')).shuffle(BUFFER_SIZE)\n",
    "train_list_ds = list_ds.take(TRAIN_SIZE)\n",
    "test_list_ds = list_ds.skip(TRAIN_SIZE)\n",
    "\n",
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    if ONE_HOT == True:\n",
    "        # return the class (second to last element in the path) in one hot encoding\n",
    "        return parts[-2] == CLASS_NAMES\n",
    "    else:\n",
    "        # return the class (second to last element in the path) as an integer (if the folder is named as an integer)\n",
    "        return int(parts[-2])\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel by different cores. \n",
    "# -1 uses all cores.\n",
    "train_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "test_labeled_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=BUFFER_SIZE):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "    else:\n",
    "        # untested! If dataset is to large for memory, a time stamped cache file is produced to take data from\n",
    "        # if the code is run again with the same time stamp, the file will be reused. For new time stamps\n",
    "        # a new file will be generated.\n",
    "        ds = ds.cache(str(TIME_STAMP) + \"_cache.txt\")\n",
    "        \n",
    "    # shuffles the dataset again\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 128, 128, 3), (None,)), types: (tf.float32, tf.int32)>\n",
      "tf.Tensor(\n",
      "[[[[0.6035366  0.49833763 0.3528506 ]\n",
      "   [0.62735516 0.5238039  0.41517815]\n",
      "   [0.6532702  0.56897175 0.5106392 ]\n",
      "   ...\n",
      "   [1.         1.         0.9921569 ]\n",
      "   [1.         1.         0.9921569 ]\n",
      "   [1.         1.         0.9921569 ]]\n",
      "\n",
      "  [[0.60665095 0.5142503  0.37939647]\n",
      "   [0.59417456 0.509719   0.4170492 ]\n",
      "   [0.6551842  0.59138864 0.5357974 ]\n",
      "   ...\n",
      "   [0.9937719  0.9937719  0.9937719 ]\n",
      "   [1.         1.         0.99286157]\n",
      "   [0.9937181  0.9937181  0.98587495]]\n",
      "\n",
      "  [[0.5944386  0.5130355  0.39194173]\n",
      "   [0.595214   0.52351624 0.43204662]\n",
      "   [0.6039324  0.5474161  0.5003302 ]\n",
      "   ...\n",
      "   [0.99758786 0.99758786 0.99758786]\n",
      "   [0.9775605  0.9775605  0.9758317 ]\n",
      "   [0.9779806  0.9779806  0.97608113]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.52022934 0.4653274  0.3516019 ]\n",
      "   [0.51702344 0.4621215  0.34839597]\n",
      "   [0.530398   0.4722945  0.3521658 ]\n",
      "   ...\n",
      "   [0.9723193  0.9792127  0.97040087]\n",
      "   [0.9755209  0.9824143  0.96875006]\n",
      "   [0.9809897  0.9916747  0.96517354]]\n",
      "\n",
      "  [[0.5307585  0.47585657 0.36213106]\n",
      "   [0.5325521  0.47765017 0.36392465]\n",
      "   [0.5384125  0.480309   0.36018032]\n",
      "   ...\n",
      "   [0.9649598  0.96888137 0.9455664 ]\n",
      "   [0.9685089  0.97243047 0.9491155 ]\n",
      "   [0.97268564 0.9770361  0.9456049 ]]\n",
      "\n",
      "  [[0.53658557 0.4816836  0.3679581 ]\n",
      "   [0.5474764  0.49257445 0.37884897]\n",
      "   [0.5333093  0.47520575 0.35507706]\n",
      "   ...\n",
      "   [0.96470594 0.96724886 0.91561043]\n",
      "   [0.96596074 0.9661892  0.9260597 ]\n",
      "   [0.9686275  0.9670084  0.9386456 ]]]\n",
      "\n",
      "\n",
      " [[[0.4502992  0.5361601  0.2158323 ]\n",
      "   [0.51572645 0.60592246 0.2927075 ]\n",
      "   [0.64904594 0.7392421  0.41824856]\n",
      "   ...\n",
      "   [0.7905476  0.73985475 0.55320424]\n",
      "   [0.87978953 0.835248   0.6960935 ]\n",
      "   [0.90975285 0.8800296  0.779936  ]]\n",
      "\n",
      "  [[0.62162596 0.7155342  0.32123274]\n",
      "   [0.570068   0.6623936  0.30275807]\n",
      "   [0.47988296 0.5740466  0.21272609]\n",
      "   ...\n",
      "   [0.91043395 0.8599291  0.7688494 ]\n",
      "   [0.98492664 0.9542586  0.87383556]\n",
      "   [0.95643365 0.9385721  0.8640623 ]]\n",
      "\n",
      "  [[0.5890068  0.68115526 0.22777818]\n",
      "   [0.556992   0.6421225  0.25054777]\n",
      "   [0.5171928  0.60227245 0.22154327]\n",
      "   ...\n",
      "   [0.93185884 0.9096755  0.7988105 ]\n",
      "   [0.8790882  0.8689577  0.7363492 ]\n",
      "   [0.89609814 0.90488756 0.7428248 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.58948857 0.7073501  0.44645855]\n",
      "   [0.579859   0.6882941  0.42554897]\n",
      "   [0.4445507  0.5402002  0.2735335 ]\n",
      "   ...\n",
      "   [0.43398297 0.45600912 0.29173216]\n",
      "   [0.5115938  0.543671   0.35345963]\n",
      "   [0.3915384  0.43395916 0.218683  ]]\n",
      "\n",
      "  [[0.3992586  0.5220681  0.24383582]\n",
      "   [0.5377462  0.65539324 0.3763494 ]\n",
      "   [0.565812   0.66777277 0.38858527]\n",
      "   ...\n",
      "   [0.9530259  0.94613796 0.834695  ]\n",
      "   [0.94279975 0.94563377 0.80238956]\n",
      "   [0.7283449  0.74589    0.5605549 ]]\n",
      "\n",
      "  [[0.45278275 0.57871884 0.28396463]\n",
      "   [0.47279966 0.5922032  0.29849413]\n",
      "   [0.4287349  0.5318905  0.23810989]\n",
      "   ...\n",
      "   [0.881124   0.8648626  0.76711094]\n",
      "   [0.85809845 0.84617543 0.715242  ]\n",
      "   [0.623829   0.6315326  0.45692572]]]\n",
      "\n",
      "\n",
      " [[[0.32009807 0.48288912 0.12602635]\n",
      "   [0.37319243 0.52526045 0.19401044]\n",
      "   [0.4117341  0.55082726 0.23034622]\n",
      "   ...\n",
      "   [0.336443   0.600697   0.3214001 ]\n",
      "   [0.35615045 0.6345818  0.26446846]\n",
      "   [0.38183978 0.6541437  0.31852025]]\n",
      "\n",
      "  [[0.431633   0.54770994 0.31223196]\n",
      "   [0.44413298 0.6667816  0.2689185 ]\n",
      "   [0.4256051  0.59220284 0.30076593]\n",
      "   ...\n",
      "   [0.37100953 0.6828125  0.22319241]\n",
      "   [0.27034315 0.5544884  0.21878831]\n",
      "   [0.3163067  0.6155714  0.20824909]]\n",
      "\n",
      "  [[0.54576445 0.6775506  0.4186045 ]\n",
      "   [0.52087164 0.7073836  0.3943934 ]\n",
      "   [0.38067558 0.554182   0.19678311]\n",
      "   ...\n",
      "   [0.36882663 0.64041823 0.25509346]\n",
      "   [0.2964231  0.5750996  0.24519762]\n",
      "   [0.2656327  0.5784314  0.17321539]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.23478097 0.3955653  0.27081037]\n",
      "   [0.21386337 0.3862056  0.26599267]\n",
      "   [0.23267464 0.40914527 0.27973348]\n",
      "   ...\n",
      "   [0.13338695 0.31247702 0.05824143]\n",
      "   [0.1302543  0.28356314 0.06003371]\n",
      "   [0.06975338 0.23029259 0.04319853]]\n",
      "\n",
      "  [[0.21431528 0.37879905 0.24423255]\n",
      "   [0.20186123 0.3786765  0.2541667 ]\n",
      "   [0.20045957 0.3847733  0.25143996]\n",
      "   ...\n",
      "   [0.1711244  0.35028344 0.12022059]\n",
      "   [0.16815259 0.35356924 0.14464615]\n",
      "   [0.15917587 0.35746017 0.16345742]]\n",
      "\n",
      "  [[0.18239892 0.33864126 0.20379902]\n",
      "   [0.19527422 0.37149206 0.24453126]\n",
      "   [0.20899205 0.3925705  0.26413912]\n",
      "   ...\n",
      "   [0.16043964 0.36754748 0.14764094]\n",
      "   [0.15981159 0.35598192 0.16176471]\n",
      "   [0.1569853  0.34840685 0.16789217]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.33823532 0.3264706  0.35980392]\n",
      "   [0.33653492 0.32817096 0.35725337]\n",
      "   [0.3314798  0.32755822 0.35037917]\n",
      "   ...\n",
      "   [0.2492494  0.2492494  0.28062195]\n",
      "   [0.22996709 0.22996709 0.26133963]\n",
      "   [0.2270374  0.2270374  0.25840992]]\n",
      "\n",
      "  [[0.32983685 0.32395452 0.34552312]\n",
      "   [0.33320314 0.32817096 0.3493145 ]\n",
      "   [0.3314798  0.3296837  0.34318706]\n",
      "   ...\n",
      "   [0.2484337  0.2454925  0.27686507]\n",
      "   [0.2286918  0.22575063 0.25712317]\n",
      "   [0.2270374  0.22409622 0.25546876]]\n",
      "\n",
      "  [[0.34832263 0.33979398 0.3392272 ]\n",
      "   [0.3555262  0.35072    0.3484988 ]\n",
      "   [0.3627451  0.3529412  0.3529412 ]\n",
      "   ...\n",
      "   [0.23961015 0.2337278  0.26411998]\n",
      "   [0.22708717 0.22120482 0.251597  ]\n",
      "   [0.23347504 0.22759269 0.25798485]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.7095895  0.7932177  0.9088925 ]\n",
      "   [0.7240082  0.8249618  0.9283893 ]\n",
      "   [0.5899127  0.713557   0.8036957 ]\n",
      "   ...\n",
      "   [0.40281865 0.4526042  0.57107466]\n",
      "   [0.5565411  0.6001034  0.69573   ]\n",
      "   [0.7063458  0.73941875 0.8156289 ]]\n",
      "\n",
      "  [[0.61500466 0.6503217  0.764166  ]\n",
      "   [0.804205   0.86444163 0.92954963]\n",
      "   [0.84983534 0.94429004 0.9642272 ]\n",
      "   ...\n",
      "   [0.32003677 0.36884195 0.49564955]\n",
      "   [0.33970207 0.38283932 0.47931606]\n",
      "   [0.46349573 0.49992344 0.5677467 ]]\n",
      "\n",
      "  [[0.52695316 0.5219439  0.6265395 ]\n",
      "   [0.56349957 0.59226793 0.67111677]\n",
      "   [0.6078661  0.6758502  0.72796804]\n",
      "   ...\n",
      "   [0.330193   0.3832491  0.50793123]\n",
      "   [0.30901504 0.3554841  0.4469631 ]\n",
      "   [0.346021   0.3832989  0.44481468]]]\n",
      "\n",
      "\n",
      " [[[0.8256685  0.8407281  0.8539941 ]\n",
      "   [0.707981   0.69137836 0.7221688 ]\n",
      "   [0.60761464 0.5675758  0.5299492 ]\n",
      "   ...\n",
      "   [0.75490767 0.7274566  0.6957471 ]\n",
      "   [0.6765806  0.63811415 0.607495  ]\n",
      "   [0.6285247  0.57949394 0.5794382 ]]\n",
      "\n",
      "  [[0.7636533  0.77119416 0.7884262 ]\n",
      "   [0.60101813 0.608364   0.6474224 ]\n",
      "   [0.57758045 0.5459712  0.53150046]\n",
      "   ...\n",
      "   [0.75906557 0.7316146  0.699905  ]\n",
      "   [0.6021653  0.5661276  0.5378842 ]\n",
      "   [0.527007   0.4849921  0.49647555]]\n",
      "\n",
      "  [[0.73078483 0.7244611  0.7512074 ]\n",
      "   [0.511469   0.5438526  0.5789307 ]\n",
      "   [0.39315432 0.38324317 0.39249688]\n",
      "   ...\n",
      "   [0.7254927  0.6901986  0.6596251 ]\n",
      "   [0.62931234 0.60019165 0.5800001 ]\n",
      "   [0.5676149  0.5323208  0.5504274 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.39705884 0.3314032  0.27204072]\n",
      "   [0.40983903 0.33925077 0.29219195]\n",
      "   [0.43093437 0.35642454 0.2900949 ]\n",
      "   ...\n",
      "   [0.3109566  0.22759263 0.18736592]\n",
      "   [0.3359797  0.26026815 0.22233081]\n",
      "   [0.30116424 0.23449756 0.20312501]]\n",
      "\n",
      "  [[0.31175232 0.23332097 0.193097  ]\n",
      "   [0.41201517 0.33358377 0.29044652]\n",
      "   [0.3898622  0.31484687 0.25622937]\n",
      "   ...\n",
      "   [0.28031558 0.20188421 0.17443322]\n",
      "   [0.28532478 0.20980807 0.18365502]\n",
      "   [0.24252453 0.18394609 0.16246524]]\n",
      "\n",
      "  [[0.389458   0.29741392 0.26562774]\n",
      "   [0.36361825 0.2735893  0.22707923]\n",
      "   [0.37651658 0.289951   0.23504902]\n",
      "   ...\n",
      "   [0.29048306 0.21597327 0.19244386]\n",
      "   [0.25544086 0.1826008  0.16241086]\n",
      "   [0.223833   0.16893102 0.15774842]]]\n",
      "\n",
      "\n",
      " [[[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   ...\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.9693181  0.9693181  0.96147496]\n",
      "   [0.9586551  0.9633426  0.9531557 ]\n",
      "   [0.9617585  0.96960163 0.9578369 ]\n",
      "   ...\n",
      "   [0.9912594  0.9967678  0.99516565]\n",
      "   [0.9935087  0.9935087  0.9935087 ]\n",
      "   [0.99447006 0.99447006 0.99447006]]\n",
      "\n",
      "  [[0.9357047  0.9357047  0.9278616 ]\n",
      "   [0.9200981  0.9247856  0.9145987 ]\n",
      "   [0.92069906 0.9285422  0.9167775 ]\n",
      "   ...\n",
      "   [0.97854996 0.98636246 0.9824562 ]\n",
      "   [0.98730624 0.98730624 0.98730624]\n",
      "   [0.9917724  0.9917724  0.9917724 ]]\n",
      "\n",
      "  [[0.9333487  0.9333487  0.9255056 ]\n",
      "   [0.90827215 0.91295964 0.9027727 ]\n",
      "   [0.89414835 0.9019915  0.8902268 ]\n",
      "   ...\n",
      "   [0.96084565 0.96865815 0.9647519 ]\n",
      "   [0.9796263  0.9796263  0.9796263 ]\n",
      "   [0.99059445 0.99059445 0.99059445]]]], shape=(1000, 128, 128, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_ds = prepare_for_training(train_labeled_ds)\n",
    "test_ds = prepare_for_training(test_labeled_ds)\n",
    "print(train_ds)\n",
    "# obtain the image and label batches for both test and training\n",
    "train_image_batch, train_label_batch = next(iter(train_ds))\n",
    "test_image_batch, test_label_batch = next(iter(test_ds))\n",
    "print(train_image_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 10s 10ms/sample - loss: 1.6951 - sparse_categorical_accuracy: 0.3720\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 9s 9ms/sample - loss: 0.9268 - sparse_categorical_accuracy: 0.6670\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 10s 10ms/sample - loss: 0.5629 - sparse_categorical_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 10s 10ms/sample - loss: 0.4456 - sparse_categorical_accuracy: 0.8390\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 10s 10ms/sample - loss: 0.2559 - sparse_categorical_accuracy: 0.9080\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 10s 10ms/sample - loss: 0.1837 - sparse_categorical_accuracy: 0.9370\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 12s 12ms/sample - loss: 0.1880 - sparse_categorical_accuracy: 0.9350\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 13s 13ms/sample - loss: 0.2139 - sparse_categorical_accuracy: 0.9180\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 12s 12ms/sample - loss: 0.0869 - sparse_categorical_accuracy: 0.9690\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 11s 11ms/sample - loss: 0.0454 - sparse_categorical_accuracy: 0.9880\n",
      "\n",
      "1000/1000 - 3s - loss: 0.4722 - sparse_categorical_accuracy: 0.8890\n",
      "\n",
      "Test accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "# build a sequential model\n",
    "model = tf.keras.Sequential([\n",
    "    # use first layer to flatten the image and take all inputs\n",
    "    tf.keras.layers.Flatten(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    # add an arbitrary number of dense layers with an arbitrary number of nodes\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # add an output layer with as many nodes as existing lables\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "])\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(8, activation='sigmoid')\n",
    "])\n",
    "\n",
    "#OPTIMIZER = \"RMSprop\"\n",
    "OPTIMIZER = \"Adam\"\n",
    "#OPTIMIZER = \"SGD\"\n",
    "if ONE_HOT == True:\n",
    "    if OPTIMIZER == \"RMSprop\":\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    elif OPTIMIZER == \"Adam\":\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    elif OPTIMIZER == \"SGD\":\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.CategoricalAccuracy()])        \n",
    "else:\n",
    "    if OPTIMIZER == \"RMSprop\":\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.SpareCategoricalAccuracy()])\n",
    "    elif OPTIMIZER == \"Adam\":\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    elif OPTIMIZER == \"SGD\":\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(train_image_batch, train_label_batch, epochs=10)\n",
    "print()\n",
    "test_loss, test_acc = model.evaluate(test_image_batch,  test_label_batch, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
