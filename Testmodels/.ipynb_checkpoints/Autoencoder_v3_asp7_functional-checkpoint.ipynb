{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = [\"phi0\", \"phi1\", \"phi2\", \"phi3\", \n",
    "                \"phi4\", \"phi5\", \"psi0\", \"psi1\", \n",
    "                \"psi2\", \"psi3\", \"psi4\", \"psi5\", \"cluster_id\"]\n",
    "LABEL_NAME = \"cluster_id\"\n",
    "INPUT_NAMES = list(COLUMN_NAMES)\n",
    "INPUT_NAMES.remove(LABEL_NAME)\n",
    "CSV_PATH = \"asp7/asp7.csv\"\n",
    "BATCH_SIZE = 10\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "CORES_USED = 3\n",
    "with open(CSV_PATH) as f:\n",
    "    ROW_COUNT = sum(1 for line in f) - 1\n",
    "f.close()\n",
    "# Sets a split size for train and test data set\n",
    "TRAIN_SIZE = int(ROW_COUNT * 0.7)\n",
    "\n",
    "original_ds = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = CSV_PATH,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    column_names=COLUMN_NAMES,\n",
    "    column_defaults=None,\n",
    "    label_name=LABEL_NAME,\n",
    "    select_columns=None,\n",
    "    field_delim=',',\n",
    "    use_quote_delim=True,\n",
    "    na_value='',\n",
    "    header=True,\n",
    "    num_epochs=None,\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    shuffle_seed=None,\n",
    "    prefetch_buffer_size=None,\n",
    "    num_parallel_reads=CORES_USED,\n",
    "    sloppy=False,\n",
    "    num_rows_for_inference=100,\n",
    "    compression_type=None,\n",
    "    ignore_errors=False).shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "train_ds = original_ds.take(TRAIN_SIZE)\n",
    "test_ds = original_ds.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch_wo_label(dataset):\n",
    "    for batch in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "def show_batch_w_label(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNumericFeatures(object):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "\n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        features['numeric'] = numeric_features\n",
    "        #returns the features is two forms acked together which can be used as input and controll output later on\n",
    "        return features, features['numeric']\n",
    "\n",
    "packed_train_ds = train_ds.map(PackNumericFeatures(INPUT_NAMES))\n",
    "packed_test_ds = test_ds.map(PackNumericFeatures(INPUT_NAMES))\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', shape=[len(INPUT_NAMES)])\n",
    "numeric_columns = [numeric_column]\n",
    "\n",
    "example_batch, labels_batch = next(iter(packed_train_ds))    \n",
    "#print(example_batch)\n",
    "#print(labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7000 steps\n",
      "7000/7000 [==============================] - 10s 1ms/step - loss: 1.7591 - mean_squared_error: 1.7591\n",
      "10/10 [==============================] - 3s 276ms/step - loss: 1.6099 - mean_squared_error: 1.6099\n",
      "\n",
      "Test accuracy: 1.6098665\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "model = tf.keras.Sequential([\n",
    "    numeric_layer,\n",
    "    Dense(len(INPUT_NAMES)),\n",
    "    Dense(1, activation='tanh'),\n",
    "    Dense(len(INPUT_NAMES))\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                    loss=tf.keras.losses.MeanSquaredError(),\n",
    "                    metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "model.fit(packed_train_ds, epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(packed_test_ds, verbose=1, steps = 10)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
