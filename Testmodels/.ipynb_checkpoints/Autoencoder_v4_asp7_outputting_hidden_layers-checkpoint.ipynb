{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = [\"phi0\", \"phi1\", \"phi2\", \"phi3\", \n",
    "                \"phi4\", \"phi5\", \"psi0\", \"psi1\", \n",
    "                \"psi2\", \"psi3\", \"psi4\", \"psi5\", \"cluster_id\"]\n",
    "LABEL_NAME = \"cluster_id\"\n",
    "INPUT_NAMES = list(COLUMN_NAMES)\n",
    "INPUT_NAMES.remove(LABEL_NAME)\n",
    "CSV_PATH = \"asp7/asp7_2.csv\"\n",
    "BATCH_SIZE = 1\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "CORES_USED = 3\n",
    "with open(CSV_PATH) as f:\n",
    "    ROW_COUNT = sum(1 for line in f) - 1\n",
    "f.close()\n",
    "# Sets a split size for train and test data set\n",
    "TRAIN_SIZE = int(ROW_COUNT * 0.7)\n",
    "\n",
    "original_ds = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = CSV_PATH,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    column_names=COLUMN_NAMES,\n",
    "    column_defaults=None,\n",
    "    label_name=LABEL_NAME,\n",
    "    select_columns=None,\n",
    "    field_delim=',',\n",
    "    use_quote_delim=True,\n",
    "    na_value='',\n",
    "    header=True,\n",
    "    num_epochs=None,\n",
    "    shuffle=True,\n",
    "    shuffle_buffer_size=SHUFFLE_BUFFER_SIZE,\n",
    "    shuffle_seed=None,\n",
    "    prefetch_buffer_size=None,\n",
    "    num_parallel_reads=CORES_USED,\n",
    "    sloppy=False,\n",
    "    num_rows_for_inference=100,\n",
    "    compression_type=None,\n",
    "    ignore_errors=False).shuffle(SHUFFLE_BUFFER_SIZE)\n",
    "\n",
    "train_ds = original_ds.take(TRAIN_SIZE)\n",
    "test_ds = original_ds.skip(TRAIN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch_wo_label(dataset):\n",
    "    for batch in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "def show_batch_w_label(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNumericFeatures(object):\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "\n",
    "    def __call__(self, features, labels):\n",
    "        numeric_features = [features.pop(name) for name in self.names]\n",
    "        numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "        numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "        #returns the features twice packed together which can be used as input and control output later on\n",
    "        return numeric_features, numeric_features\n",
    "\n",
    "packed_train_ds = train_ds.map(PackNumericFeatures(INPUT_NAMES))\n",
    "packed_test_ds = test_ds.map(PackNumericFeatures(INPUT_NAMES))\n",
    "#numeric_column = tf.feature_column.numeric_column('numeric', shape=[len(INPUT_NAMES)])\n",
    "#numeric_columns = [numeric_column]\n",
    "#print(numeric_column)\n",
    "#print(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7000 steps\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.2631 - mean_squared_error: 0.2631\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.0187 - mean_squared_error: 0.0187\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 7s 1ms/step - loss: 0.0087 - mean_squared_error: 0.0087\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 7s 1ms/step - loss: 0.0071 - mean_squared_error: 0.0071\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 7s 1ms/step - loss: 0.0062 - mean_squared_error: 0.0062\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.0053 - mean_squared_error: 0.0053\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.0050 - mean_squared_error: 0.0050\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 8s 1ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff27c23be50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(len(INPUT_NAMES),input_shape=(len(INPUT_NAMES),)),\n",
    "    tf.keras.layers.Dense(24, activation='tanh'),\n",
    "    tf.keras.layers.Dense(len(INPUT_NAMES))])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                    loss=tf.keras.losses.MeanSquaredError(),\n",
    "                    metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "tf.print\n",
    "\n",
    "model.fit(packed_train_ds, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                312       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                300       \n",
      "=================================================================\n",
      "Total params: 768\n",
      "Trainable params: 768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.61  -1.25  -1.061 -1.292  2.279 -1.778  1.74  -1.115 -0.754 -0.228\n",
      "  -0.877  1.213]]\n",
      "tf.Tensor(\n",
      "[[-1.475  1.252 -2.295 -0.802 -1.9   -1.878  1.897  2.299  2.008  2.938\n",
      "   2.485  0.422]], shape=(1, 12), dtype=float32)\n",
      "(<tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.87 , -0.378, -1.344, -2.541, -1.215, -2.111, -0.134,  1.092,\n",
      "         2.98 ,  2.217, -2.496,  2.545]], dtype=float32)>, <tf.Tensor: shape=(1, 12), dtype=float32, numpy=\n",
      "array([[ 1.87 , -0.378, -1.344, -2.541, -1.215, -2.111, -0.134,  1.092,\n",
      "         2.98 ,  2.217, -2.496,  2.545]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "#print(model.trainable_weights)\n",
    "took_5 = packed_train_ds.take(2)\n",
    "prediction = model.predict(\n",
    "    took_5,\n",
    "    batch_size=None,\n",
    "    verbose=0,\n",
    "    steps=1,\n",
    "    callbacks=None,\n",
    "    max_queue_size=10,\n",
    "    workers=1,\n",
    "    use_multiprocessing=False)\n",
    "pred = model.predict_on_batch(took_5)\n",
    "#model.save(\"model.h5\")\n",
    "print(prediction)\n",
    "print(pred)\n",
    "example_batch, _ = iter(took_5) \n",
    "print(example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.003 -1.093  0.902 -1.004 -2.086 -2.36   2.502  2.278  2.643 -1.06\n",
      "  -0.662 -1.054]], shape=(1, 12), dtype=float32)\n",
      "[[-0.253  0.198 -0.403  0.386  0.238 -0.447  0.     0.289 -0.012 -0.464\n",
      "  -0.162  0.104 -0.187  0.079 -0.238  0.152 -0.376 -0.401  0.387 -0.115\n",
      "   0.195  0.202 -0.222  0.011]]\n"
     ]
    }
   ],
   "source": [
    "example_batch, _ = next(iter(packed_train_ds))    \n",
    "print(example_batch)\n",
    "\n",
    "\n",
    "\n",
    "# with a Sequential model\n",
    "output = K.function([model.layers[0].input],\n",
    "                    [model.layers[1].output])\n",
    "layer_output = output(example_batch)[0]\n",
    "'''''''''''''''''''''''''''''''Do Stuff'\n",
    "print(output([]))\n",
    "print(layer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0039 - mean_squared_error: 0.0039\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(packed_test_ds, verbose=1, steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #2 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-f2216baa3cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-213-f2216baa3cd1>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(model, x, layer_name, nodes_to_evaluate, output_format, auto_compile)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraft_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnodes_to_evaluate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_to_evaluate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcraft_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# collision detected in the keys.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-f2216baa3cd1>\u001b[0m in \u001b[0;36mcraft_output\u001b[0;34m(output_format_)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcraft_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mactivations_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mactivations_inputs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_layer_outputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mresult_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations_inputs_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #2 must support iteration"
     ]
    }
   ],
   "source": [
    "\"\"\"from keras import backend as K\n",
    "get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n",
    "                                  [model.layers[1].output])\n",
    "\n",
    "# output in test mode = 0\n",
    "layer_output = get_3rd_layer_output([label_batch, 0])[0]\n",
    "print(layer_output)\n",
    "# output in train mode = 1\n",
    "layer_output = get_3rd_layer_output([label_batch, 1])[0]\"\"\"\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def n_(node, output_format_):\n",
    "    node_name = str(node.name)\n",
    "    if output_format_ == 'simple':\n",
    "        if '/' in node_name:\n",
    "            return node_name.split('/')[0]\n",
    "        elif ':' in node_name:\n",
    "            return node_name.split(':')[0]\n",
    "        else:\n",
    "            return node_name\n",
    "    return node_name\n",
    "\n",
    "\n",
    "def _evaluate(model: Model, nodes_to_evaluate, x, y=None, auto_compile=False):\n",
    "    if not model._is_compiled:\n",
    "        if model.name in ['vgg16', 'vgg19', 'inception_v3', 'inception_resnet_v2', 'mobilenet_v2', 'mobilenetv2']:\n",
    "            print('Transfer learning detected. Model will be compiled with (\"categorical_crossentropy\", \"adam\").')\n",
    "            print('If you want to change the default behaviour, then do in python:')\n",
    "            print('model.name = \"\"')\n",
    "            print('Then compile your model with whatever loss you want: https://keras.io/models/model/#compile.')\n",
    "            print('If you want to get rid of this message, add this line before calling keract:')\n",
    "            print('model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")')\n",
    "            model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        else:\n",
    "            if auto_compile:\n",
    "                model.compile(loss='mse', optimizer='adam')\n",
    "            else:\n",
    "                print('Please compile your model first! https://keras.io/models/model/#compile.')\n",
    "                print('If you only care about the activations (outputs of the layers), '\n",
    "                      'then just compile your model like that:')\n",
    "                print('model.compile(loss=\"mse\", optimizer=\"adam\")')\n",
    "                raise Exception('Compilation of the model required.')\n",
    "\n",
    "def get_activations(model, x, layer_name=None, nodes_to_evaluate=None,\n",
    "                    output_format='simple', auto_compile=True):\n",
    "    \"\"\"\n",
    "    Fetch activations (nodes/layers outputs as Numpy arrays) for a Keras model and an input X.\n",
    "    By default, all the activations for all the layers are returned.\n",
    "    :param model: Keras compiled model or one of ['vgg16', 'vgg19', 'inception_v3', 'inception_resnet_v2',\n",
    "    'mobilenet_v2', 'mobilenetv2', ...].\n",
    "    :param x: Model input (Numpy array). In the case of multi-inputs, x should be of type List.\n",
    "    :param layer_name: (optional) Name of a layer for which activations should be returned only. It is useful in\n",
    "    very big networks when it is computationally expensive to evaluate all the layers/nodes.\n",
    "    :param nodes_to_evaluate: (optional) List of Keras nodes to be evaluated. Useful when the nodes are not\n",
    "    in model.layers.\n",
    "    :param output_format: Change the output dictionary key of the function.\n",
    "    - 'simple': output key will match the names of the Keras layers. For example Dense(1, name='d1') will\n",
    "    return {'d1': ...}.\n",
    "    - 'full': output key will match the full name of the output layer name. In the example above, it will\n",
    "    return {'d1/BiasAdd:0': ...}.\n",
    "    - 'numbered': output key will be an index range, based on the order of definition of each layer within the model.\n",
    "    :param auto_compile: If set to True, will auto-compile the model if needed.\n",
    "    :return: Dict {layer_name (specified by output_format) -> activation of the layer output/node (Numpy array)}.\n",
    "    \"\"\"\n",
    "    if nodes_to_evaluate is None:\n",
    "        nodes = [layer.output for layer in model.layers if layer.name == layer_name or layer_name is None]\n",
    "    else:\n",
    "        if layer_name is not None:\n",
    "            raise ValueError('Do not specify a [layer_name] with [nodes_to_evaluate]. It will not be used.')\n",
    "        nodes = nodes_to_evaluate\n",
    "\n",
    "    if len(nodes) == 0:\n",
    "        if layer_name is not None:\n",
    "            network_layers = ', '.join([layer.name for layer in model.layers])\n",
    "            raise KeyError('Could not find a layer with name: [{}]. '\n",
    "                           'Network layers are [{}]'.format(layer_name, network_layers))\n",
    "        else:\n",
    "            raise ValueError('Nodes list is empty. Or maybe the model is empty.')\n",
    "\n",
    "    # The placeholders are processed later (Inputs node in Keras). Due to a small bug in tensorflow.\n",
    "    input_layer_outputs, layer_outputs = [], []\n",
    "    [input_layer_outputs.append(node) if 'input_' in node.name else layer_outputs.append(node) for node in nodes]\n",
    "    activations = _evaluate(model, layer_outputs, x, y=None, auto_compile=auto_compile)\n",
    "\n",
    "    def craft_output(output_format_):\n",
    "        activations_dict = OrderedDict(zip([n_(output, output_format_) for output in layer_outputs], activations))\n",
    "        activations_inputs_dict = OrderedDict(zip([n_(output, output_format_) for output in input_layer_outputs], x))\n",
    "        result_ = activations_inputs_dict.copy()\n",
    "        result_.update(activations_dict)\n",
    "        if output_format_ == 'numbered':\n",
    "            result_ = OrderedDict([(i, v) for i, (k, v) in enumerate(result_.items())])\n",
    "        return result_\n",
    "\n",
    "    result = craft_output(output_format)\n",
    "    if nodes_to_evaluate is not None and len(result) != len(nodes_to_evaluate):\n",
    "        result = craft_output(output_format_='full')  # collision detected in the keys.\n",
    "\n",
    "    return result\n",
    "\n",
    "print(get_activations(model, label_batch))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
