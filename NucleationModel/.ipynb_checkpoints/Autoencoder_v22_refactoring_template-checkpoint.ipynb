{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from copy import deepcopy \n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#import time\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_correlation(test_snapshot_list):\n",
    "    column_list = np.transpose(test_snapshot_list)\n",
    "    covariance_matrix = np.cov(column_list)\n",
    "    strong_correlated_input_list = []\n",
    "    weak_correlated_input_list = []\n",
    "    for row_nr in range(len(covariance_matrix)):\n",
    "        for entry_nr in range(len(covariance_matrix[row_nr])):\n",
    "            if row_nr > entry_nr:\n",
    "                if abs(covariance_matrix[row_nr][entry_nr]) >= STRONG_CORRELATION_TRESHOLD:\n",
    "                    strong_correlated_input_list.append([[str(row_nr), str(entry_nr)], \"{:.3f}\".format(covariance_matrix[row_nr][entry_nr])])\n",
    "                elif abs(covariance_matrix[row_nr][entry_nr]) >= WEAK_CORRELATION_TRESHOLD:\n",
    "                    weak_correlated_input_list.append([[str(row_nr), str(entry_nr)], \"{:.3f}\".format(covariance_matrix[row_nr][entry_nr])])\n",
    "    if len(strong_correlated_input_list) > 0 or len(weak_correlated_input_list) > 0:\n",
    "        print(\"Caution!\\n\\\n",
    "Correlation between input data can affect the reliability of the importance measure.\\n\\\n",
    "Strong correlations of more than {} were found between {} pair(s) of input variables:\\n\\t\"\\\n",
    ".format(STRONG_CORRELATION_TRESHOLD, len(strong_correlated_input_list)) + \"\\n\\t\".join([\": \".join([\",\".\\\n",
    "join(subentry) if isinstance(subentry, list) else subentry for subentry in entry]) for entry \\\n",
    "in strong_correlated_input_list])+ \"\\nAdditionally, weak correlations of more than {} were \\\n",
    "found between {} pair(s) of input variables:\\n\\t\".format(WEAK_CORRELATION_TRESHOLD, \\\n",
    "len(weak_correlated_input_list)) + \"\\n\\t\".join([\": \".join([\",\".join(subentry) if isinstance(subentry, list) \\\n",
    "else subentry for subentry in entry]) for entry in weak_correlated_input_list]))\n",
    "    else:\n",
    "        print(\"No correlation above {} found between the inputs.\".format(WEAK_CORRELATION_TRESHOLD))\n",
    "    return strong_correlated_input_list, weak_correlated_input_list\n",
    "\n",
    "# if DROP_REMAINDER = True, TEST_SIZE will not fit the size of the snapshot_list anymore\n",
    "# either remove the update of TEST_SIZE, adapt test_snapshot_list, or replace TEST_SIZE with a different value\n",
    "\n",
    "def perturb_snapshot_list(column_list, mod_along, perturbation):\n",
    "    transposed_list = deepcopy(column_list)\n",
    "    rand_array = np.random.uniform(1 - perturbation, 1 + perturbation, TEST_SIZE)\n",
    "    transposed_list[mod_along] = column_list[mod_along] * rand_array\n",
    "    return np.transpose(transposed_list)\n",
    "\n",
    "def set_mean_snapshot_list(column_list, mod_along, column_mean):\n",
    "    transposed_list = deepcopy(column_list)\n",
    "    transposed_list[mod_along] = column_mean\n",
    "    return np.transpose(transposed_list)\n",
    "\n",
    "def HIPR_snapshot_list(column_list, mod_along, min_value, max_value):\n",
    "    transposed_list = deepcopy(column_list)\n",
    "    rand_array = np.random.uniform(min_value, max_value, TEST_SIZE)\n",
    "    transposed_list[mod_along] = rand_array\n",
    "    return np.transpose(transposed_list)\n",
    "\n",
    "def shuffle_snapshot_list(column_list, mod_along):\n",
    "    # since independent runs should use different shuffled lists, nothing is passed down\n",
    "    transposed_list = deepcopy(column_list)\n",
    "    transposed_list[mod_along] = shuffle(transposed_list[mod_along])\n",
    "    return np.transpose(transposed_list)\n",
    "\n",
    "def input_importance(mode, mode_var, model, test_ds, test_past_snapshot_list, test_snapshot_label_list, \\\n",
    "                     test_snapshot_list, check_vars, repetitions):\n",
    "    orig_t_loss, orig_l_loss, orig_r_loss = model.evaluate(test_ds, verbose=0, steps = EVAL_STEP_NUMBER)\n",
    "    meta_loss_list = []\n",
    "    test_column_list = np.transpose(test_past_snapshot_list)\n",
    "    # initialization dependent on the mode\n",
    "    if mode == \"Perturb\":\n",
    "        perturbation = mode_var\n",
    "    elif mode == \"Mean\":\n",
    "        if repetitions > 1:\n",
    "            print(\"The mean mode does not entail stochasticity. \\nNumber of repetitions was set to '1' for this measurement.\")\n",
    "            repetitions = 1\n",
    "        mean_value_array = np.mean(test_past_snapshot_list, axis = 0)\n",
    "    elif mode == \"HIPR\":\n",
    "        min_value = mode_var[0]\n",
    "        max_value = mode_var[1]\n",
    "    elif mode == \"Shuffle\":\n",
    "        pass\n",
    "        \n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    for repetition in range(repetitions):\n",
    "        print(\"Repetition {}.\".format(repetition+1))\n",
    "        loss_list = [[],[],[]]\n",
    "        for variable_nr in check_vars:\n",
    "            print(\"\\tPerturbing variable {}.\".format(variable_nr))\n",
    "            # generating modified snapshot_lists\n",
    "            if mode == \"Perturb\":\n",
    "                mod_test_past_snapshot_list = perturb_snapshot_list(test_column_list, variable_nr, perturbation)\n",
    "            elif mode == \"Mean\":\n",
    "                mod_test_past_snapshot_list = set_mean_snapshot_list(test_column_list, variable_nr, mean_value_array[variable_nr])\n",
    "            elif mode == \"HIPR\":\n",
    "                mod_test_past_snapshot_list = HIPR_snapshot_list(test_column_list, variable_nr, min_value, max_value)\n",
    "            elif mode == \"Shuffle\":\n",
    "                mod_test_past_snapshot_list = shuffle_snapshot_list(test_column_list, variable_nr)\n",
    "                \n",
    "            mod_test_ds = tf.data.Dataset.from_tensor_slices(({INPUT_NAME: mod_test_past_snapshot_list},\n",
    "                    {OUTPUT_NAME_1: test_snapshot_label_list, \\\n",
    "                    OUTPUT_NAME_2: test_snapshot_list}, \\\n",
    "                    {OUTPUT_NAME_1: test_snapshot_weight_list, \\\n",
    "                    OUTPUT_NAME_2: test_snapshot_weight_list})\\\n",
    "                    ).shuffle(DATASET_SIZE).batch(BATCH_SIZE, drop_remainder=DROP_REMAINDER)\n",
    "            # calculate the different losses with the new dataset\n",
    "            t_loss, l_loss, r_loss = model.evaluate(mod_test_ds, verbose=0, steps = EVAL_STEP_NUMBER)\n",
    "            # append the losses to a collective list for later comparison\n",
    "            loss_list[0].append(max(0,t_loss-orig_t_loss))\n",
    "            loss_list[1].append(max(0,l_loss-orig_l_loss))\n",
    "            loss_list[2].append(max(0,r_loss-orig_r_loss))\n",
    "        # average over the loss lists\n",
    "        # negative increases of loss are set to zero\n",
    "        for row_nr in range(len(loss_list)):\n",
    "            full_loss = sum(loss_list[row_nr])\n",
    "            for col_nr in range(len(loss_list[row_nr])):\n",
    "                if full_loss > 0:\n",
    "                    loss_list[row_nr][col_nr] = loss_list[row_nr][col_nr]/full_loss\n",
    "                else:\n",
    "                    loss_list[row_nr][col_nr] = 0\n",
    "        meta_loss_list.append(np.array(loss_list))\n",
    "    \n",
    "    total_normalized_losses = np.transpose([sum(np.transpose(sum(meta_loss_list)))])\n",
    "    # sets value to 1 if all the losses add up to 0 and would cause a divide by zero error\n",
    "    total_normalized_losses = np.array([value[0] if value != 0 else 1 for value in total_normalized_losses])\n",
    "    #print(np.array(sum(meta_loss_list)/repetitions))\n",
    "    return sum(meta_loss_list)/np.transpose([total_normalized_losses])\n",
    "\n",
    "def plot_all_input_importance(model, mode_list, test_ds, test_past_snapshot_list, \\\n",
    "        test_snapshot_label_list, test_snapshot_list, variable_list, repetitions):\n",
    "    if model == encoder:\n",
    "        model_name = \"encoder\"\n",
    "    elif model == decoder_1:\n",
    "        model_name = \"decoder_1\"\n",
    "    elif model == decoder_2:\n",
    "        model_name = \"decoder_2\"\n",
    "    elif model == autoencoder_1:\n",
    "        model_name = \"autoencoder_1\"\n",
    "    elif model == autoencoder_2:\n",
    "        model_name = \"autoencoder_2\"\n",
    "    elif model == autoencoder:\n",
    "        model_name = \"autoencoder\"\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(LOSS_TYPE_COUNT, len(mode_list), figsize=(FIG_SIZE,FIG_SIZE/len(mode_list)*LOSS_TYPE_COUNT))\n",
    "    fig.suptitle(\"Input importance measures\", fontsize=FIG_SIZE*1.1, y=0.95)\n",
    "    for instance_nr in range(len(mode_list)):\n",
    "        loss_list = input_importance(mode_list[instance_nr][0], \\\n",
    "                mode_list[instance_nr][1], model, test_ds, test_past_snapshot_list, \\\n",
    "                test_snapshot_label_list, test_snapshot_list, variable_list, repetitions)\n",
    "        for loss_type in range(LOSS_TYPE_COUNT):\n",
    "            axs[loss_type][instance_nr].bar(range(len(variable_list)), loss_list[loss_type])\n",
    "            if loss_type == 0:\n",
    "                axs[loss_type][instance_nr].set_title(mode_list[instance_nr][0], fontsize = FIG_SIZE)\n",
    "            if instance_nr == 0:\n",
    "                axs[loss_type][instance_nr].set_ylabel(\"{}\".format(LOSS_NAMES[loss_type]),fontsize=FIG_SIZE)\n",
    "            axs[loss_type][instance_nr].tick_params(\n",
    "                        axis='both',\n",
    "                        which='major',\n",
    "                        labelsize = FIG_SIZE*0.9)\n",
    "    plt.setp(axs, xticks=range(len(variable_list)), xticklabels=variable_list, yticks=[0,1], yticklabels=[0,1])\n",
    "    plt.savefig(\"Importance_measure_{}_repetitions_{}_progress_{}.png\"\\\n",
    "                .format(model_name, repetitions, PROGRESS_LABEL)) \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_vs_val(history, epochs, loss_name):\n",
    "    plt.figure()\n",
    "    plt.scatter(range(1,epochs+1),history.history[loss_name], label = \"train\")\n",
    "    plt.scatter(range(1,epochs+1),history.history[\"val_\"+loss_name], label = \"val\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.title(loss_name)\n",
    "    plt.show()\n",
    "#plot_train_vs_val(history, EPOCHS, \"loss\")\n",
    "plot_train_vs_val(history, EPOCHS, \"label_loss\")\n",
    "plot_train_vs_val(history, EPOCHS, \"reconstruction_loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RESOLUTION = 51\n",
    "STAMP = \"r\" + str(RESOLUTION) + \"_p\" + str(PROGRESS_LABEL)[0] + \"_o\" + str(OFFSET) \\\n",
    "    + \"_bn\" + str(BOTTLENECK_SIZE) \\\n",
    "    + \"_\" + str(ENCODER_ACT_FUNC) + \"_\" + str(DECODER_1_ACT_FUNC) \\\n",
    "    + \"_\" + str(DECODER_2_ACT_FUNC) + \"_\" + str(NODE_MULT) + \"*(\" \\\n",
    "    + str(ENCODER_HIDDEN) + \"+\" + str(DECODER_1_HIDDEN) + \"|\" + str(DECODER_2_HIDDEN) + \")_e\" + str(EPOCHS) + \"_\" \\\n",
    "    + str(LABEL_LOSS_WEIGHT) + \":\" \\\n",
    "    + str(RECONSTRUCTION_LOSS_WEIGHT) \\\n",
    "    + \"_ot\" + str(ONLY_TRANSITION)[0]\n",
    "print(STAMP)\n",
    "\n",
    "def plot_one_map(label_map, mode, stamp):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(label_map[0])[::-1], \\\n",
    "            cmap='coolwarm', \\\n",
    "            interpolation='nearest', norm=mpl.colors.SymLogNorm(linthresh=0.01, linscale=0.01,\n",
    "                                  vmin=-1.0, vmax=1.0))\n",
    "    plt.colorbar()\n",
    "    plt.savefig(\"{}_{}.png\".format(stamp, mode)) \n",
    "    plt.show()\n",
    "\n",
    "print(\"Generated\")\n",
    "label_map = map_generated(autoencoder_1, \"snapshot\", \"label\", \\\n",
    "        np.linspace(min_values[0], max_values[0], RESOLUTION), \\\n",
    "        np.linspace(min_values[1], max_values[1], RESOLUTION), \\\n",
    "        0, 0, 1)\n",
    "plot_one_map(label_map, \"gen\", STAMP)\n",
    "\n",
    "print(\"Given_train\")\n",
    "label_map = map_given_labels(train_past_snapshot_list, \\\n",
    "    train_snapshot_label_list, \\\n",
    "    np.linspace(min_values[0], max_values[0], RESOLUTION), \\\n",
    "    np.linspace(min_values[1], max_values[1], RESOLUTION), \\\n",
    "    0, 1)\n",
    "plot_one_map(label_map, \"giv\", STAMP)\n",
    "\n",
    "#print(\"Given_test\")\n",
    "#label_map = map_given_labels(test_past_snapshot_list, \\\n",
    "#    test_snapshot_label_list, \\\n",
    "#    np.linspace(min_values[0], max_values[0], RESOLUTION), \\\n",
    "#    np.linspace(min_values[1], max_values[1], RESOLUTION), \\\n",
    "#    0, 1)\n",
    "#plot_one_map(label_map, \"given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_SIZE = 50\n",
    "make_scatter_figure(autoencoder_2, -1, 1, RESOLUTION, DIMENSIONS, 0, STAMP)\n",
    "#make_scatter_figure(autoencoder_2, -1, 1, RESOLUTION, DIMENSIONS, -0.5)\n",
    "#make_scatter_figure(autoencoder_2, -1, 1, RESOLUTION, DIMENSIONS, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-6c8ecd581424>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6c8ecd581424>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    test_snapshot_label_list, test_snapshot_list, range(10), 1)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "strong_correlated_input_list, weak_correlated_input_list = measure_correlation(test_past_snapshot_list)\n",
    "\n",
    "#mode_list = [[\"Perturb\", 0.5],[\"Mean\", None],[\"HIPR\", [-0.9,0.9]],[\"Shuffle\", None]]\n",
    "mode_list = [[\"Mean\", None],[\"HIPR\", [-0.9,0.9]],[\"Shuffle\", None]]\n",
    "\n",
    "plot_all_input_importance(autoencoder, mode_list, test_ds_batch, test_past_snapshot_list, \\\n",
    "            test_snapshot_label_list, test_snapshot_list, range(10), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-05eca9a0ccf6>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-05eca9a0ccf6>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    resolution = RESOLUTION, i_range = 2, j_range = 2, \\\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#meta_label_map_1 = make_label_map_figure(mode = \"given\", mode_var = [test_past_snapshot_list, test_snapshot_label_list], \\\n",
    "#        resolution = RESOLUTION, i_range = DIMENSIONS, j_range = DIMENSIONS, \\\n",
    "#        vmin = 0, vmax = 1)\n",
    "\n",
    "#meta_label_map_2 = make_label_map_figure(mode = \"generated\", mode_var = [autoencoder_1, 0], \\\n",
    "#        resolution = RESOLUTION, i_range = DIMENSIONS, j_range = DIMENSIONS, \\\n",
    "#        vmin = 0, vmax = 1)\n",
    "\n",
    "#meta_label_map_3 = make_label_map_figure(mode = \"generated\", mode_var = [autoencoder_2, 0], \\\n",
    "#        resolution = RESOLUTION, i_range = DIMENSIONS, j_range = DIMENSIONS, \\\n",
    "#        vmin = -1, vmax = 1)\n",
    "\n",
    "meta_label_map_4 = make_label_map_figure(mode = \"generated\", mode_var = [encoder, 0], \\\n",
    "        resolution = RESOLUTION, i_range = 2, j_range = 2, \\\n",
    "        vmin = -1, vmax = 1)\n",
    "\n",
    "meta_label_map_5 = make_label_map_figure(mode = \"generated\", mode_var = [decoder_1, 0], \\\n",
    "        resolution = RESOLUTION, i_range = BOTTLENECK_SIZE, j_range = BOTTLENECK_SIZE, \\\n",
    "        vmin = 0, vmax = 1)\n",
    "\n",
    "#meta_label_map_6 = make_label_map_figure(mode = \"generated\", mode_var = [decoder_2, 0], \\\n",
    "#        resolution = RESOLUTION, i_range = BOTTLENECK_SIZE, j_range = BOTTLENECK_SIZE, \\\n",
    "#        vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../5_ZPotential_Autoencoder/trajectory_list_3.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af041d7b4cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#trajectory_label_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_label_list_2.p\", \"rb\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtrajectory_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../5_ZPotential_Autoencoder/trajectory_list_3.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mtrajectory_label_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../5_ZPotential_Autoencoder/trajectory_label_list_3.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../5_ZPotential_Autoencoder/trajectory_list_3.p'"
     ]
    }
   ],
   "source": [
    "#COLUMN_NAMES = [\"dim1\", \"dim2\", \"dim3\", \"dim4\", \n",
    "#                \"dim5\", \"dim6\", \"dim7\", \"dim8\", \n",
    "#                \"dim9\", \"dim10\", \"label\"]\n",
    "#LABEL_NAME = \"label\"\n",
    "#INPUT_NAMES = list(COLUMN_NAMES)\n",
    "#INPUT_NAMES.remove(LABEL_NAME)\n",
    "\n",
    "# loading the trafectories from pickle files\n",
    "###trajectory_list = pickle.load(open(\"../5_N-Dim_Doublewell-generator/trajectory_list.p\", \"rb\"))\n",
    "###trajectory_label_list = pickle.load(open(\"../3_N-Dim_Doublewell-generator/trajectory_label_list.p\", \"rb\"))\n",
    "\n",
    "#trajectory_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_list.p\", \"rb\"))\n",
    "#trajectory_label_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_label_list.p\", \"rb\"))\n",
    "\n",
    "#trajectory_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_list_2.p\", \"rb\"))\n",
    "#trajectory_label_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_label_list_2.p\", \"rb\"))\n",
    "\n",
    "trajectory_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_list_3.p\", \"rb\"))\n",
    "trajectory_label_list = pickle.load(open(\"../5_ZPotential_Autoencoder/trajectory_label_list_3.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"System parameters\"\"\"\n",
    "# number of cores used\n",
    "CORES_USED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional section can be used for generation of present/future pairs\n",
    "def generate_offset_snapshot_list(trajectory_list, offset):\n",
    "    # if this function is called with offset 0, present and future trajectory list are the same\n",
    "    # as the trajectory_list and are therefore returned like this without any calculation\n",
    "    if offset == 0:\n",
    "        return trajectory_list, trajectory_list\n",
    "    # Takes in a list or np.array of trajectories and an offset value and generates two np.arrays with respective new versions\n",
    "    # of the trajectories. \n",
    "    # present_trajectory_list contains all snapshots of the trajectories excluding\n",
    "    # the last n (speficified by offset) of each.\n",
    "    # future_trajectory_list contains all snapshots of the trajectories excluding\n",
    "    # the first n (specified by offset) of each.\n",
    "    # Consequently the both lists can be used as input and desired output of an autoencoder to\n",
    "    # train for future predictive variables.\n",
    "    past_trajectory_list = np.array([trajectory[:-offset] for trajectory in trajectory_list])  \n",
    "    #truncated_present_trajectory_list = \n",
    "    trajectory_list = np.array([trajectory[offset:] for trajectory in trajectory_list])\n",
    "    return past_trajectory_list, trajectory_list\n",
    "\n",
    "def get_snapshot_label_and_weight_list(trajectory_list, trajectory_label_list, offset = 0, progress_label = False):\n",
    "    # takes in a list of trajectories and corresponding labels and generates concatenated lists of snapshots, \n",
    "    # snapshot label and snapshot progress labels\n",
    "    # can be used for present/future trajcetory lists by use or offset (same as used for generation of the list)\n",
    "    # and future = True for the future trajectory list\n",
    "    snapshot_list = []\n",
    "    snapshot_label_list = []\n",
    "    snapshot_weight_list = []\n",
    "#    len_AA = sum([1 for x in trajectory_label_list if x == 0.0])\n",
    "#    len_AB = sum([1 for x in trajectory_label_list if x == 0.5])\n",
    "#    len_BB = len(trajectory_label_list) - len_AA - len_AB\n",
    "    len_AA = sum([len(trajectory_list[x]) for x in range(len(trajectory_label_list)) \\\n",
    "                  if trajectory_label_list[x] == -1.0])\n",
    "    len_AB = sum([len(trajectory_list[x]) for x in range(len(trajectory_label_list)) \\\n",
    "                  if trajectory_label_list[x] == -0.5])\n",
    "    len_BA = sum([len(trajectory_list[x]) for x in range(len(trajectory_label_list)) \\\n",
    "                  if trajectory_label_list[x] == 0.5])\n",
    "    len_BB = sum([len(trajectory_list[x]) for x in range(len(trajectory_label_list)) \\\n",
    "                  if trajectory_label_list[x] == 1.0])\n",
    "    if ONLY_TRANSITION:\n",
    "        len_all_paths = len_AB + len_BA\n",
    "    else:\n",
    "        len_all_paths = len_AA + len_AB + len_BA + len_BB\n",
    "    \"\"\"JUST for testing\"\"\"\n",
    "#    len_all_paths = 1 \n",
    "    reciprocal_len_AA = 1.0/len_AA*len_all_paths\n",
    "    reciprocal_len_AB = 1.0/len_AB*len_all_paths\n",
    "    reciprocal_len_BA = 1.0/len_BA*len_all_paths\n",
    "    reciprocal_len_BB = 1.0/len_BB*len_all_paths\n",
    "    print(len_AA, len_AB, len_BA, len_BB)\n",
    "    for trajectory_nr in range(len(trajectory_list)):\n",
    "        trajectory = trajectory_list[trajectory_nr]\n",
    "        trajectory_label = trajectory_label_list[trajectory_nr]\n",
    "        for snapshot_nr in range(len(trajectory)):\n",
    "            \"\"\"\n",
    "            Assigns the label and weight for each of the snapshots.\n",
    "            For AA and BB paths the label is set to 0 or 1 respectively, \n",
    "            and the weight set to the corresponding value for this type of path.\n",
    "            If it is a transition path the corresponding weight is assigned to the snapshot,\n",
    "            and depending on whether progress_label is true or not either the label copied\n",
    "            or recalculated based on how far the path has gotten.\n",
    "            \"\"\"\n",
    "            # Calculates the progress along the path for AB paths. If the path label is 1 or 0,\n",
    "            # all snapshot are assigned the same label. If the path label is different (e.g. 0.5),\n",
    "            # indicating a sucessfull transition a progress along the snapshots is calculated based on\n",
    "            # the position within the trajectory and the total trajectory length.\n",
    "            # For present/future lists, the offset needs to be taken into account in the denominator\n",
    "            # If the dataset is a future variant of an offset trajectory list the progress label \n",
    "            # needs to additionally take the offset into account in the nominator.\n",
    "            if ONLY_TRANSITION == False:\n",
    "                if trajectory_label == -1.0:\n",
    "                    snapshot_list.append(trajectory[snapshot_nr])                \n",
    "                    snapshot_weight_list.append(reciprocal_len_AA*LABEL_AA_weight_factor)\n",
    "                    snapshot_label_list.append(-1.0)\n",
    "    #                snapshot_label_list.append(trajectory_label)\n",
    "                elif trajectory_label == 1.0:\n",
    "                    snapshot_list.append(trajectory[snapshot_nr])        \n",
    "                    snapshot_weight_list.append(reciprocal_len_BB*LABEL_BB_weight_factor)\n",
    "                    snapshot_label_list.append(1.0)\n",
    "            if trajectory_label != -1.0 and trajectory_label != 1.0:\n",
    "                snapshot_list.append(trajectory[snapshot_nr])\n",
    "#                snapshot_weight_list.append(reciprocal_len_AB*LABEL_AB_weight_factor \\\n",
    "#                                          + reciprocal_len_BA*LABEL_BA_weight_factor)\n",
    "                # leaving out \"Future == True\" from prior versions of the code\n",
    "                # means that the snapshot_label_list of the past version will be incorrect\n",
    "                # since it is not used, however, this should not pose a problem\n",
    "\n",
    "                # if the path is an AB path, the progress label counts upwards\n",
    "                # if it is a BA path it counts down\n",
    "                if trajectory_label == -0.5:\n",
    "                    snapshot_weight_list.append(reciprocal_len_AB*LABEL_AB_weight_factor)\n",
    "                    if progress_label == False:\n",
    "                        snapshot_label_list.append(0.0)\n",
    "                    else:\n",
    "                        snapshot_label_list.append((2*(snapshot_nr + offset)\\\n",
    "                                    /(len(trajectory) - 1.0 + offset))-1)\n",
    "                        #print(trajectory_label, snapshot_label_list[-1], snapshot_list[-1][:2])\n",
    "                elif trajectory_label == 0.5:\n",
    "                    snapshot_weight_list.append(reciprocal_len_BA*LABEL_BA_weight_factor)\n",
    "                    if progress_label == False:\n",
    "                        snapshot_label_list.append(0.0)\n",
    "                    else:\n",
    "                        snapshot_label_list.append((2*(len(trajectory)\\\n",
    "                                    -(snapshot_nr + offset + 1))/(len(trajectory) + offset - 1))-1)\n",
    "                        #print(trajectory_label, snapshot_label_list[-1], snapshot_list[-1][:2])\n",
    "                #print(snapshot_weight_list[-1])\n",
    "    return np.array(snapshot_list), np.array(snapshot_label_list), np.array(snapshot_weight_list)\n",
    "\n",
    "def show_batch(dataset):\n",
    "    for batch, label, weights in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))\n",
    "            \n",
    "def remove_outliers(snapshot_list, lower_bound, upper_bound):\n",
    "    \"\"\"Sets the values of a snapshopt that lie outside of the bounds to that \n",
    "    bound while leaving the other values unchanged.\n",
    "    Initially transposes the snapshot_list to a column list\n",
    "    For each column, it iterates over all entries and compares them to the \n",
    "    lower or upper bound of that column. If they are lower or higher, they are changed to\n",
    "    the value of that bound.\n",
    "    Returns the transpose of the column list, thereby giving the cleaned snapshot_list.\n",
    "    \"\"\"\n",
    "    \n",
    "    column_list = np.transpose(snapshot_list)\n",
    "    column_list = [[min(upper_bound[col_nr],max(lower_bound[col_nr],entry)) \\\n",
    "                                for entry in column_list[col_nr]] \\\n",
    "                                for col_nr in range(len(lower_bound))]\n",
    "    return np.transpose(column_list)\n",
    "\n",
    "def normalize_snapshots(snapshot_list, mean, std):\n",
    "    \"\"\"Normalizes the snapshot_list by substracting the mean and dividing by the standard deviation.\"\"\"\n",
    "    return (snapshot_list - mean)/std\n",
    "\n",
    "\n",
    "def shuffled_train_test_split(trajectory_list, trajectory_label_list, split_ratio, offset = 0, progress_label = False):\n",
    "    assert isinstance(split_ratio, float), \"Split ratio needs to be a float between 0.0 and 1.0\"\n",
    "    past_trajectory_list, trajectory_list = generate_offset_snapshot_list(trajectory_list, offset)\n",
    "    past_snapshot_list, _, _ \\\n",
    "            = get_snapshot_label_and_weight_list( \\\n",
    "            past_trajectory_list, trajectory_label_list, offset, \\\n",
    "            progress_label = progress_label)\n",
    "    snapshot_list, snapshot_label_list, snapshot_weight_list \\\n",
    "            = get_snapshot_label_and_weight_list( \\\n",
    "            trajectory_list, trajectory_label_list, offset, \\\n",
    "            progress_label = progress_label)\n",
    "    past_snapshot_list, snapshot_list, snapshot_label_list, snapshot_weight_list \\\n",
    "            = shuffle(past_snapshot_list, snapshot_list, snapshot_label_list, \\\n",
    "            snapshot_weight_list)\n",
    "    \n",
    "    # could consider removing outliers here, but normialization and bounds are calculated\n",
    "    # based on the training set and therefore do not exist before the split\n",
    "    \n",
    "    train_size = int(len(snapshot_label_list) * split_ratio)\n",
    "    \n",
    "    train_past_snapshot_list = past_snapshot_list[:train_size].copy()\n",
    "    test_past_snapshot_list = past_snapshot_list[train_size:].copy()\n",
    "    train_snapshot_list = snapshot_list[:train_size].copy()\n",
    "    test_snapshot_list = snapshot_list[train_size:].copy()\n",
    "    train_snapshot_label_list = snapshot_label_list[:train_size].copy()\n",
    "    test_snapshot_label_list = snapshot_label_list[train_size:].copy()    \n",
    "    train_snapshot_weight_list = snapshot_weight_list[:train_size].copy()\n",
    "    test_snapshot_weight_list = snapshot_weight_list[train_size:].copy()\n",
    "\n",
    "    # calculates the lower and upper bound for the dataset according to the OUTLIER_CUTOFF\n",
    "    lower_bound = np.percentile(train_past_snapshot_list, 100*OUTLIER_CUTOFF, axis = 0)\n",
    "    upper_bound = np.percentile(train_past_snapshot_list, 100*(1-OUTLIER_CUTOFF), axis = 0)\n",
    "\n",
    "    # removes outliers\n",
    "    train_past_snapshot_list = remove_outliers(train_past_snapshot_list, lower_bound, upper_bound)\n",
    "    test_past_snapshot_list = remove_outliers(test_past_snapshot_list, lower_bound, upper_bound)\n",
    "    train_snapshot_list = remove_outliers(train_snapshot_list, lower_bound, upper_bound)\n",
    "    test_snapshot_list = remove_outliers(test_snapshot_list, lower_bound, upper_bound)\n",
    "\n",
    "    # Calculate mean and std of the test snapshots\n",
    "    train_mean = np.mean(train_snapshot_list, axis = 0)\n",
    "    train_std = np.std(train_snapshot_list, axis = 0)\n",
    "    \n",
    "    # Normalize the data\n",
    "    train_past_snapshot_list = normalize_snapshots(train_past_snapshot_list, train_mean, train_std)\n",
    "    test_past_snapshot_list = normalize_snapshots(test_past_snapshot_list, train_mean, train_std)\n",
    "    train_snapshot_list = normalize_snapshots(train_snapshot_list, train_mean, train_std)\n",
    "    test_snapshot_list = normalize_snapshots(test_snapshot_list, train_mean, train_std)\n",
    "\n",
    "    return train_past_snapshot_list, train_snapshot_list, \\\n",
    "            train_snapshot_label_list, train_snapshot_weight_list, \\\n",
    "            test_past_snapshot_list, test_snapshot_list, \\\n",
    "            test_snapshot_label_list, test_snapshot_weight_list\n",
    "\n",
    "def generate_ds(trajectory_list, trajectory_label_list, split_ratio, offset = 0, progress_label = False):\n",
    "    \"\"\"Insert docstring\"\"\"\n",
    "    train_past_snapshot_list, train_snapshot_list, \\\n",
    "    train_snapshot_label_list, train_snapshot_weight_list, \\\n",
    "    test_past_snapshot_list, test_snapshot_list, \\\n",
    "    test_snapshot_label_list, test_snapshot_weight_list \\\n",
    "        = shuffled_train_test_split(trajectory_list, \\\n",
    "        trajectory_label_list, split_ratio, offset = offset, \\\n",
    "        progress_label = progress_label)    \n",
    "    \n",
    "    dataset_size = len(train_snapshot_list) + len(test_snapshot_list)\n",
    "    # generates the dataset by feeding in a tuple, of dictionaries (alternative would be a tuble of lists)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(({INPUT_NAME: train_past_snapshot_list},\n",
    "            {OUTPUT_NAME_1: train_snapshot_label_list, \n",
    "            OUTPUT_NAME_2: train_snapshot_list},\n",
    "            {OUTPUT_NAME_1: train_snapshot_weight_list,\n",
    "            OUTPUT_NAME_2: train_snapshot_weight_list})).shuffle(dataset_size)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(({INPUT_NAME: test_past_snapshot_list},\n",
    "            {OUTPUT_NAME_1: test_snapshot_label_list, \n",
    "            OUTPUT_NAME_2: test_snapshot_list},\n",
    "            {OUTPUT_NAME_1: test_snapshot_weight_list,\n",
    "            OUTPUT_NAME_2: test_snapshot_weight_list})).shuffle(dataset_size)\n",
    "\n",
    "    return train_ds, test_ds, \\\n",
    "            train_past_snapshot_list, train_snapshot_list, \\\n",
    "            train_snapshot_label_list, train_snapshot_weight_list, \\\n",
    "            test_past_snapshot_list, test_snapshot_list, \\\n",
    "            test_snapshot_label_list, test_snapshot_weight_list\n",
    "\n",
    "def map_given_labels(snapshot_list, snapshot_label_list, x_list, y_list, x_var_pos, y_var_pos):\n",
    "    x_nr = len(x_list)\n",
    "    y_nr = len(y_list)\n",
    "    min_x = min(x_list)\n",
    "    max_x = max(x_list)\n",
    "    min_y = min(y_list)\n",
    "    max_y = max(y_list)\n",
    "    x_span = max_x - min_x\n",
    "    y_span = max_y - min_y\n",
    "\n",
    "    # generate a list of lists of lists to store all labels in\n",
    "    label_map = [[[] for y in y_list] for x in x_list]\n",
    "    # sort the labels of each snapshot to the corresponding \"positions\" in the grid (by sorting them in the list)\n",
    "    for snapshot_nr in range(len(snapshot_list)):\n",
    "        x_snap = snapshot_list[snapshot_nr][x_var_pos]\n",
    "        y_snap = snapshot_list[snapshot_nr][y_var_pos]\n",
    "        # utilizes \"int\" to be able to use for iteration, \"round\" to round to closest full number, \n",
    "        # \"i-min_x\" to offset to start at 0, \"\"/x_span*(x-nr-1)\"\" to rescale\n",
    "        x_int = int(round((x_snap - min_x)/x_span*(x_nr-1)))\n",
    "        y_int = int(round((y_snap - min_y)/y_span*(y_nr-1)))\n",
    "        # after the gridpoint closest to the snapshot position is determined \n",
    "        # if the snapshot lies within the bounds\n",
    "        # the snapshots label is appended to the list corresponding to that grid point\n",
    "        if x_int >= 0 and x_int <= x_nr-1 and y_int >= 0 and y_int <= y_nr-1:\n",
    "            label_map[x_int][y_int].append(snapshot_label_list[snapshot_nr])\n",
    "        else:\n",
    "            # snapshot lies outside of the bounds\n",
    "            pass\n",
    "    # takes the mean of all labels associated to each respective grid point\n",
    "    for row_ind in range(len(label_map)):\n",
    "        for col_ind in range(len(label_map[row_ind])):\n",
    "            if len(label_map[row_ind][col_ind]) > 0:\n",
    "                label_map[row_ind][col_ind] = np.mean(label_map[row_ind][col_ind])\n",
    "            else:\n",
    "                label_map[row_ind][col_ind] = float('NaN')\n",
    "    return [label_map]\n",
    "\n",
    "def map_generated(model, input_type, output_type, x_list, y_list, additional_dim_val, x_var_pos, y_var_pos):\n",
    "    assert x_var_pos != y_var_pos, \"x_var_pos and y_var_pos need to differ\"\n",
    "    if input_type == \"bn\":\n",
    "        dimensions = BOTTLENECK_SIZE\n",
    "    elif input_type == \"snapshot\":\n",
    "        dimensions = DIMENSIONS\n",
    "    else:\n",
    "        assert True, \"input_type needs to be set to 'bn' (bottleneck) or 'snapshot'.\"\n",
    "    if output_type == \"label\":\n",
    "        output_len = 1\n",
    "    elif output_type == \"bn\":\n",
    "        output_len = BOTTLENECK_SIZE\n",
    "    elif output_type == \"snapshot\":\n",
    "        output_len = DIMENSIONS\n",
    "    else:\n",
    "        assert True, \"output_type needs to be set to 'bn' (bottleneck), 'label' or 'snapshot'.\"\n",
    "    output_map = [[] for i in range(output_len)]\n",
    "    for x in x_list:\n",
    "        output_current_row = [[] for i in range(output_len)]\n",
    "        for y in y_list:\n",
    "            # make predicition for current grid point\n",
    "            prediction = model.predict([[x if x_var_pos == pos_nr else y if \\\n",
    "                    y_var_pos == pos_nr else additional_dim_val \\\n",
    "                    for pos_nr in range(dimensions)]])[0]\n",
    "            for i in range(output_len):\n",
    "                output_current_row[i].append(prediction[i])\n",
    "        for i in range(output_len):\n",
    "            output_map[i].append(output_current_row[i])\n",
    "    return np.array(output_map)    \n",
    "\n",
    "def make_label_map_figure(mode, mode_var, resolution, i_range, j_range, vmin, vmax):\n",
    "    if mode == \"generated\":\n",
    "        model = mode_var[0]\n",
    "        additional_dim_value = mode_var[1]\n",
    "        if model == encoder:\n",
    "            input_type = \"snapshot\"\n",
    "            output_type = \"bn\"\n",
    "            suptitle = \"Predicted bottlenecks depending on snapshot input\"\n",
    "            model_name = \"encoder\"\n",
    "        elif model == decoder_1:\n",
    "            input_type = \"bn\"\n",
    "            output_type = \"label\"\n",
    "            suptitle = \"Predicted labels on bottleneck input\"\n",
    "            model_name = \"decoder_1\"\n",
    "        elif model == decoder_2:\n",
    "            input_type = \"bn\"\n",
    "            output_type = \"snapshot\"\n",
    "            suptitle = \"Predicted snapshots depending on bottleneck input\"\n",
    "            model_name = \"decoder_2\"\n",
    "        elif model == autoencoder_1:\n",
    "            input_type = \"snapshot\"\n",
    "            output_type = \"label\"\n",
    "            suptitle = \"Predicted labels depending on snapshot input\"\n",
    "            model_name = \"autoencoder_1\"\n",
    "        elif model == autoencoder_2:\n",
    "            input_type = \"snapshot\"\n",
    "            output_type = \"snapshot\"\n",
    "            suptitle = \"Predicted snapshots depending on snapshot input\"\n",
    "            model_name = \"autoencoder_2\"\n",
    "    elif mode == \"given\":\n",
    "        snapshot_list = mode_var[0]\n",
    "        snapshot_label_list = mode_var[1]\n",
    "        model_name = \"given\"\n",
    "    meta_label_map = []            \n",
    "    for i in range(i_range):\n",
    "        meta_label_map.append([])   \n",
    "        for j in range(j_range):\n",
    "            meta_label_map[i].append([])\n",
    "            if j > i:\n",
    "                print(i, j)\n",
    "                if mode == \"generated\":\n",
    "                    label_map = map_generated(model, input_type, output_type, \\\n",
    "                        np.linspace(min_values[i], max_values[i], resolution), \\\n",
    "                        np.linspace(min_values[j], max_values[j], resolution), \\\n",
    "                        additional_dim_value, i, j)\n",
    "                elif mode == \"given\":\n",
    "                    label_map = map_given_labels(snapshot_list, \\\n",
    "                        snapshot_label_list, \\\n",
    "                        np.linspace(min_values[i], max_values[i], resolution), \\\n",
    "                        np.linspace(min_values[j], max_values[j], resolution), \\\n",
    "                        i, j)\n",
    "                    suptitle = \"Given labels depending on snapshot input\"\n",
    "                meta_label_map[i][j].append(label_map)         \n",
    "    # gets output len by simply taking the length of the last generated label_map\n",
    "    for k in range(len(label_map)):\n",
    "        print(k)\n",
    "        fig, axs = plt.subplots(i_range, j_range, figsize=(FIG_SIZE,FIG_SIZE))\n",
    "        fig.suptitle(suptitle, fontsize=FIG_SIZE*1.5)                \n",
    "        for i in range(i_range):\n",
    "            for j in range(j_range):\n",
    "                axs[i][j].tick_params(\n",
    "                    axis='both',\n",
    "                    which='both',\n",
    "                    bottom=False,\n",
    "                    top=False,\n",
    "                    labelbottom=False,\n",
    "                    left = False,\n",
    "                    labelleft= False)   \n",
    "        for i in range(i_range):  \n",
    "            for j in range(j_range):\n",
    "                if j > i:\n",
    "                    im = axs[j][i].imshow(np.transpose(meta_label_map[i][j][0][k])[::-1], \\\n",
    "                        cmap='coolwarm', \\\n",
    "                        interpolation='nearest', norm=mpl.colors.SymLogNorm(linthresh=0.25, linscale=0.25,\n",
    "                                              vmin=-1.0, vmax=1.0))\n",
    "                    if mode == \"given\":\n",
    "                        if j == j_range - 1:\n",
    "                            axs[j][i].set_xlabel(\"x{}\".format(i),fontsize=FIG_SIZE)\n",
    "                        if i == 0:\n",
    "                            axs[j][i].set_ylabel(\"x{}\".format(j),fontsize=FIG_SIZE)\n",
    "                    else:\n",
    "                        if input_type == \"snapshot\":\n",
    "                            if j == j_range - 1:\n",
    "                                axs[j][i].set_xlabel(\"x{}\".format(i),fontsize=FIG_SIZE)\n",
    "                            if i == 0:\n",
    "                                axs[j][i].set_ylabel(\"x{}\".format(j),fontsize=FIG_SIZE)\n",
    "                        elif input_type == \"bn\":\n",
    "                            if j == j_range - 1:\n",
    "                                axs[j][i].set_xlabel(\"b{}\".format(i),fontsize=FIG_SIZE)\n",
    "                            if i == 0:\n",
    "                                axs[j][i].set_ylabel(\"b{}\".format(j),fontsize=FIG_SIZE)\n",
    "                else:\n",
    "                    axs[j][i].axis(\"off\")\n",
    "        cax,kw = mpl.colorbar.make_axes([ax for ax in axs])\n",
    "        cbar = plt.colorbar(im, cax=cax, **kw)\n",
    "        cbar.ax.tick_params(labelsize=FIG_SIZE)\n",
    "        plt.savefig(\"Map_{}_res_{}_output_node_{}_progress_{}.png\".format(model_name, \\\n",
    "                    resolution, k, str(PROGRESS_LABEL))) \n",
    "        plt.show()\n",
    "    return meta_label_map\n",
    "\n",
    "def plot_scatter_for_each_dim(model, x_list, x_var_pos, additional_dim_val):\n",
    "    dimensions = DIMENSIONS\n",
    "    y_list = []\n",
    "    for x in x_list:\n",
    "        prediction = model.predict([[x if x_var_pos == pos_nr else additional_dim_val \\\n",
    "                for pos_nr in range(dimensions)]])[0]\n",
    "        y_list.append(prediction[x_var_pos])\n",
    "    return x_list, y_list \n",
    "\n",
    "def make_scatter_figure(model, x_min, x_max, resolution, i_range, additional_dim_val, stamp):\n",
    "    \"\"\"Generates a superfigure of scater plots.\n",
    "    Iterates over the different dimensions and based on different input values for one dimensions\n",
    "    as well as a fixed value fr all other dimensions, predicts the reconstructed value for that dimension.\n",
    "    An optimal encoding and decoding will yield a diagonal line for each dimension indifferent of the value\n",
    "    chosen for the other dimensions.\n",
    "    \"\"\"\n",
    "    suptitle = \"Predicted snapshots depending on snapshot_input\"\n",
    "    fig, axs = plt.subplots(1, i_range, figsize=(FIG_SIZE,FIG_SIZE/i_range/0.8))\n",
    "    fig.suptitle(suptitle, fontsize=FIG_SIZE*1.5, y = 0.99)                \n",
    "\n",
    "    for i in range(i_range):   \n",
    "#        x_list, y_list = plot_scatter_for_each_dim(model, np.linspace(lower_bound[i], \\\n",
    "#                        upper_bound[i], resolution), i, additional_dim_val)\n",
    "        x_list, y_list = plot_scatter_for_each_dim(model, np.linspace(x_min, \\\n",
    "                        x_max, resolution), i, additional_dim_val)\n",
    "\n",
    "        axs[i].tick_params(\n",
    "                axis='both',\n",
    "                which='both',\n",
    "                top=False,\n",
    "                bottom=True,\n",
    "                labelbottom=True,\n",
    "                left = True,\n",
    "                labelleft= True)    \n",
    "        im = axs[i].scatter(x_list, y_list, s=FIG_SIZE*4)\n",
    "        axs[i].set_xlim([x_min,x_max])\n",
    "        axs[i].set_ylim([x_min,x_max])\n",
    "        axs[i].set_xlabel(\"x{}\".format(i),fontsize=FIG_SIZE)\n",
    "    plt.tight_layout(rect = [0, 0, 1, 0.8])\n",
    "    plt.savefig(\"{}_ad{}_scat.png\"\\\n",
    "            .format(stamp, additional_dim_val)) \n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "toy_ds = test_ds\n",
    "toy_ds = tf.data.Dataset.from_tensor_slices(({INPUT_NAME:[[0,0], [1,2], [3,3]]},{\"b\":[1,2,3]},{\"c\":[1,2,3]})).shuffle(4)\n",
    "\n",
    "filtered_toy_ds = test_ds.filter(lambda x,y,z: tf.reduce_all([x[INPUT_NAME][i] < upper_bound[i] \\\n",
    "                                                        and x[INPUT_NAME][i] > lower_bound[i] \\\n",
    "                                                        for i in range(len(x[INPUT_NAME]))]))\n",
    "#filtered_toy_ds = toy_ds.filter(lambda x,y,z: x[\"a\"][0] < 3) \n",
    "#filtered_toy_ds = toy_ds.filter(lambda x,y,z: all([x[\"a\"][i] < 2 for i in range(2)]))\n",
    "\n",
    "show_batch(filtered_toy_ds)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Filter outliers\"\"\"\n",
    "\n",
    "\"\"\"Problem: If done for test set this way there will simply be no prediction\"\"\"\n",
    "# calculates the lower and upper bound for the normalized dataset according to the OUTLIER_CUTOFF\n",
    "lower_bound = np.percentile(train_past_snapshot_list, 100*OUTLIER_CUTOFF, axis = 0)\n",
    "upper_bound = np.percentile(train_past_snapshot_list, 100*(1-OUTLIER_CUTOFF), axis = 0)\n",
    "\n",
    "# uses dataset.filter to filter out all snapshots that have at least one value outside of the bounds\n",
    "# the lambda function returns True or False for each instance of the dataset depending on whether \n",
    "#the conditions are met\n",
    "# tf.reduce_all returns True if all values in the list composed by list comprehension are True\n",
    "# the list comprehension generates a list of True and False values for each dimension of the input\n",
    "#and whether the corresponding value lies within the bounds of this dimension\n",
    "test_ds = test_ds.filter(lambda inputs,labels,weights: tf.reduce_all([inputs[INPUT_NAME][i] < upper_bound[i] \\\n",
    "                                                        and inputs[INPUT_NAME][i] > lower_bound[i] \\\n",
    "                                                        for i in range(len(inputs[INPUT_NAME]))]))\n",
    "train_ds = train_ds.filter(lambda inputs,labels,weights: tf.reduce_all([inputs[INPUT_NAME][i] < upper_bound[i] \\\n",
    "                                                        and inputs[INPUT_NAME][i] > lower_bound[i] \\\n",
    "                                                        for i in range(len(inputs[INPUT_NAME]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perturbed_loss_list= input_importance(\"Perturb\", 0.5, autoencoder, test_norm_ds_batch, test_past_snapshot_list, \\\n",
    "#        test_snapshot_label_list, test_snapshot_list, range(3), 1)\n",
    "#plot_input_importance(perturbed_loss_list,1,\"Perturb_0.5\")\n",
    "#mean_loss_list = input_importance(\"Mean\", None, autoencoder, test_norm_ds_batch, test_past_snapshot_list, \\\n",
    "#        test_snapshot_label_list, test_snapshot_list, range(3), 1)\n",
    "#plot_input_importance(mean_loss_list,1,\"Mean\")\n",
    "#hipr_loss_list = input_importance(\"HIPR\", [-0.9,0.9], autoencoder, test_norm_ds_batch, test_past_snapshot_list, \\\n",
    "#        test_snapshot_label_list, test_snapshot_list, range(3), 1)\n",
    "#plot_input_importance(hipr_loss_list,1,\"HIPR_range[-0.9,0.9]\")\n",
    "#shuffle_loss_list = input_importance(\"Shuffle\", None, autoencoder, test_norm_ds_batch, test_past_snapshot_list, \\\n",
    "#        test_snapshot_label_list, test_snapshot_list, range(3), 1)\n",
    "#plot_input_importance(shuffle_loss_list,1,\"Shuffle\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.172 -0.771]]\n",
      "[[ 0.172 -0.771]]\n"
     ]
    }
   ],
   "source": [
    "# accessing the output of all layers\n",
    "features_list = [layer.output for layer in encoder.layers]\n",
    "feat_extraction_model = keras.Model(inputs=encoder.input, outputs=features_list[-1])\n",
    "img = np.random.random((1,10)).astype('float32')\n",
    "print(encoder(img).numpy())\n",
    "extracted_features = feat_extraction_model(img)\n",
    "print(extracted_features.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
