{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the future functions actually necessary?\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import imp\n",
    "\n",
    "from data_read import read_paths\n",
    "from pathData import PathData\n",
    "from snapData import SnapData\n",
    "from datasetData import DatasetData\n",
    "from plotData import PlotData\n",
    "from importanceData import ImportanceData\n",
    "from globalConstants import Const\n",
    "#from data_plot import map_generated\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# allows for automatic reloading of imports and makes it unncessecary to restart the kernel\n",
    "# whenever a functio is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setter c.used_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const()\n",
    "\n",
    "used_vars = [0, 1, 7, 8, 9, 10, 11, 13, 3]\n",
    "used_vars = [0, 2, 5, 6, 7, 8, 9, 11, 13, 17, 18, 19, 20, 21]\n",
    "var_names = c.used_names(used_vars)\n",
    "var_order = c.used_order(used_vars)\n",
    "print(var_names)\n",
    "\n",
    "c.turnedback = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathO = PathData(\n",
    "    *read_paths(\n",
    "        c.folder_name, \n",
    "        c.mcg_A, \n",
    "        c.mcg_B,\n",
    "        used_vars), \n",
    "    c.path_type_labels, \n",
    "    c.path_type_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapO = SnapData(\n",
    "    *pathO.snapshots_labels_weights(\n",
    "        offset = c.offset, \n",
    "        progress = c.progress, \n",
    "        transitioned = c.transitioned, \n",
    "        turnedback = c.turnedback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataO = DatasetData(\n",
    "    *snapO.split_lists(c.train_ratio, c.val_ratio), \n",
    "    outlier_cutoff = c.outlier_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates the dataset by feeding in a tuple, of dictionaries (alternative would be a tuble of lists)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.train_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: dataO.train_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_weights, \n",
    "    c.output_name_2: dataO.train_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"train_ds generated\")\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.val_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: dataO.val_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_weights, \n",
    "    c.output_name_2: dataO.val_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"val_ds generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = keras.Input(shape=(dataO.dimensions,),name=c.input_name)\n",
    "x = keras.layers.Dense(\n",
    "    dataO.dimensions * c.node_mult, \n",
    "    activation = c.encoder_act_func)(encoder_input)\n",
    "for i in range(c.encoder_hidden - 1):\n",
    "    x = keras.layers.Dense(\n",
    "        dataO.dimensions * c.node_mult, \n",
    "        activation = c.encoder_act_func)(x)\n",
    "#x = keras.layers.Dropout(0.1)(x)\n",
    "#x = keras.layers.Dense(DIMENSIONS*NODE_MULT, activation='tanh')(x)\n",
    "#x = keras.layers.Dense(DIMENSIONS*NODE_MULT, activation='tanh')(x)\n",
    "\n",
    "encoder_output = keras.layers.Dense(\n",
    "    c.bottleneck_size, \n",
    "    activation=c.encoder_act_func, \n",
    "    name='bottleneck')(x)\n",
    "\n",
    "encoder = keras.Model(\n",
    "    encoder_input, \n",
    "    encoder_output, \n",
    "    name = \"Encoder\")\n",
    "#encoder.summary()\n",
    "\n",
    "decoder_input = keras.Input(shape = (c.bottleneck_size,), name = \"encoded_snapshots\")\n",
    "\n",
    "x1 = keras.layers.Dense(\n",
    "    dataO.dimensions * c.node_mult, \n",
    "    activation = c.decoder_1_act_func)(decoder_input)\n",
    "for i in range(c.decoder_1_hidden):\n",
    "    x1 = keras.layers.Dense(\n",
    "        dataO.dimensions * c.node_mult, \n",
    "        activation = c.decoder_1_act_func)(x1)\n",
    "#x1 = keras.layers.Dropout(0.1)(x1)\n",
    "#x1 = keras.layers.Dense(DIMENSIONS*NODE_MULT, activation='tanh')(x1)\n",
    "#x1 = keras.layers.Dense(DIMENSIONS*NODE_MULT, activation='tanh')(x1)\n",
    "decoder_output_1 = keras.layers.Dense(\n",
    "    1, \n",
    "    activation = c.decoder_1_act_func, \n",
    "    name = c.output_name_1)(x1)\n",
    "\n",
    "decoder_1 = keras.Model(\n",
    "    decoder_input, \n",
    "    decoder_output_1, \n",
    "    name = c.output_name_1)\n",
    "#decoder_1.summary()\n",
    "\n",
    "x2 = keras.layers.Dense(\n",
    "    dataO.dimensions * c.node_mult, \n",
    "    activation = c.decoder_2_act_func)(decoder_input)\n",
    "for i in range(c.decoder_2_hidden):\n",
    "    x2 = keras.layers.Dense(\n",
    "        dataO.dimensions * c.node_mult, \n",
    "        activation = c.decoder_2_act_func)(x2)\n",
    "#x2 = keras.layers.Dropout(0.1)(x2)\n",
    "#x2 = keras.layers.Dense(DIMENSIONS*NODE_MULT, activation='tanh')(x2)\n",
    "#x2 = keras.layers.Dense(DIMENSIONS*NODE_MULT, activation='tanh')(x2)\n",
    "decoder_output_2 = keras.layers.Dense(\n",
    "    dataO.dimensions, \n",
    "    activation = c.decoder_2_act_func,\n",
    "    name = c.output_name_2)(x2)\n",
    "\n",
    "decoder_2 = keras.Model(\n",
    "    decoder_input, \n",
    "    decoder_output_2, \n",
    "    name = c.output_name_2)\n",
    "#decoder_2.summary()\n",
    "\n",
    "autoencoder_input = keras.Input(shape = (dataO.dimensions,), name = c.input_name)\n",
    "encoded_snaphot = encoder(autoencoder_input)\n",
    "label_snapshot = decoder_1(encoded_snaphot)\n",
    "reconstructed_snapshot = decoder_2(encoded_snaphot)\n",
    "\n",
    "autoencoder = keras.Model(\n",
    "    inputs = autoencoder_input, \\\n",
    "    outputs = [label_snapshot,reconstructed_snapshot], \\\n",
    "    name = \"Autoencoder\")\n",
    "\n",
    "#model_layout = keras.utils.plot_model(autoencoder, 'autoencoder.png', show_shapes=True)\n",
    "#display.display(model_layout)\n",
    "\n",
    "#display.display(keras.utils.plot_model(encoder, 'encoder.png', show_shapes=True))\n",
    "#display.display(keras.utils.plot_model(decoder_1, 'decoder_1.png', show_shapes=True))\n",
    "#display.display(keras.utils.plot_model(decoder_2, 'decoder_2.png', show_shapes=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "#   loss={OUTPUT_NAME_1:keras.losses.CategoricalHinge(),\n",
    "    loss={c.output_name_1:keras.losses.MeanSquaredError(),\n",
    "        c.output_name_2: keras.losses.MeanAbsoluteError()},\n",
    "    loss_weights=[c.label_loss_weight, c.reconstruction_loss_weight])\n",
    "\n",
    "autoencoder_1 = keras.Model(\n",
    "    inputs=autoencoder_input, \n",
    "    outputs=label_snapshot, \n",
    "    name = \"Autoencoder_1\")\n",
    "autoencoder_1.compile(optimizer=keras.optimizers.RMSprop(1e-3), \n",
    "#   loss={OUTPUT_NAME_1:keras.losses.CategoricalHinge()}, \\\n",
    "    loss={c.output_name_1:keras.losses.MeanSquaredError()},\n",
    "    loss_weights=[c.label_loss_weight])\n",
    "\n",
    "autoencoder_2 = keras.Model(\n",
    "    inputs=autoencoder_input, \n",
    "    outputs=reconstructed_snapshot, \n",
    "    name = \"Autoencoder_2\")\n",
    "autoencoder_2.compile(optimizer=keras.optimizers.RMSprop(1e-3), \n",
    "    loss={c.output_name_2:keras.losses.MeanSquaredError()}, \n",
    "    loss_weights=[c.reconstruction_loss_weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#autoencoder.fit(train_ds_batch,epochs=EPOCHS, class_weight=class_weight)\n",
    "\n",
    "history = autoencoder.fit(train_ds,epochs = c.epochs, validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotO = PlotData(*dataO.plot_data(), c.stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotO.plot_super_map(\n",
    "    subfig_size = c.subfig_size, \n",
    "    i_s = var_order, js = var_order, \n",
    "    stamp = c.stamp, var_names = var_names, \n",
    "    resolution = c.resolution, \n",
    "    vmin = -1.0, vmax = 1.0, model = None, fill_val = 0)\n",
    "        \n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotO.plot_super_map(\n",
    "    subfig_size = c.subfig_size, \n",
    "    i_s = var_order, js = var_order, \n",
    "    stamp = c.stamp, var_names = var_names, \n",
    "    resolution = c.resolution, \n",
    "    vmin = -1.0, vmax = 1.0, \n",
    "    model = autoencoder_1, fill_val = 0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotO.plot_super_scatter(\n",
    "    subfig_size = c.subfig_size, i_s = var_order, \n",
    "    stamp = c.stamp, var_names = var_names, \n",
    "    resolution = c.resolution, model = autoencoder_2, \n",
    "    max_row_len = 6, fill_val = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO = ImportanceData(\n",
    "    *dataO.importance_data(),\n",
    "    c.corr_thresholds)\n",
    "\n",
    "impO.measure_correlation()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.val_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: dataO.val_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_weights, \n",
    "    c.output_name_2: dataO.val_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"val_ds generated\")\n",
    "\n",
    "modes = [[\"Mean\", None],[\"HIPR\", [-0.9,0.9]],[\"Shuffle\", None]]\n",
    "#modes = [[\"Perturb\", 0.5]]\n",
    "\n",
    "impO.plot_super_importance(\n",
    "    subfig_size = c.subfig_size, i_s = var_order, \n",
    "    stamp = c.stamp, var_names = var_names, \n",
    "    repetitions = 1, modes = modes,\n",
    "    loss_names = c.loss_names,\n",
    "    val_ds = val_ds, model = autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_ds generated\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.test_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.test_snapshot_labels, \n",
    "    c.output_name_2: dataO.test_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.test_snapshot_weights, \n",
    "    c.output_name_2: dataO.test_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"test_ds generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trT_reT_pT_o0_oc0.05_bn2_4*(4None+4tanh|4tanh)_pw1:10:0:0_lw1:1_e2\n",
      "1\n",
      "[3, 4]\n",
      "4\n",
      "trT_reT_pT_o0_oc0.05_bn2_4*(4None+4tanh|4tanh)_pw1:10:0:0_lw3:4_e2\n"
     ]
    }
   ],
   "source": [
    "print(c.stamp)\n",
    "print(c.reconstruction_loss_weight)\n",
    "c.loss_weights = [3,4]\n",
    "print(c.loss_weights)\n",
    "print(c.reconstruction_loss_weight)\n",
    "print(c.stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VAR_NAMES = {i:ALL_VAR_NAMES[i] for i in USED_VARS}\n",
    "#VAR_ORDER = [i for i in ALL_VAR_ORDER if i in USED_VARS]\n",
    "#VAR_ORDER = [i for i in range(len(ALL_VAR_ORDER)) if ALL_VAR_ORDER[i] in USED_VARS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VAR_NAMES = {\n",
    "    0: \"MCG\",\n",
    "    11: \"N_{s,2}\",\n",
    "    12:\"N_{s,3}\",    \n",
    "    15: \"N_{s,4}\",\n",
    "    13: \"N_{c,2}\",\n",
    "    14: \"N_{c,3}\",\n",
    "    16: \"N_{c,4}\",\n",
    "    3: \"N_{w,2}\",   \n",
    "    2: \"N_{w,3}\",\n",
    "    1: \"N_{w,4}\",\n",
    "    5: \"N_{sw,2-3}\",    \n",
    "    4: \"N_{sw,3-4}\",\n",
    "    9: \"5^{12}\",\n",
    "    8: \"5^{12}6^{2}\",\n",
    "    17: \"5^{12}6^{3}\",\n",
    "    18: \"5^{12}6^{4}\",\n",
    "    19: \"4^{1}5^{10}6^{2}\",\n",
    "    20: \"4^{1}5^{10}6^{3}\",\n",
    "    21: \"4^{1}5^{10}6^{4}\",\n",
    "    10: \"CR\",\n",
    "    7: \"R_g\",\n",
    "    6: \"F4\"\n",
    "    }\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "VAR_NAMES = {\n",
    "    0: \"MCG\"\n",
    "    1: \"N_{s,2}\"\n",
    "    2 :\"N_{s,3}\"\n",
    "    3: \"N_{s,4}\"\n",
    "    4: \"N_{c,2}\"\n",
    "    5: \"N_{c,3}\"\n",
    "    6: \"N_{c,4}\"\n",
    "    7: \"N_{w,2}\"\n",
    "    8: \"N_{w,3}\"\n",
    "    9: \"N_{w,4}\"\n",
    "    10: \"N_{sw,2-3}\"\n",
    "    11: \"N_{sw,3-4}\"\n",
    "    12: \"5^{12}\"\n",
    "    13: \"5^{12}6^{2}\"\n",
    "    14: \"5^{12}6^{3}\"\n",
    "    15: \"5^{12}6^{4}\"\n",
    "    16: \"4^{1}5^{10}6^{2}\"\n",
    "    17: \"4^{1}5^{10}6^{3}\"\n",
    "    18: \"4^{1}5^{10}6^{4}\"\n",
    "    19: \"CR\"\n",
    "    20: \"R_g\"\n",
    "    21: \"F4\"\n",
    "    }\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder\n",
      "['input_snapshots']\n",
      "['label', 'reconstruction']\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder\n",
    "print(model.name)\n",
    "print(model.input_names)\n",
    "print(model.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label, weights in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#past, present = data.offset_path_lists(5)\n",
    "#print(past[0][:10])\n",
    "#print(present[0][:5])\n",
    "#print(data.path_list[0])\n",
    "#print(data.label_list[2])\n",
    "#print(data.mc_weight_list[2])\n",
    "#AA_past_snapshot_list, AB_past_snapshot_list, \\\n",
    "#    BA_past_snapshot_list, BB_past_snapshot_list, \\\n",
    "#    AA_snapshot_list, AB_snapshot_list, \\\n",
    "#    BA_snapshot_list, BB_snapshot_list, \\\n",
    "#    AA_snapshot_label_list, AB_snapshot_label_list, \\\n",
    "#    BA_snapshot_label_list, BB_snapshot_label_list, \\\n",
    "#    AA_snapshot_weight_list, AB_snapshot_weight_list, \\\n",
    "#    BA_snapshot_weight_list, BB_snapshot_weight_list = paths.snapshot_label_weight_lists(0,True,True,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
