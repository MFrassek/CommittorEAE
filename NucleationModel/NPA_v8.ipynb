{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the future functions actually necessary?\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import imp\n",
    "\n",
    "from data_read import read_RPE_and_TPS\n",
    "from pathData import PathData\n",
    "from snapData import SnapData\n",
    "from datasetData import DatasetData\n",
    "from plotData import PlotData\n",
    "from importanceData import ImportanceData\n",
    "from globalConstants import Const\n",
    "from autoEncoder import AutoEncoder \n",
    "from stepwiseData import StepwiseData\n",
    "from gridData import GridData\n",
    "from reduce import Reducer\n",
    "from trimmer import Trimmer\n",
    "from pB_balancer import pB_Balancer\n",
    "#from data_plot import map_generated\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# allows for automatic reloading of imports and makes it unncessecary to restart the kernel\n",
    "# whenever a function is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const()\n",
    "#c.RPE_folder_name = \"RPE_org\"\n",
    "\n",
    "#used_vars = [0, 1, 7, 8, 9, 10, 11, 13, 3]\n",
    "#used_vars = [0, 2, 5, 6, 7, 8, 9, 11, 13, 17, 18, 19, 20, 21]\n",
    "#used_vars = [\n",
    "#    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
    "#    12, 13, 14, 15, 16, 17, 18, 19, 20, 21\n",
    "#    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame, mcg, now4, now3, now2, sw34, sw23, f4_value, \n",
    "# rg2_value, cage_big, cage_small, cage_ratio, \n",
    "# surface_carbon_2, surface_carbon_3, n_core_2, \n",
    "# n_core_3, surface_carbon_4, n_core_4\n",
    "\n",
    "\"5^{12}6^{3}, 5^{12}6^{4}, 4^{1}5^{10}6^{2}, 4^{1}5^{10}6^{3}, 4^{1}5^{10}6^{4}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathO = PathData(\n",
    "    *read_RPE_and_TPS(\n",
    "        c.RPE_folder_name, \n",
    "        c.TPS_folder_name, \n",
    "        c.mcg_A, \n",
    "        c.mcg_B,\n",
    "        c.big_C,\n",
    "        c.used_frac,\n",
    "        c.used_frac), \n",
    "    c.path_type_labels, \n",
    "    c.path_type_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from data_read import read_TPS\n",
    "#from data_read import read_RPE\n",
    "#RPE_paths, RPE_labels, RPE_weights, RPE_names = read_RPE(c.RPE_folder_name, \n",
    "#    c.mcg_A, c.mcg_B, c.big_C, 0.1) \n",
    "#TPS_paths, TPS_labels, TPS_weights, TPS_names = read_TPS(c.TPS_folder_name, \n",
    "#    c.mcg_A, c.mcg_B, c.big_C, 0.7, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapO = SnapData(\n",
    "    *pathO.snapshots_labels_weights(\n",
    "        offset = c.offset, \n",
    "        progress = c.progress, \n",
    "        transitioned = c.transitioned, \n",
    "        turnedback = c.turnedback))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataO = DatasetData(\n",
    "    *snapO.split_lists(c.train_ratio, c.val_ratio), \n",
    "    outlier_cutoff = c.outlier_cutoff, \n",
    "    resolution = c.resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridO = GridData(c.resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_list_var_names = [\n",
    "    \"MCG\", \"5^{12}6^{2}\", \"5^{12}\",\n",
    "    \"CR\", \"R_g\", \"F4\"\n",
    "    ]\n",
    "\n",
    "#reduced_list_var_names = [\n",
    "#    \"MCG\", \"5^{12}6^{2}\", \"5^{12}\",\n",
    "#    \"CR\", \"R_g\", \"F4\", \n",
    "#    \"N_{s,2}\", \"N_{s,3}\", \"N_{s,4}\", \n",
    "#    \"N_{c,2}\", \"N_{c,3}\", \"N_{c,4}\",\n",
    "#    \"N_{w,2}\", \"N_{w,3}\", \"N_{w,4}\",\n",
    "#    \"N_{sw,2-3}\", \"N_{sw,3-4}\"\n",
    "#    ]\n",
    "#reduced_list_var_names = ['MCG', 'N_{s,2}', 'N_{s,3}', 'N_{s,4}', 'N_{c,2}', \"5^{12}6^{2}\", \"5^{12}\"]\n",
    "reduced_name_to_list_position = {reduced_list_var_names[i]: i for i in range(len(reduced_list_var_names))}\n",
    "\n",
    "reducer6 = Reducer(reduced_list_var_names, c.name_to_list_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_past_snapshots = dataO.train_grid_past_snapshots\n",
    "train_reduced_grid_past_snapshots = reducer6.reduce_snapshots(train_grid_past_snapshots)\n",
    "#train_grid_snapshots = train_grid_past_snapshots\n",
    "val_grid_past_snapshots = dataO.val_grid_past_snapshots\n",
    "val_reduced_grid_past_snapshots = reducer6.reduce_snapshots(val_grid_past_snapshots)\n",
    "#val_grid_snapshots = val_grid_past_snapshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pB_dict, train_pBs = gridO.approximate_pB(\n",
    "    train_reduced_grid_past_snapshots,\n",
    "    dataO.train_snapshot_labels,\n",
    "    dataO.train_snapshot_weights)\n",
    "train_pBs_len = len(train_pBs)\n",
    "\n",
    "val_pB_dict, val_pBs = gridO.approximate_pB(\n",
    "    val_reduced_grid_past_snapshots,\n",
    "    dataO.val_snapshot_labels, \n",
    "    dataO.val_snapshot_weights)\n",
    "val_pBs_len = len(val_pBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trimmed_pB_dict = Trimmer.trim_dict(train_pB_dict)\n",
    "print(len(train_trimmed_pB_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 10\n",
    "\n",
    "train_pB_balanced_weights = pB_Balancer.balance(train_pBs, bins)\n",
    "train_pB_balanced_and_masked_weights = pB_Balancer.balance_and_mask(train_pBs, bins)\n",
    "train_pB_trimmed_and_balanced_weights = pB_Balancer.trim_and_balance(train_pBs, bins)\n",
    "\n",
    "val_pB_balanced_weights = pB_Balancer.balance(val_pBs, bins)\n",
    "val_pB_balanced_and_masked_weights = pB_Balancer.balance_and_mask(val_pBs, bins)\n",
    "val_pB_trimmed_and_balanced_weights = pB_Balancer.trim_and_balance(val_pBs, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pB_Balancer.balance(train_pBs, bins)[:100])\n",
    "print(len(pB_Balancer.balance(train_pBs, bins)))\n",
    "print(pB_Balancer.balance_and_trim(train_pBs, bins)[:100])\n",
    "print(len(pB_Balancer.balance_and_trim(train_pBs, bins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_full_pB_normed_weights = dataO.train_snapshot_weights * train_pB_normed_weights\n",
    "val_full_pB_normed_weights = dataO.val_snapshot_weights * val_pB_normed_weights\n",
    "\n",
    "train_full_pB_normed_and_trimmed__weights = dataO.train_snapshot_weights * train_pB_normed_and_trimmed_weights\n",
    "val_full_pB_normed_and_trimmed_weights = dataO.val_snapshot_weights * val_pB_normed_and_trimmed_weights\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_reduced_normed_snapshots = \\\n",
    "    reducer6.reduce_snapshots(dataO.train_norm_past_snapshots)\n",
    "val_reduced_normed_snapshots = \\\n",
    "    reducer6.reduce_snapshots(dataO.val_norm_past_snapshots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trimmer = Trimmer(train_pBs)\n",
    "val_trimmer = Trimmer(val_pBs)\n",
    "print(len(train_trimmer.trim(train_reduced_normed_snapshots)))\n",
    "print(len(val_trimmer.trim(val_reduced_normed_snapshots)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nonZeroOne_gridpoints = [key for key, label in train_pB_dict.items() if label > 0  and label < 1]\n",
    "train_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_normed_weights, \n",
    "    c.output_name_2: train_pB_normed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_normed_weights, \n",
    "    c.output_name_2: val_pB_normed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelO = AutoEncoder(c)\n",
    "(autoencoder, autoencoder_1, autoencoder_2) = \\\n",
    "    modelO.model(len(reduced_list_var_names))\n",
    "history = autoencoder.fit(\n",
    "    train_pBn_ds,epochs = 1,  \n",
    "    validation_data = val_pBn_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotO = PlotData(train_reduced_grid_past_snapshots, \n",
    "    dataO.minima,\n",
    "    dataO.maxima,\n",
    "    dataO.train_snapshot_labels,\n",
    "    dataO.train_snapshot_weights,\n",
    "    \"testing_\"+c.stamp)\n",
    "super_map = plotO.plot_super_map(\n",
    "    subfig_size = c.subfig_size, \n",
    "    used_variable_names = reduced_list_var_names,\n",
    "    name_to_list_position = reduced_name_to_list_position, \n",
    "    resolution = c.resolution,\n",
    "    vmin = c.min_label, vmax = c.max_label, \n",
    "    #method = plotO.calc_map_generated,\n",
    "    #model = autoencoder_1, points_of_interest = None, fill_val = 0)\n",
    "    #method = plotO.calc_map_given,\n",
    "    #model = None, points_of_interest = None, fill_val = 0)\n",
    "    #method = plotO.calc_partial_map_generated,\n",
    "    #model = autoencoder_1, points_of_interest = train_trimmed_pB_dict, fill_val = 0)\n",
    "    method = plotO.calc_partial_map_given,\n",
    "    model = None, points_of_interest = train_trimmed_pB_dict, fill_val = 0)\n",
    "#pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_different_settings(train_ds, val_ds, bonus_stamp):\n",
    "    modelO = AutoEncoder(c)\n",
    "    #(autoencoder, autoencoder_1, autoencoder_2) = \\\n",
    "    #    modelO.model(dataO.dimensions)\n",
    "    (autoencoder, autoencoder_1, autoencoder_2) = \\\n",
    "        modelO.model(len(reduced_list_var_names))\n",
    "    #autoencoder.fit(train_ds_batch,epochs=EPOCHS, class_weight=class_weight)\n",
    "    history = autoencoder.fit(\n",
    "        train_ds,epochs = 1,  \n",
    "        validation_data = val_ds)\n",
    "    plotO = PlotData(train_reduced_grid_past_snapshots, \n",
    "        dataO.minima,\n",
    "        dataO.maxima,\n",
    "        dataO.train_snapshot_labels,\n",
    "        dataO.train_snapshot_weights,\n",
    "        bonus_stamp+c.stamp)\n",
    "#    plotO.plot_super_map(\n",
    "#        subfig_size = c.subfig_size, \n",
    "#        used_variable_names = reduced_list_var_names, \n",
    "#        name_to_list_position = reduced_name_to_list_position, \n",
    "#        resolution = c.resolution,\n",
    "#        vmin = c.min_label, vmax = c.max_label, \n",
    "#        method = plotO.calc_partial_map_given,\n",
    "#        model = None, \n",
    "#        points_of_interest = train_trimmed_pB_dict, \n",
    "#        fill_val = 0)\n",
    "#    plotO.plot_super_map(\n",
    "#        subfig_size = c.subfig_size, \n",
    "#        used_variable_names = reduced_list_var_names, \n",
    "#        name_to_list_position = reduced_name_to_list_position, \n",
    "#        resolution = c.resolution,\n",
    "#        vmin = c.min_label, vmax = c.max_label, \n",
    "#        method = plotO.calc_map_given,\n",
    "#        model = None, \n",
    "#        points_of_interest = None, \n",
    "#        fill_val = 0)\n",
    "    plotO.plot_super_map(\n",
    "        subfig_size = c.subfig_size, \n",
    "        used_variable_names = reduced_list_var_names, \n",
    "        name_to_list_position = reduced_name_to_list_position, \n",
    "        resolution = c.resolution,\n",
    "        vmin = c.min_label, vmax = c.max_label, \n",
    "        method = plotO.calc_partial_map_generated,\n",
    "        model = autoencoder_1, \n",
    "        points_of_interest = train_trimmed_pB_dict, \n",
    "        fill_val = 0)\n",
    "#    plotO.plot_super_map(\n",
    "#        subfig_size = c.subfig_size, \n",
    "#        used_variable_names = reduced_list_var_names, \n",
    "#        name_to_list_position = reduced_name_to_list_position, \n",
    "#        resolution = c.resolution,\n",
    "#        vmin = c.min_label, vmax = c.max_label, \n",
    "#        method = plotO.calc_map_generated,\n",
    "#        model = autoencoder_1, \n",
    "#        points_of_interest = None, \n",
    "#        fill_val = 0)\n",
    "    plotO.plot_super_scatter(\n",
    "        subfig_size = c.subfig_size, \n",
    "        used_variable_names = reduced_list_var_names, \n",
    "        name_to_list_position = reduced_name_to_list_position, \n",
    "        resolution = c.resolution,\n",
    "        model = autoencoder_2, max_row_len = 6,\n",
    "        fill_val = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pBl_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: train_pBs, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_normed_weights, \n",
    "    c.output_name_2: train_pB_normed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBl_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pBs, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_normed_weights, \n",
    "    c.output_name_2: val_pB_normed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_pBl_pBn_ds, val_pBl_pBn_ds, \"pBs_as_labels_pB_normed_as_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pBl_pBnat_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: train_pBs, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_normed_and_trimmed_weights, \n",
    "    c.output_name_2: train_pB_normed_and_trimmed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBl_pBnat_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pBs, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_normed_and_trimmed_weights, \n",
    "    c.output_name_2: val_pB_normed_and_trimmed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "print(len([ele for ele in train_pBs if ele == 0 or ele == 1]))\n",
    "print(len(train_pBs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_pBl_pBnat_ds, val_pBl_pBnat_ds, \"pBs_as_labels_pB_normed_and_trimmed_as_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trimm all points of pB 0 and pB 1 away for snapshots, labels and weights\n",
    "\"\"\"\n",
    "\n",
    "trimmer = [ele == 0 for ele in train_pB_normed_and_trimmed_weights]\n",
    "print(len(train_reduced_normed_snapshots[trimmer]))\n",
    "train_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: train_pBs, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_normed_and_trimmed_weights, \n",
    "    c.output_name_2: train_pB_normed_and_trimmed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pBs, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_normed_and_trimmed_weights, \n",
    "    c.output_name_2: val_pB_normed_and_trimmed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_pBn_ds, val_pBn_ds, \"all_trimmed_pBs_as_labels_pB_normed_as_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_normed_weights, \n",
    "    c.output_name_2: train_pB_normed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBn_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_normed_weights, \n",
    "    c.output_name_2: val_pB_normed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_pBn_ds, val_pBn_ds, \"pB_normed_as_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pBnat_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_normed_and_trimmed_weights, \n",
    "    c.output_name_2: train_pB_normed_and_trimmed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBnat_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_normed_and_trimmed_weights, \n",
    "    c.output_name_2: val_pB_normed_and_trimmed_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_pBnat_ds, val_pBnat_ds, \"pB_normed_and_trimmed_as_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: dataO.train_snapshot_weights, \n",
    "    c.output_name_2: dataO.train_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_weights, \n",
    "    c.output_name_2: dataO.val_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_ds, val_ds, \"standard_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pB_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pBs, \n",
    "    c.output_name_2: train_pBs})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pB_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pBs, \n",
    "    c.output_name_2: val_pBs})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_different_settings(train_pB_ds, val_pB_ds, \"pBs_as_weights_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(autoencoder_1.predict([[0,0,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0]]))\n",
    "#print(model.predict([[x if x_var == pos_nr else y if \\\n",
    "#                      y_var == pos_nr else fill_val \\\n",
    "#                      for pos_nr in range(in_size)]])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepO = StepwiseData(*dataO.stepwise_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.names_in_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "used, loss = stepO.bottom_up(\n",
    "    used = [], \n",
    "    #unused = c.names_in_order, \n",
    "    unused = reduced_list_var_names,\n",
    "    name_to_list_position = reduced_name_to_list_position,\n",
    "    param_limit = 8, \n",
    "    epochs = 2, \n",
    "    repetitions = 1, \n",
    "    const = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(used, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used, loss = stepO.top_down( \n",
    "    #used = c.names_in_order, \n",
    "    used = reduced_list_var_names, \n",
    "    unused = [], \n",
    "    name_to_list_position = reduced_name_to_list_position,\n",
    "    param_limit = 3, \n",
    "    epochs = 2, \n",
    "    repetitions = 1, \n",
    "    const = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(used, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO = ImportanceData(\n",
    "    *dataO.importance_data(),\n",
    "    c.corr_thresholds)\n",
    "\n",
    "impO.measure_correlation()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.val_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: dataO.val_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_weights, \n",
    "    c.output_name_2: dataO.val_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"val_ds generated\")\n",
    "\n",
    "modes = [[\"Mean\", None],[\"HIPR\", [-0.9,0.9]],[\"Shuffle\", None]]\n",
    "#modes = [[\"Perturb\", 0.5]]\n",
    "\n",
    "impO.plot_super_importance(\n",
    "    subfig_size = c.subfig_size, i_s = var_order, \n",
    "    stamp = c.stamp, var_names = var_names, \n",
    "    repetitions = 1, modes = modes,\n",
    "    loss_names = c.loss_names,\n",
    "    val_ds = val_ds, model = autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.test_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.test_snapshot_labels, \n",
    "    c.output_name_2: dataO.test_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.test_snapshot_weights, \n",
    "    c.output_name_2: dataO.test_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"test_ds generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO.reduced_set_importance([1], [2,3,4,5,6], \"a\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder\n",
    "print(model.name)\n",
    "print(model.input_names)\n",
    "print(model.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label, weights in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(\"{:20s}: {}\".format(key,value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#past, present = data.offset_path_lists(5)\n",
    "#print(past[0][:10])\n",
    "#print(present[0][:5])\n",
    "#print(data.path_list[0])\n",
    "#print(data.label_list[2])\n",
    "#print(data.mc_weight_list[2])\n",
    "#AA_past_snapshot_list, AB_past_snapshot_list, \\\n",
    "#    BA_past_snapshot_list, BB_past_snapshot_list, \\\n",
    "#    AA_snapshot_list, AB_snapshot_list, \\\n",
    "#    BA_snapshot_list, BB_snapshot_list, \\\n",
    "#    AA_snapshot_label_list, AB_snapshot_label_list, \\\n",
    "#    BA_snapshot_label_list, BB_snapshot_label_list, \\\n",
    "#    AA_snapshot_weight_list, AB_snapshot_weight_list, \\\n",
    "#    BA_snapshot_weight_list, BB_snapshot_weight_list = paths.snapshot_label_weight_lists(0,True,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setter c.used_vars\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "bce = tf.keras.losses.CategoricalCrossentropy()\n",
    "bce = tf.keras.losses.CategoricalHinge()\n",
    "loss = bce([1.], [0.])\n",
    "#print('Loss: ', loss.numpy())  # Loss: 11.522857\n",
    "def neg_likelihood(y_actual, y_pred):\n",
    "#    return -(y_actual*math.log(y_pred) + (1-y_actual)*math.log(1-y_pred))\n",
    "    #print(y_actual*math.log(y_pred), (1-y_actual)*math.log(1-y_pred))\n",
    "    # tf.math.log()\n",
    "    return -(y_actual * tf.math.log(y_pred) + (1-y_actual) * tf.math.log(1-y_pred))\n",
    "#print(likelihood_max_loss(1., 0.9999))\n",
    "loss = neg_likelihood(tf.constant([1.,1.]), tf.constant([0.000000000001,0.9]))\n",
    "print('Loss: ', loss.numpy())  # Loss: 11.522857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_columns = np.transpose(train_grid_snapshots)\n",
    "def rec_cols(used, unused, lim, p):\n",
    "    if len(used) == lim:\n",
    "        p.append(used)\n",
    "        return\n",
    "    else:\n",
    "        for i in range(len(unused)):\n",
    "            rec_cols(used+[unused[i]], unused[i+1:], lim, p)\n",
    "\n",
    "def mean_pB_attributes(dimensions, train_grid_columns, dataO, tries):\n",
    "    p = []\n",
    "    rec_cols([],list(range(22)),dimensions, p)\n",
    "    pB_uniques = []\n",
    "    pB_unique_means = []\n",
    "    pB_means = []\n",
    "    pB_unique_zeroes = []\n",
    "    for i in range(tries):\n",
    "        choice = random.choice(p)\n",
    "        short_grid_snapshots = []\n",
    "        for j in choice:\n",
    "            short_grid_snapshots.append(train_grid_columns[j])\n",
    "\n",
    "        pB_dict, pBs = gridO.approximate_pB(np.transpose(short_grid_snapshots), dataO.train_snapshot_labels, dataO.train_snapshot_weights)\n",
    "        pB_uniques.append(len(pB_dict)/len(pBs))\n",
    "        pB_unique_means.append(np.mean([label for key, label in pB_dict.items()]))\n",
    "        pB_means.append(np.mean(pBs))\n",
    "        pB_unique_zeroes.append(len([label for key, label in pB_dict.items() if label == 0])/len(pBs))\n",
    "    return np.mean(pB_uniques), np.mean(pB_unique_means), np.mean(pB_means), np.mean(pB_unique_zeroes)\n",
    "over_list = []\n",
    "for i in range(1,23):\n",
    "    print(i)\n",
    "    over_list.append(list(mean_pB_attributes(i, train_grid_columns, dataO, 5)))\n",
    "\n",
    "print(over_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_list = [[1.4504122192394765e-05, 0.4575422359289151, 0.35037916491845283, 0.0], \n",
    "             [8.823341000373481e-05, 0.31197805032254855, 0.24989161958688205, 1.4987592932141258e-05], \n",
    "             [0.0011390570628427355, 0.254336987822713, 0.21065742919045424, 0.00041312574711337754], \n",
    "             [0.0031104090041590574, 0.18133452313340712, 0.2170178744045542, 0.0019471784043289969], \n",
    "             [0.019666622751407806, 0.14815356079849162, 0.1799223783174298, 0.016739932628352418], \n",
    "             [0.03723860852286394, 0.0766339470321534, 0.15529480428303702, 0.03335392112900087], \n",
    "             [0.10585688540897394, 0.04434703300207507, 0.12188629749425983, 0.10030736652279384], \n",
    "             [0.14584233293970758, 0.03858060089555661, 0.12155443609723238, 0.13996139486143125], \n",
    "             [0.2389070660457291, 0.030955704907381622, 0.08891275517831804, 0.2316479946238054], \n",
    "             [0.3461106592002669, 0.02109568034865818, 0.07299244764109925, 0.33821316466650786], \n",
    "             [0.40314715278037977, 0.023028707168210764, 0.06465460885978427, 0.3932804819236334], \n",
    "             [0.5168345720256579, 0.020024200833715332, 0.055430137658731636, 0.5062564739753744], \n",
    "             [0.6097532969682758, 0.01770556773880911, 0.04620075248938048, 0.5986738397608754], \n",
    "             [0.649903849756633, 0.019642215099454723, 0.040862941059977745, 0.6368426459870116], \n",
    "             [0.648240710411905, 0.018966941544334908, 0.04145155090339449, 0.6356276840180286], \n",
    "             [0.694312812820677, 0.019123793821338247, 0.037413562972544265, 0.6808039151460504], \n",
    "             [0.7417976167309884, 0.0178504232145612, 0.03518109760382279, 0.7283716342882285], \n",
    "             [0.7338169652299931, 0.018954433483644333, 0.035358386024052696, 0.7197073551612314], \n",
    "             [0.7123308003495493, 0.01780718726971691, 0.03705239598119732, 0.6994197142446192], \n",
    "             [0.7675257538819679, 0.01880599290810691, 0.03314824301337975, 0.7529500780200907], \n",
    "             [0.7685473275550522, 0.02063167015576706, 0.032021440958897376, 0.752516404766538], \n",
    "             [0.7835114721563158, 0.021141123648464055, 0.031053932448914478, 0.766812392805472]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[1])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[2])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "plt.figure()\n",
    "plt.scatter(list(range(1,23)),over_columns[0], label = \"Unique entries\")\n",
    "plt.scatter(list(range(1,23)),over_columns[3], label = \"Unique entries = 0\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.ylim(-0.1,1)\n",
    "plt.xlim(0,23)\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.title(\"Fraction of unique entries of all entries\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(list(range(1,23)),(over_columns[0]-over_columns[3])/over_columns[0])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.title(\"Fraction of non-zero entries of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(balance_bins: int):\n",
    "    for i in range(1,balance_bins + 1):\n",
    "        #print(len([ele for ele in pBs if (ele >= (i-1)*0.1 and ele < i*0.1)]))\n",
    "        print(i/balance_bins, len([ele for ele in train_pBs \\\n",
    "            if (ele >= (i-1)/balance_bins and ele < i/balance_bins)]))\n",
    "#balance(20)\n",
    "\n",
    "bins = 20\n",
    "lens = [len([ele for ele in train_pBs if (ele >= (b-1)/(bins-1) and ele < b/(bins-1))]) for b in range(1, bins +1)]\n",
    "\n",
    "train_pBs_len = len(train_pBs)\n",
    "\n",
    "balancers = [train_pBs_len / elem for elem \\\n",
    "    in [len([ele for ele in train_pBs \\\n",
    "    if (ele >= (b-1)/(bins-1) and ele < b/(bins-1))]) for b in range(1, bins +1)] if elem > 0]\n",
    "sum_ba = sum(balancers)\n",
    "balancers = [ele / sum_ba for ele in balancers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print([*zip([(key, label) for key, label in pB_dict.items()])])\n",
    "#print([key for key, label in pB_dict.items()])\n",
    "#print([label for key, label in pB_dict.items()])\n",
    "trimmed_keys = list(map(list,[list(map(lambda x: float(x)/(c.resolution-1), key)) for key, label in pB_dict.items() if label > 0.0 and label < 1.0]))\n",
    "trimmed_labels = [label for key, label in pB_dict.items() if label > 0.0 and label < 1.0]\n",
    "trimmed_back_keys = list(map(list,[list(map(lambda x: float(x)/(c.resolution-1), key)) for key, label in pB_dict.items() if label < 1.0]))\n",
    "trimmed_back_labels = [label for key, label in pB_dict.items() if label < 1.0]\n",
    "print(len(trimmed_keys))\n",
    "print(len(trimmed_back_keys))\n",
    "\n",
    "#print(trimmed_keys[:10])\n",
    "#print(trimmed_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridO.plot_distribution(train_grid_snapshots,6,20,var_names,\"untrimmed\")\n",
    "gridO.plot_distribution(trimmed_keys,6,20,var_names,\"trimmed_both\")\n",
    "gridO.plot_distribution(trimmed_back_keys,6,20,var_names,\"trimmed_back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(pB_dict))\n",
    "#print(max([label for key, label in pB_dict.items()]))\n",
    "#print(min([label for key, label in pB_dict.items()]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label == 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label < 0.25]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.25]))\n",
    "\n",
    "def broken_hist(xs, bins, y_lower_1, y_upper_1, y_lower_2, y_upper_2, filename):\n",
    "    f, (ax, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "    # ax.hist([label for key, label in pB_dict.items()], 100)\n",
    "    # ax2.hist([label for key, label in pB_dict.items()], 100)\n",
    "    ax.hist(xs, bins)\n",
    "    ax2.hist(xs, bins)\n",
    "    ax.set_ylim(y_lower_2, y_upper_2)  # outliers only\n",
    "    ax2.set_ylim(y_lower_1, y_upper_1)  # most of the data\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "    ax2.xaxis.tick_bottom()\n",
    "\n",
    "    d = .015  # how big to make the diagonal lines in axes coordinates\n",
    "    # arguments to pass to plot, just so we don't keep repeating them\n",
    "    kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax.plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\n",
    "    ax.plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "    kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.hist(train_pBs, 10)\n",
    "plt.savefig(\"pB_untrimmed.png\")\n",
    "plt.show()\n",
    "\n",
    "broken_hist(train_pBs, 10, 0, 20000, 100000, 850000, \"pBs.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(trimmed_labels, 10)\n",
    "plt.savefig(\"trimmed_labels.png\")\n",
    "plt.show()\n",
    "    \n",
    "broken_hist(trimmed_back_labels, 10, 0, 8000, 10000, 400000, \"Hist_scaled_labels.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates the dataset by feeding in a tuple, of dictionaries \n",
    "# (alternative would be a tuble of lists)\n",
    "\"\"\"\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.train_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_labels, \n",
    "    c.output_name_2: dataO.train_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.train_snapshot_weights, \n",
    "    c.output_name_2: dataO.train_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"train_ds generated\")\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.val_norm_past_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_labels, \n",
    "    c.output_name_2: dataO.val_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.val_snapshot_weights, \n",
    "    c.output_name_2: dataO.val_snapshot_weights})) \\\n",
    "        .shuffle(snapO.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"val_ds generated\")\n",
    "\"\"\"\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
