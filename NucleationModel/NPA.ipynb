{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the future functions actually necessary?\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import imp\n",
    "\n",
    "from globalConstants import Const\n",
    "from datasetData import DatasetData\n",
    "from pipeline import Pipeline\n",
    "from autoEncoder import AutoEncoder \n",
    "from corrector import Corrector\n",
    "from importanceData import ImportanceData\n",
    "from stepper import Stepper\n",
    "\n",
    "from plotter import *\n",
    "from data_read import *\n",
    "from helperFunctions import *\n",
    "from losses import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# allows for automatic reloading of imports and makes it unncessecary to restart the kernel\n",
    "# whenever a function is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetType = \"DW\" #\"DW\", \"ZP\", or \"MH\"\n",
    "assert dataSetType == \"DW\" or dataSetType == \"ZP\" or dataSetType == \"MH\",\\\n",
    "    \"dataSetType needs to be set to 'DW', 'ZP' or 'MH'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataSetType == \"DW\" or dataSetType == \"ZP\":\n",
    "    train_val_test_function = make_train_val_test_from_toy\n",
    "    reduced_list_var_names = [\n",
    "        \"x_{1}\", \"x_{2}\", \"x_{3}\", \"x_{4}\", \"x_{5}\"]\n",
    "    get_paths_function=get_toy_paths\n",
    "elif dataSetType == \"MH\":\n",
    "    train_val_test_function = make_train_val_test_from_TIS_and_TPS\n",
    "    reduced_list_var_names = [\n",
    "        \"MCG\", \"5^{12}6^{2}\", \"5^{12}\",\n",
    "        \"CR\", \"R_g\", \"F4\", \"N_{w,3}\", \"5^{12}6^{4}\"]\n",
    "#    reduced_list_var_names = [\n",
    "#        \"MCG\", \"N_{w,4}\", \"N_{w,3}\", \"N_{w,2}\", \"N_{sw,3-4}\",\n",
    "#        \"N_{sw,2-3}\", \"F4\", \"R_g\", \"5^{12}6^{2}\", \"5^{12}\",\n",
    "#        \"CR\", \"N_{s,2}\", \"N_{s,3}\", \"N_{c,2}\", \"N_{c,3}\",\n",
    "#        \"N_{s,4}\", \"N_{c,4}\", \"5^{12}6^{3}\", \"5^{12}6^{4}\", \n",
    "#        \"4^{1}5^{10}6^{2}\", \"4^{1}5^{10}6^{3}\", \"4^{1}5^{10}6^{4}\"]\n",
    "    get_paths_function=get_TPS_and_TIS_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, valData, _ = DatasetData\\\n",
    "    .initialize_train_val_test_datasets(\n",
    "        *train_val_test_function(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_name_to_list_position = \\\n",
    "    {reduced_list_var_names[i]: i for i in range(len(reduced_list_var_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_coverage(list_var_names, trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(c, trainData.snapshots)\n",
    "print(get_size(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "c = Const(dataSetType)\n",
    "c.bottleneck_size = 1\n",
    "Stepper.iter_top_down(\n",
    "    pipeline=pipeline,\n",
    "    train_dataset=trainData,\n",
    "    val_dataset=valData,\n",
    "    used=reduced_list_var_names,\n",
    "    param_limit=1,\n",
    "    epochs=1,\n",
    "    repetitions=1,\n",
    "    const=c)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, train_bnrn_minima, train_bnrn_maxima, \\\n",
    "    train_corrected_1D, train_corrected_2D = \\\n",
    "    pipeline.prepare_prediction_plotter(\n",
    "        reduced_list_var_names,\n",
    "        trainData)\n",
    "\n",
    "val_ds, _, _, _, _ = \\\n",
    "    pipeline.prepare_prediction_plotter(\n",
    "        reduced_list_var_names,\n",
    "        valData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(train_corrected_1D, open(\"train_corrected_1D_22.p\", \"wb\"))\n",
    "#pickle.dump(train_corrected_2D, open(\"train_corrected_2D_22.p\", \"wb\"))\n",
    "\n",
    "# train_corrected_1D = pickle.load(open(\"train_corrected_1D_22.p\", \"rb\"))\n",
    "# train_corrected_2D = pickle.load(open(\"train_corrected_2D_22.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)\n",
    "c.bottleneck_size = 1\n",
    "c.epochs = 3\n",
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "    AutoEncoder.make_models(\n",
    "        len(reduced_list_var_names),\n",
    "        binaryNegLikelihood,\n",
    "        c)\n",
    "history = autoencoder.fit(\n",
    "    x=train_ds,\n",
    "    epochs=c.epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model_weights(\n",
    "    \"results/model_weights\", autoencoder, autoencoder_1,\n",
    "    autoencoder_2, encoder, decoder_1, decoder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "        load_model_weights(\n",
    "            \"results/model_weights\", \n",
    "            *AutoEncoder.make_models(\n",
    "                len(reduced_list_var_names),\n",
    "                binaryNegLikelihood,\n",
    "                c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    representations=train_corrected_2D,\n",
    "    norm=\"lin\",\n",
    "    stamp=\"x1x2_Prediction_\" + c.model_stamp + c.data_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in get_relative_encoder_importances(encoder, reduced_list_var_names):\n",
    "    print(\"{:8s}\\t{}\".format(component[0],\n",
    "        \"\\t\".join(list(map(lambda x: str(round(x, 4)),component[1:])))))\n",
    "\n",
    "plot_relative_importances(\n",
    "    *list(zip(*get_relative_encoder_importances(encoder, reduced_list_var_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_minimum, latent_maximum = \\\n",
    "    plot_example_paths_on_latent_space(\n",
    "        get_paths_function=get_paths_function,\n",
    "        const=c,\n",
    "        pipeline=pipeline,\n",
    "        reduced_list_var_names=reduced_list_var_names,\n",
    "        steps=20,\n",
    "        pre_stamp=dataSetType,\n",
    "        encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "plot_super_map(\n",
    "    used_variable_names=reduced_list_var_names,\n",
    "    name_to_list_position=reduced_name_to_list_position,\n",
    "    lower_bound=pipeline.lower_bound,\n",
    "    upper_bound=pipeline.upper_bound,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMean_map\",\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1,\n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    representations=train_corrected_2D,\n",
    "    norm=\"lin\")\n",
    "#\"\"\"\n",
    "#\"\"\"\n",
    "plot_super_scatter(\n",
    "    used_variable_names=reduced_list_var_names,\n",
    "    name_to_list_position=reduced_name_to_list_position,\n",
    "    lower_bound=pipeline.lower_bound,\n",
    "    upper_bound=pipeline.upper_bound,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMean_scatter\",\n",
    "    model=autoencoder_2,\n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    method=calc_represented_scatter_generated,\n",
    "    representations=train_corrected_1D,\n",
    "    max_row_len=8)\n",
    "#\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction_from_latent_space(\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    latent_minimum=latent_minimum,\n",
    "    latent_maximum=latent_maximum,\n",
    "    steps=11,\n",
    "    recon_decoder=decoder_2,\n",
    "    pre_stamp=\"DW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_snapshots, train_labels, train_weights = pipeline.prepare_groundTruth(\n",
    "    reduced_list_var_names,\n",
    "    trainData)\n",
    "\n",
    "plot_ground_truth(\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    grid_snapshots=train_grid_snapshots,\n",
    "    labels=train_labels,\n",
    "    weights=train_weights,\n",
    "    pre_stamp=\"GroundTruth_Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    norm=\"lin\",\n",
    "    stamp=\"x1x2_GroundTruth_\" + c.data_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trimmed_grid_snapshots, train_trimmed_labels, train_trimmed_weights = \\\n",
    "    pipeline.prepare_trimmedGroundTruth(\n",
    "        reduced_list_var_names,\n",
    "        trainData)\n",
    "\n",
    "plot_ground_truth(\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    grid_snapshots=train_trimmed_grid_snapshots,\n",
    "    labels=train_trimmed_labels,\n",
    "    weights=train_trimmed_weights,\n",
    "    pre_stamp=\"GroundTruthTrimmed_Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_encoder_decoder(\n",
    "    const=c,\n",
    "    loss_function=binaryNegLikelihood,\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    pipeline=pipeline)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_points, shooting_labels = read_shooting_points(\n",
    "    \"total_data_till_982mc_280K.txt\")\n",
    "\n",
    "shootingData = DatasetData(\n",
    "    shooting_points,\n",
    "    shooting_labels,\n",
    "    np.ones(len(shooting_labels)),\n",
    "    flag=\"Shooting\")\n",
    "shooting_grid_snapshots, \\\n",
    "    shooting_norm_snapshots, \\\n",
    "    shooting_pB_dict, \\\n",
    "    shooting_pBs, \\\n",
    "    shooting_pB_weights = pipeline.rbnga(shootingData)\n",
    "shooting_trimmed_grid_snapshots, \\\n",
    "    shooting_trimmed_norm_snapshots, \\\n",
    "    shooting_trimmed_labels, \\\n",
    "    shooting_trimmed_weights, \\\n",
    "    shooting_trimmed_pB_dict, \\\n",
    "    shooting_trimmed_pBs, \\\n",
    "    shooting_trimmed_pB_weights, \\\n",
    "    shooting_trimmed_balanced_pB_weights, \\\n",
    "    shooting_trimmed_balanced_hc_weights = pipeline.rbngatnb(shootingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCG_BigCage(x):\n",
    "    return 68.14 - 0.4286*x\n",
    "\n",
    "def now_BigCage(x):\n",
    "    return 30 - 0.0553*x\n",
    "\n",
    "def make_linspaces(function, pipeline, x_int, y_int):\n",
    "    xs = np.linspace(\n",
    "        pipeline.lower_bound[x_int],\n",
    "        pipeline.upper_bound[x_int],\n",
    "        11)\n",
    "    ys = np.array([function(x) for x in xs])\n",
    "    xs = (xs - pipeline.lower_bound[x_int]) / (pipeline.upper_bound[x_int] - pipeline.lower_bound[x_int])\n",
    "    ys = (ys - pipeline.lower_bound[y_int]) / (pipeline.upper_bound[y_int] - pipeline.lower_bound[y_int])\n",
    "    return np.array(xs), np.array(ys)\n",
    "c = Const()\n",
    "\n",
    "def plot_one_map(function, x_int, y_int, stamp, method, **kwargs):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    plt.plot(\n",
    "        *make_linspaces(function, pipeline, x_int, y_int),\n",
    "        c=\"r\")\n",
    "    plt.imshow(\n",
    "        np.maximum(\n",
    "            np.transpose(\n",
    "                method(\n",
    "                    x_pos=x_int,\n",
    "                    y_pos=y_int,\n",
    "                    resolution=c.resolution,\n",
    "                    **kwargs)[0])[::-1],\n",
    "            c.logvmin / 2),\n",
    "        cmap=c.cmap,\n",
    "        interpolation='nearest',\n",
    "        norm=mpl.colors.LogNorm(\n",
    "            vmin=c.logvmin,\n",
    "            vmax=1.0-c.logvmin),\n",
    "        extent=[0, 1, 0, 1])\n",
    "    ax.set_xticks(np.linspace(0,1,3))\n",
    "    ax.set_xticklabels(\n",
    "        np.around(\n",
    "            np.linspace(\n",
    "                pipeline.lower_bound[x_int], \n",
    "                pipeline.upper_bound[x_int],\n",
    "                3),\n",
    "            2),\n",
    "        rotation=60)\n",
    "    ax.set_yticks(np.linspace(0,1,3))\n",
    "    ax.set_yticklabels(np.around(\n",
    "        np.linspace(\n",
    "            pipeline.lower_bound[y_int], \n",
    "            pipeline.upper_bound[y_int],\n",
    "            3),\n",
    "        2))\n",
    "    ax.set_xlabel(\n",
    "        \"${}$\".format(reduced_list_var_names[x_int]),\n",
    "        fontsize=c.subfig_size * 10)\n",
    "    ax.set_ylabel(\n",
    "        \"${}$\".format(reduced_list_var_names[y_int]),\n",
    "        fontsize=c.subfig_size * 10)\n",
    "    plt.colorbar(extend=\"both\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/{}_x{}_y_{}.png\".format(stamp, x_int, y_int))\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    MCG_BigCage,\n",
    "    0,\n",
    "    1,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=shooting_grid_snapshots, \n",
    "    labels=shootingData.labels, \n",
    "    weights=shootingData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7shootingData_MCG_BigCage\" + c.data_stamp)\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    MCG_BigCage,\n",
    "    0,\n",
    "    1,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=trainData.labels, \n",
    "    weights=trainData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7trainData_MCG_BigCage\" + c.data_stamp)\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    now_BigCage,\n",
    "    6,\n",
    "    1,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=shooting_grid_snapshots, \n",
    "    labels=shootingData.labels, \n",
    "    weights=shootingData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7shootingData_now3_BigCage\" + c.data_stamp)\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    now_BigCage,\n",
    "    6,\n",
    "    1,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=trainData.labels, \n",
    "    weights=trainData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7trainData_now3_BigCage\" + c.data_stamp)\n",
    "\n",
    "c.epochs = 10\n",
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "    AutoEncoder.make_models(len(reduced_list_var_names), differenceOfLogs, c)\n",
    "#autoencoder.fit(train_ds_batch,epochs=EPOCHS, class_weight=class_weight)\n",
    "history = autoencoder.fit(\n",
    "    x=train_trimmed_pBl_pBbw_ds,\n",
    "    epochs=c.epochs,\n",
    "    validation_data=val_trimmed_pBl_pBbw_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)])\n",
    "\n",
    "plot_one_map(\n",
    "    MCG_BigCage,\n",
    "    0,\n",
    "    1,\n",
    "    method=calc_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    stamp=\"DividingSurfaceTest_Gen_MCG_BigCage\" + c.data_stamp)\n",
    "plot_one_map(\n",
    "    now_BigCage,\n",
    "    6,\n",
    "    1,\n",
    "    method=calc_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    stamp=\"DividingSurfaceTest_Gen_now3_BigCage\" + c.data_stamp)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_range_retained(outlier_cutoff):\n",
    "    snapshots = trainData.snapshots\n",
    "    span = np.amax(snapshots, axis=0) - np.amin(snapshots, axis=0)\n",
    "    percentile_span = np.percentile(snapshots, 100 - outlier_cutoff, axis=0) \\\n",
    "        - np.percentile(snapshots, outlier_cutoff, axis=0)\n",
    "    print(np.mean(percentile_span/span))\n",
    "    \n",
    "def estimate_reduction_on_AA_and_AB():\n",
    "    reducer = Reducer(\n",
    "        reduced_list_var_names,\n",
    "        c.name_to_list_position)\n",
    "    reduced_snapshots = reducer.reduce_snapshots(trainData.snapshots)\n",
    "    bounder = Bounder(reduced_snapshots, c.outlier_cutoff)\n",
    "    bound_snapshots = bounder.bound_snapshots(reduced_snapshots)\n",
    "\n",
    "    all_AA_frames = len([1 for i, label in enumerate(trainData.labels) if label == 0])\n",
    "    all_AB_frames = len([1 for i, label in enumerate(trainData.labels) if label == 1])\n",
    "    bound_AA_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 0 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "    bound_AB_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 1 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "\n",
    "    print(bound_AA_frames/all_AA_frames)\n",
    "    print(bound_AB_frames/all_AB_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_map_scatter(dataset, x, y, function):\n",
    "        xs, ys = zip(*[(snapshot[x],snapshot[y]) for snapshot in dataset.snapshots])\n",
    "        xlin = np.linspace(min(xs), max(xs),11)\n",
    "        ylin = np.array([function(x) for x in xlin])\n",
    "        weights = dataset.weights*(1/max(dataset.weights))\n",
    "        labels = [[label, 0.0, 1 - label, 0.5] for i,label in enumerate(dataset.labels)]\n",
    "        weights = [[0, weights[i], weights[i], 0.5] for i,_ in enumerate(labels)]\n",
    "        plt.plot(xlin, ylin, c=\"k\")\n",
    "        plt.scatter(xs, ys, s=0.5, c=labels)\n",
    "        plt.xlim((min(xs),max(xs)))\n",
    "        plt.ylim((min(ys),max(ys)))\n",
    "        plt.savefig(\"results/scatter_{}_{}_labels\".format(dataset.flag, function_to_str(function),))\n",
    "        plt.show()\n",
    "        plt.plot(xlin, ylin, c=\"k\")\n",
    "        plt.scatter(xs, ys, s=0.5, c=weights)\n",
    "        plt.xlim((min(xs),max(xs)))\n",
    "        plt.ylim((min(ys),max(ys)))\n",
    "        plt.savefig(\"results/scatter_{}_{}_weights\".format(dataset.flag, function_to_str(function),))\n",
    "        plt.show()\n",
    "        \n",
    "plot_one_map_scatter(\n",
    "    trainData,\n",
    "    0,\n",
    "    8,\n",
    "    MCG_BigCage)\n",
    "plot_one_map_scatter(\n",
    "    shootingData,\n",
    "    0,\n",
    "    8,\n",
    "    MCG_BigCage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO = ImportanceData(\n",
    "    *dataO.importance_data(),\n",
    "    c.corr_thresholds)\n",
    "\n",
    "impO.measure_correlation()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO.reduced_set_importance([1], [2,3,4,5,6], \"a\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder\n",
    "print(model.name)\n",
    "print(model.input_names)\n",
    "print(model.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_columns = np.transpose(train_grid_snapshots)\n",
    "def rec_cols(used, unused, lim, p):\n",
    "    if len(used) == lim:\n",
    "        p.append(used)\n",
    "        return\n",
    "    else:\n",
    "        for i in range(len(unused)):\n",
    "            rec_cols(used+[unused[i]], unused[i+1:], lim, p)\n",
    "\n",
    "def mean_pB_attributes(dimensions, train_grid_columns, dataO, tries):\n",
    "    p = []\n",
    "    rec_cols([],list(range(22)),dimensions, p)\n",
    "    pB_uniques = []\n",
    "    pB_unique_means = []\n",
    "    pB_means = []\n",
    "    pB_unique_zeroes = []\n",
    "    for i in range(tries):\n",
    "        choice = random.choice(p)\n",
    "        short_grid_snapshots = []\n",
    "        for j in choice:\n",
    "            short_grid_snapshots.append(train_grid_columns[j])\n",
    "\n",
    "        pB_dict, pBs = gridO.approximate_pB(np.transpose(short_grid_snapshots), dataO.train_labels, dataO.train_weights)\n",
    "        pB_uniques.append(len(pB_dict)/len(pBs))\n",
    "        pB_unique_means.append(np.mean([label for key, label in pB_dict.items()]))\n",
    "        pB_means.append(np.mean(pBs))\n",
    "        pB_unique_zeroes.append(len([label for key, label in pB_dict.items() if label == 0])/len(pBs))\n",
    "    return np.mean(pB_uniques), np.mean(pB_unique_means), np.mean(pB_means), np.mean(pB_unique_zeroes)\n",
    "over_list = []\n",
    "for i in range(1,23):\n",
    "    print(i)\n",
    "    over_list.append(list(mean_pB_attributes(i, train_grid_columns, dataO, 5)))\n",
    "\n",
    "print(over_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_list = [[1.4504122192394765e-05, 0.4575422359289151, 0.35037916491845283, 0.0], \n",
    "             [8.823341000373481e-05, 0.31197805032254855, 0.24989161958688205, 1.4987592932141258e-05], \n",
    "             [0.0011390570628427355, 0.254336987822713, 0.21065742919045424, 0.00041312574711337754], \n",
    "             [0.0031104090041590574, 0.18133452313340712, 0.2170178744045542, 0.0019471784043289969], \n",
    "             [0.019666622751407806, 0.14815356079849162, 0.1799223783174298, 0.016739932628352418], \n",
    "             [0.03723860852286394, 0.0766339470321534, 0.15529480428303702, 0.03335392112900087], \n",
    "             [0.10585688540897394, 0.04434703300207507, 0.12188629749425983, 0.10030736652279384], \n",
    "             [0.14584233293970758, 0.03858060089555661, 0.12155443609723238, 0.13996139486143125], \n",
    "             [0.2389070660457291, 0.030955704907381622, 0.08891275517831804, 0.2316479946238054], \n",
    "             [0.3461106592002669, 0.02109568034865818, 0.07299244764109925, 0.33821316466650786], \n",
    "             [0.40314715278037977, 0.023028707168210764, 0.06465460885978427, 0.3932804819236334], \n",
    "             [0.5168345720256579, 0.020024200833715332, 0.055430137658731636, 0.5062564739753744], \n",
    "             [0.6097532969682758, 0.01770556773880911, 0.04620075248938048, 0.5986738397608754], \n",
    "             [0.649903849756633, 0.019642215099454723, 0.040862941059977745, 0.6368426459870116], \n",
    "             [0.648240710411905, 0.018966941544334908, 0.04145155090339449, 0.6356276840180286], \n",
    "             [0.694312812820677, 0.019123793821338247, 0.037413562972544265, 0.6808039151460504], \n",
    "             [0.7417976167309884, 0.0178504232145612, 0.03518109760382279, 0.7283716342882285], \n",
    "             [0.7338169652299931, 0.018954433483644333, 0.035358386024052696, 0.7197073551612314], \n",
    "             [0.7123308003495493, 0.01780718726971691, 0.03705239598119732, 0.6994197142446192], \n",
    "             [0.7675257538819679, 0.01880599290810691, 0.03314824301337975, 0.7529500780200907], \n",
    "             [0.7685473275550522, 0.02063167015576706, 0.032021440958897376, 0.752516404766538], \n",
    "             [0.7835114721563158, 0.021141123648464055, 0.031053932448914478, 0.766812392805472]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[1])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[2])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "plt.figure()\n",
    "plt.scatter(list(range(1,23)),over_columns[0], label = \"Unique entries\")\n",
    "plt.scatter(list(range(1,23)),over_columns[3], label = \"Unique entries = 0\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.ylim(-0.1,1)\n",
    "plt.xlim(0,23)\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.title(\"Fraction of unique entries of all entries\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(list(range(1,23)),(over_columns[0]-over_columns[3])/over_columns[0])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.title(\"Fraction of non-zero entries of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridO.plot_distribution(train_grid_snapshots,6,20,var_names,\"untrimmed\")\n",
    "gridO.plot_distribution(trimmed_keys,6,20,var_names,\"trimmed_both\")\n",
    "gridO.plot_distribution(trimmed_back_keys,6,20,var_names,\"trimmed_back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(pB_dict))\n",
    "#print(max([label for key, label in pB_dict.items()]))\n",
    "#print(min([label for key, label in pB_dict.items()]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label == 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label < 0.25]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.25]))\n",
    "\n",
    "def broken_hist(xs, bins, y_lower_1, y_upper_1, y_lower_2, y_upper_2, filename):\n",
    "    f, (ax, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "    # ax.hist([label for key, label in pB_dict.items()], 100)\n",
    "    # ax2.hist([label for key, label in pB_dict.items()], 100)\n",
    "    ax.hist(xs, bins)\n",
    "    ax2.hist(xs, bins)\n",
    "    ax.set_ylim(y_lower_2, y_upper_2)  # outliers only\n",
    "    ax2.set_ylim(y_lower_1, y_upper_1)  # most of the data\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "    ax2.xaxis.tick_bottom()\n",
    "\n",
    "    d = .015  # how big to make the diagonal lines in axes coordinates\n",
    "    # arguments to pass to plot, just so we don't keep repeating them\n",
    "    kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax.plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\n",
    "    ax.plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "    kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.hist(train_pBs, 10)\n",
    "plt.savefig(\"pB_untrimmed.png\")\n",
    "plt.show()\n",
    "\n",
    "broken_hist(train_pBs, 10, 0, 20000, 100000, 850000, \"pBs.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(trimmed_labels, 10)\n",
    "plt.savefig(\"trimmed_labels.png\")\n",
    "plt.show()\n",
    "    \n",
    "broken_hist(trimmed_back_labels, 10, 0, 8000, 10000, 400000, \"Hist_scaled_labels.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
