{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the future functions actually necessary?\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import imp\n",
    "\n",
    "from globalConstants import Const\n",
    "from datasetData import DatasetData\n",
    "from pipeline import Pipeline\n",
    "from autoEncoder import AutoEncoder \n",
    "from plotter import Plotter\n",
    "from importanceData import ImportanceData\n",
    "from stepwiseData import StepwiseData\n",
    "\n",
    "from data_read import *\n",
    "from helperFunctions import *\n",
    "from plotHelperFunctions import *\n",
    "from losses import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# allows for automatic reloading of imports and makes it unncessecary to restart the kernel\n",
    "# whenever a function is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, valData, testData = DatasetData\\\n",
    "    .initialize_train_val_test_datasets(\n",
    "        *make_train_val_test_from_TIS_and_TPS(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_var_names = [\"$MCG$\", \"$N_{w,4}$\", \"$N_{w,3}$\", \"$N_{w,2}$\", \"$N_{sw,3-4}$\",\n",
    "    \"$N_{sw,2-3}$\", \"$F4$\", \"$R_g$\", \"$5^{12}6^{2}$\", \"$5^{12}$\",\n",
    "    \"$CR$\", \"$N_{s,2}$\", \"$N_{s,3}$\", \"$N_{c,2}$\", \"$N_{c,3}$\",\n",
    "    \"$N_{s,4}$\", \"$N_{c,4}$\", \"$5^{12}6^{3}$\", \"$5^{12}6^{4}$\", \n",
    "    \"$4^{1}5^{10}6^{2}$\", \"$4^{1}5^{10}6^{3}$\", \"$4^{1}5^{10}6^{4}$\"]\n",
    "\n",
    "reduced_list_var_names = [\n",
    "    \"MCG\", \"5^{12}6^{2}\", \"5^{12}\",\n",
    "    \"CR\", \"R_g\", \"F4\", \"N_{w,3}\"\n",
    "    ]\n",
    "\n",
    "reduced_name_to_list_position = {reduced_list_var_names[i]: i for i in range(len(reduced_list_var_names))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_coverage(list_var_names, trainData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c = Const()\n",
    "pipeline = Pipeline(c, reduced_list_var_names, trainData.snapshots)\n",
    "print(get_size(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_snapshots, \\\n",
    "    train_norm_snapshots, \\\n",
    "    train_pB_dict, \\\n",
    "    train_pBs, \\\n",
    "    train_pB_weights = pipeline.rbnga(trainData)\n",
    "train_trimmed_grid_snapshots, \\\n",
    "    train_trimmed_norm_snapshots, \\\n",
    "    train_trimmed_labels, \\\n",
    "    train_trimmed_weights, \\\n",
    "    train_trimmed_pB_dict, \\\n",
    "    train_trimmed_pBs, \\\n",
    "    train_trimmed_pB_weights, \\\n",
    "    train_trimmed_balanced_pB_weights, \\\n",
    "    train_trimmed_balanced_hc_weights = pipeline.rbngatnb(trainData)\n",
    "\n",
    "val_grid_snapshots, \\\n",
    "    val_norm_snapshots, \\\n",
    "    val_pB_dict, \\\n",
    "    val_pBs, \\\n",
    "    val_pB_weights = pipeline.rbnga(valData)\n",
    "val_trimmed_grid_snapshots, \\\n",
    "    val_trimmed_norm_snapshots, \\\n",
    "    val_trimmed_labels, \\\n",
    "    val_trimmed_weights, \\\n",
    "    val_trimmed_pB_dict, \\\n",
    "    val_trimmed_pBs, \\\n",
    "    val_trimmed_pB_weights, \\\n",
    "    val_trimmed_balanced_pB_weights, \\\n",
    "    val_trimmed_balanced_hc_weights = pipeline.rbngatnb(valData)\n",
    "\n",
    "train_trimmed_pBl_pBbw_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_trimmed_norm_snapshots},\n",
    "    {c.output_name_1: train_trimmed_pBs,\n",
    "    c.output_name_2: train_trimmed_norm_snapshots},\n",
    "    {c.output_name_1: train_trimmed_balanced_pB_weights,\n",
    "    c.output_name_2: train_trimmed_balanced_hc_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_trimmed_pBl_pBbw_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_trimmed_norm_snapshots},\n",
    "    {c.output_name_1: val_trimmed_pBs, \n",
    "    c.output_name_2: val_trimmed_norm_snapshots},\n",
    "    {c.output_name_1: val_trimmed_balanced_pB_weights, \n",
    "    c.output_name_2: val_trimmed_balanced_hc_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_1D_mean_representations = get_all_1D_correlated_representations(\n",
    "    train_trimmed_grid_snapshots, get_means_from_tuples)\n",
    "\n",
    "all_1D_mode_representations = get_all_1D_correlated_representations(\n",
    "    train_trimmed_grid_snapshots, get_modes_from_tuples)\n",
    "\n",
    "#all_2D_mean_representations = get_all_2D_correlated_representations(\n",
    "#    train_grid_snapshots, get_means_from_tuples)\n",
    "\n",
    "#all_2D_mode_representations = get_all_2D_correlated_representations(\n",
    "#    train_grid_snapshots, get_modes_from_tuples)\n",
    "\n",
    "all_2D_trimmed_mean_representations = get_all_2D_correlated_representations(\n",
    "    train_trimmed_grid_snapshots, get_means_from_tuples)\n",
    "\n",
    "all_2D_trimmed_mode_representations = get_all_2D_correlated_representations(\n",
    "    train_trimmed_grid_snapshots, get_modes_from_tuples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "    AutoEncoder.model(len(reduced_list_var_names), differenceOfLogs, c)\n",
    "history = autoencoder.fit(\n",
    "    x=train_trimmed_pBl_pBbw_ds,\n",
    "    epochs=c.epochs,\n",
    "    validation_data=val_trimmed_pBl_pBbw_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_components = get_encoder_formula(encoder, reduced_list_var_names)\n",
    "for component in formula_components:\n",
    "    print(component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths, labels, _, origins = make_paths_from_TPS_data(1, c)\n",
    "for i, path in enumerate(paths):\n",
    "    map_path_on_2D_latent_space(\n",
    "        pipeline=pipeline,\n",
    "        path=path,\n",
    "        encoder=encoder,\n",
    "        skip=10,\n",
    "        pre_stamp=str(i),\n",
    "        const=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in enumerate(paths):\n",
    "    map_path_on_1D_latent_space(\n",
    "        pipeline=pipeline,\n",
    "        path=path,\n",
    "        encoder=encoder,\n",
    "        skip=10,\n",
    "        pre_stamp=str(i),\n",
    "        const=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in enumerate(paths):\n",
    "    map_path_on_timed_1D_latent_space(\n",
    "        pipeline=pipeline,\n",
    "        path=path,\n",
    "        encoder=encoder,\n",
    "        skip=10,\n",
    "        pre_stamp=str(i),\n",
    "        const=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.plot_super_map(\n",
    "    used_variable_names=reduced_list_var_names,\n",
    "    name_to_list_position=reduced_name_to_list_position,\n",
    "    lower_bound=pipeline.lower_bound,\n",
    "    upper_bound=pipeline.upper_bound,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMean\",\n",
    "    method=Plotter.calc_represented_map_generated,\n",
    "    model=autoencoder_1,\n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    representations=all_2D_trimmed_mean_representations)\n",
    "pass\n",
    "\n",
    "Plotter.plot_super_map(\n",
    "    used_variable_names=reduced_list_var_names,\n",
    "    name_to_list_position=reduced_name_to_list_position,\n",
    "    lower_bound=pipeline.lower_bound,\n",
    "    upper_bound=pipeline.upper_bound,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMode\",\n",
    "    method=Plotter.calc_represented_map_generated,\n",
    "    model=autoencoder_1,\n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    representations=all_2D_trimmed_mode_representations)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.plot_super_scatter(\n",
    "    used_variable_names=reduced_list_var_names,\n",
    "    name_to_list_position=reduced_name_to_list_position,\n",
    "    lower_bound=pipeline.lower_bound,\n",
    "    upper_bound=pipeline.upper_bound,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMean\",\n",
    "    model=autoencoder_2,\n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    method=Plotter.calc_represented_scatter_generated,\n",
    "    representations=all_1D_mean_representations,\n",
    "    max_row_len=6)\n",
    "\n",
    "Plotter.plot_super_scatter(\n",
    "    used_variable_names=reduced_list_var_names,\n",
    "    name_to_list_position=reduced_name_to_list_position,\n",
    "    lower_bound=pipeline.lower_bound,\n",
    "    upper_bound=pipeline.upper_bound,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMode\",\n",
    "    model=autoencoder_2,\n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    method=Plotter.calc_represented_scatter_generated,\n",
    "    representations=all_1D_mode_representations,\n",
    "    max_row_len=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_ground_truth(\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    grid_snapshots=train_grid_snapshots,\n",
    "    labels=trainData.labels,\n",
    "    weights=trainData.weights,\n",
    "    pre_stamp=\"GroundTruthTrimmed_Train\")\n",
    "\n",
    "plot_ground_truth(\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    grid_snapshots=train_trimmed_grid_snapshots,\n",
    "    labels=train_trimmed_labels,\n",
    "    weights=train_trimmed_weights,\n",
    "    pre_stamp=\"GroundTruth_Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_used = [\n",
    "    logBinaryNegLikelihood, logBinomialNegLikelihood,\n",
    "    differenceOfLogs, logLoss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods_used:\n",
    "    print(str(method).split(\" \")[1]+\"\\n\")\n",
    "    super_pBsLabels_pBbWeights = plot_with_different_settings(\n",
    "        reduced_list_var_names=reduced_list_var_names,\n",
    "        reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "        const=c,\n",
    "        train_ds=train_trimmed_pBl_pBbw_ds,\n",
    "        val_ds=val_trimmed_pBl_pBbw_ds,\n",
    "        loss_function=method,\n",
    "        pipeline=pipeline,\n",
    "        pre_stamp=\"trimmed_pBsLabels_pBb+hcbWeights_{}\"\n",
    "            .format(function_to_str(method)),\n",
    "        minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "        maxima=np.amax(train_trimmed_norm_snapshots, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_snapshots, \\\n",
    "    train_norm_snapshots, \\\n",
    "    train_labels, \\\n",
    "    train_weights, \\\n",
    "    train_s_pB_dict, \\\n",
    "    train_s_pBs, \\\n",
    "    train_pB_weights, \\\n",
    "    train_s_balanced_pB_weights, \\\n",
    "    train_balanced_hc_weights = pipeline.rbngasb(trainData)\n",
    "\n",
    "val_grid_snapshots, \\\n",
    "    val_norm_snapshots, \\\n",
    "    val_labels, \\\n",
    "    val_weights, \\\n",
    "    val_s_pB_dict, \\\n",
    "    val_s_pBs, \\\n",
    "    val_pB_weights, \\\n",
    "    val_s_balanced_pB_weights, \\\n",
    "    val_balanced_hc_weights = pipeline.rbngasb(valData)\n",
    "\n",
    "train_squeezed_pBl_pBbw_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_norm_snapshots}, \n",
    "    {c.output_name_1: train_s_pBs, \n",
    "    c.output_name_2: train_norm_snapshots},\n",
    "    {c.output_name_1: train_s_balanced_pB_weights, \n",
    "    c.output_name_2: train_balanced_hc_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_squeezed_pBl_pBbw_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_norm_snapshots}, \n",
    "    {c.output_name_1: val_s_pBs, \n",
    "    c.output_name_2: val_norm_snapshots},\n",
    "    {c.output_name_1: val_s_balanced_pB_weights, \n",
    "    c.output_name_2: val_balanced_hc_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods_used:\n",
    "    print(str(method).split(\" \")[1]+\"\\n\")\n",
    "    super_pBsLabels_pBbWeights = plot_with_different_settings(\n",
    "        reduced_list_var_names=reduced_list_var_names,\n",
    "        reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "        const=c,\n",
    "        train_ds=train_squeezed_pBl_pBbw_ds,\n",
    "        val_ds=val_squeezed_pBl_pBbw_ds,\n",
    "        loss_function=method,\n",
    "        pipeline=pipeline,\n",
    "        pre_stamp=\"squeezed_pBsLabels_pBb+hcbWeights_{}\"\\\n",
    "            .format(function_to_str(method)),\n",
    "        minima=np.amin(train_norm_snapshots, axis=0),\n",
    "        maxima=np.amax(train_norm_snapshots, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_encoder_decoder(\n",
    "    const=c,\n",
    "    loss_function=differenceOfLogs,\n",
    "    reduced_list_var_names=reduced_list_var_names,\n",
    "    reduced_name_to_list_position=reduced_name_to_list_position,\n",
    "    train_ds=train_trimmed_pBl_pBbw_ds,\n",
    "    val_ds=val_trimmed_pBl_pBbw_ds,\n",
    "    pipeline=pipeline)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_points, shooting_labels = read_shooting_points(\n",
    "    \"total_data_till_982mc_280K.txt\")\n",
    "\n",
    "shootingData = DatasetData(\n",
    "    shooting_points,\n",
    "    shooting_labels,\n",
    "    np.ones(len(shooting_labels)),\n",
    "    flag=\"Shooting\")\n",
    "shooting_grid_snapshots, \\\n",
    "    shooting_norm_snapshots, \\\n",
    "    shooting_pB_dict, \\\n",
    "    shooting_pBs, \\\n",
    "    shooting_pB_weights = pipeline.rbnga(shootingData)\n",
    "shooting_trimmed_grid_snapshots, \\\n",
    "    shooting_trimmed_norm_snapshots, \\\n",
    "    shooting_trimmed_labels, \\\n",
    "    shooting_trimmed_weights, \\\n",
    "    shooting_trimmed_pB_dict, \\\n",
    "    shooting_trimmed_pBs, \\\n",
    "    shooting_trimmed_pB_weights, \\\n",
    "    shooting_trimmed_balanced_pB_weights, \\\n",
    "    shooting_trimmed_balanced_hc_weights = pipeline.rbngatnb(shootingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MCG_BigCage(x):\n",
    "    return 68.14 - 0.4286*x\n",
    "\n",
    "def now_BigCage(x):\n",
    "    return 30 - 0.0553*x\n",
    "\n",
    "def make_linspaces(function, pipeline, x_int, y_int):\n",
    "    xs = np.linspace(\n",
    "        pipeline.lower_bound[x_int],\n",
    "        pipeline.upper_bound[x_int],\n",
    "        11)\n",
    "    ys = np.array([function(x) for x in xs])\n",
    "    xs = (xs - pipeline.lower_bound[x_int]) / (pipeline.upper_bound[x_int] - pipeline.lower_bound[x_int])\n",
    "    ys = (ys - pipeline.lower_bound[y_int]) / (pipeline.upper_bound[y_int] - pipeline.lower_bound[y_int])\n",
    "    return np.array(xs), np.array(ys)\n",
    "c = Const()\n",
    "\n",
    "def plot_one_map(function, x_int, y_int, stamp, method, **kwargs):\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    plt.plot(\n",
    "        *make_linspaces(function, pipeline, x_int, y_int),\n",
    "        c=\"r\")\n",
    "    plt.imshow(\n",
    "        np.maximum(\n",
    "            np.transpose(\n",
    "                method(\n",
    "                    x_pos=x_int,\n",
    "                    y_pos=y_int,\n",
    "                    resolution=c.resolution,\n",
    "                    **kwargs)[0])[::-1],\n",
    "            c.logvmin / 2),\n",
    "        cmap=c.cmap,\n",
    "        interpolation='nearest',\n",
    "        norm=mpl.colors.LogNorm(\n",
    "            vmin=c.logvmin,\n",
    "            vmax=1.0-c.logvmin),\n",
    "        extent=[0, 1, 0, 1])\n",
    "    ax.set_xticks(np.linspace(0,1,3))\n",
    "    ax.set_xticklabels(\n",
    "        np.around(\n",
    "            np.linspace(\n",
    "                pipeline.lower_bound[x_int], \n",
    "                pipeline.upper_bound[x_int],\n",
    "                3),\n",
    "            2),\n",
    "        rotation=60)\n",
    "    ax.set_yticks(np.linspace(0,1,3))\n",
    "    ax.set_yticklabels(np.around(\n",
    "        np.linspace(\n",
    "            pipeline.lower_bound[y_int], \n",
    "            pipeline.upper_bound[y_int],\n",
    "            3),\n",
    "        2))\n",
    "    ax.set_xlabel(\n",
    "        \"${}$\".format(reduced_list_var_names[x_int]),\n",
    "        fontsize=c.subfig_size * 10)\n",
    "    ax.set_ylabel(\n",
    "        \"${}$\".format(reduced_list_var_names[y_int]),\n",
    "        fontsize=c.subfig_size * 10)\n",
    "    plt.colorbar(extend=\"both\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/{}_x{}_y_{}.png\".format(stamp, x_int, y_int))\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    MCG_BigCage,\n",
    "    0,\n",
    "    1,\n",
    "    method=Plotter.calc_map_given,\n",
    "    grid_snapshots=shooting_grid_snapshots, \n",
    "    labels=shootingData.labels, \n",
    "    weights=shootingData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7shootingData_MCG_BigCage\" + c.data_stamp)\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    MCG_BigCage,\n",
    "    0,\n",
    "    1,\n",
    "    method=Plotter.calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=trainData.labels, \n",
    "    weights=trainData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7trainData_MCG_BigCage\" + c.data_stamp)\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    now_BigCage,\n",
    "    6,\n",
    "    1,\n",
    "    method=Plotter.calc_map_given,\n",
    "    grid_snapshots=shooting_grid_snapshots, \n",
    "    labels=shootingData.labels, \n",
    "    weights=shootingData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7shootingData_now3_BigCage\" + c.data_stamp)\n",
    "\"\"\"\n",
    "plot_one_map(\n",
    "    now_BigCage,\n",
    "    6,\n",
    "    1,\n",
    "    method=Plotter.calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=trainData.labels, \n",
    "    weights=trainData.weights,\n",
    "    stamp=\"DividingSurfaceTest_0.7trainData_now3_BigCage\" + c.data_stamp)\n",
    "\n",
    "c.epochs = 10\n",
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "    AutoEncoder.model(len(reduced_list_var_names), differenceOfLogs, c)\n",
    "#autoencoder.fit(train_ds_batch,epochs=EPOCHS, class_weight=class_weight)\n",
    "history = autoencoder.fit(\n",
    "    x=train_trimmed_pBl_pBbw_ds,\n",
    "    epochs=c.epochs,\n",
    "    validation_data=val_trimmed_pBl_pBbw_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3)])\n",
    "\n",
    "plot_one_map(\n",
    "    MCG_BigCage,\n",
    "    0,\n",
    "    1,\n",
    "    method=Plotter.calc_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    stamp=\"DividingSurfaceTest_Gen_MCG_BigCage\" + c.data_stamp)\n",
    "plot_one_map(\n",
    "    now_BigCage,\n",
    "    6,\n",
    "    1,\n",
    "    method=Plotter.calc_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=np.amin(train_trimmed_norm_snapshots, axis=0),\n",
    "    maxima=np.amax(train_trimmed_norm_snapshots, axis=0),\n",
    "    stamp=\"DividingSurfaceTest_Gen_now3_BigCage\" + c.data_stamp)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_range_retained(outlier_cutoff):\n",
    "    snapshots = trainData.snapshots\n",
    "    span = np.amax(snapshots, axis=0) - np.amin(snapshots, axis=0)\n",
    "    percentile_span = np.percentile(snapshots, 100 - outlier_cutoff, axis=0) \\\n",
    "        - np.percentile(snapshots, outlier_cutoff, axis=0)\n",
    "    print(np.mean(percentile_span/span))\n",
    "    \n",
    "def estimate_reduction_on_AA_and_AB():\n",
    "    reducer = Reducer(\n",
    "        reduced_list_var_names,\n",
    "        c.name_to_list_position)\n",
    "    reduced_snapshots = reducer.reduce_snapshots(trainData.snapshots)\n",
    "    bounder = Bounder(reduced_snapshots, c.outlier_cutoff)\n",
    "    bound_snapshots = bounder.bound_snapshots(reduced_snapshots)\n",
    "\n",
    "    all_AA_frames = len([1 for i, label in enumerate(trainData.labels) if label == 0])\n",
    "    all_AB_frames = len([1 for i, label in enumerate(trainData.labels) if label == 1])\n",
    "    bound_AA_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 0 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "    bound_AB_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 1 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "\n",
    "    print(bound_AA_frames/all_AA_frames)\n",
    "    print(bound_AB_frames/all_AB_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_map_scatter(dataset, x, y, function):\n",
    "        xs, ys = zip(*[(snapshot[x],snapshot[y]) for snapshot in dataset.snapshots])\n",
    "        xlin = np.linspace(min(xs), max(xs),11)\n",
    "        ylin = np.array([function(x) for x in xlin])\n",
    "        weights = dataset.weights*(1/max(dataset.weights))\n",
    "        labels = [[label, 0.0, 1 - label, 0.5] for i,label in enumerate(dataset.labels)]\n",
    "        weights = [[0, weights[i], weights[i], 0.5] for i,_ in enumerate(labels)]\n",
    "        plt.plot(xlin, ylin, c=\"k\")\n",
    "        plt.scatter(xs, ys, s=0.5, c=labels)\n",
    "        plt.xlim((min(xs),max(xs)))\n",
    "        plt.ylim((min(ys),max(ys)))\n",
    "        plt.savefig(\"results/scatter_{}_{}_labels\".format(dataset.flag, function_to_str(function),))\n",
    "        plt.show()\n",
    "        plt.plot(xlin, ylin, c=\"k\")\n",
    "        plt.scatter(xs, ys, s=0.5, c=weights)\n",
    "        plt.xlim((min(xs),max(xs)))\n",
    "        plt.ylim((min(ys),max(ys)))\n",
    "        plt.savefig(\"results/scatter_{}_{}_weights\".format(dataset.flag, function_to_str(function),))\n",
    "        plt.show()\n",
    "        \n",
    "plot_one_map_scatter(\n",
    "    trainData,\n",
    "    0,\n",
    "    8,\n",
    "    MCG_BigCage)\n",
    "plot_one_map_scatter(\n",
    "    shootingData,\n",
    "    0,\n",
    "    8,\n",
    "    MCG_BigCage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_trimmed_pBsl_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_trimmed_norm_snapshots}, \n",
    "    {c.output_name_1: train_trimmed_pBs, \n",
    "    c.output_name_2: train_trimmed_norm_snapshots})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_trimmed_pBsl_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_trimmed_norm_snapshots}, \n",
    "    {c.output_name_1: val_trimmed_pBs, \n",
    "    c.output_name_2: val_trimmed_norm_snapshots})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "for method in methods_used:\n",
    "    super_pBsLabels = plot_with_different_settings(\n",
    "        train_trimmed_pBsl_ds, \n",
    "        val_trimmed_pBsl_ds, \n",
    "        method, \n",
    "        \"trimmed_pBsLabels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepO = StepwiseData(*dataO.stepwise_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.names_in_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "used, loss = stepO.bottom_up(\n",
    "    used = [], \n",
    "    #unused = c.names_in_order, \n",
    "    unused = reduced_list_var_names,\n",
    "    name_to_list_position = reduced_name_to_list_position,\n",
    "    param_limit = 8, \n",
    "    epochs = 2, \n",
    "    repetitions = 1, \n",
    "    const = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(used, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used, loss = stepO.top_down( \n",
    "    #used = c.names_in_order, \n",
    "    used = reduced_list_var_names, \n",
    "    unused = [], \n",
    "    name_to_list_position = reduced_name_to_list_position,\n",
    "    param_limit = 3, \n",
    "    epochs = 2, \n",
    "    repetitions = 1, \n",
    "    const = c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(used, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO = ImportanceData(\n",
    "    *dataO.importance_data(),\n",
    "    c.corr_thresholds)\n",
    "\n",
    "impO.measure_correlation()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.val_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.val_labels, \n",
    "    c.output_name_2: dataO.val_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.val_weights, \n",
    "    c.output_name_2: dataO.val_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"val_ds generated\")\n",
    "\n",
    "modes = [[\"Mean\", None],[\"HIPR\", [-0.9,0.9]],[\"Shuffle\", None]]\n",
    "#modes = [[\"Perturb\", 0.5]]\n",
    "\n",
    "impO.plot_super_importance(\n",
    "    subfig_size = c.subfig_size, i_s = var_order, \n",
    "    stamp = c.stamp, var_names = var_names, \n",
    "    repetitions = 1, modes = modes,\n",
    "    loss_names = c.loss_names,\n",
    "    val_ds = val_ds, model = autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: dataO.test_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.test_labels, \n",
    "    c.output_name_2: dataO.test_norm_snapshots}, \n",
    "    {c.output_name_1: dataO.test_weights, \n",
    "    c.output_name_2: dataO.test_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "print(\"test_ds generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impO.reduced_set_importance([1], [2,3,4,5,6], \"a\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder\n",
    "print(model.name)\n",
    "print(model.input_names)\n",
    "print(model.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_columns = np.transpose(train_grid_snapshots)\n",
    "def rec_cols(used, unused, lim, p):\n",
    "    if len(used) == lim:\n",
    "        p.append(used)\n",
    "        return\n",
    "    else:\n",
    "        for i in range(len(unused)):\n",
    "            rec_cols(used+[unused[i]], unused[i+1:], lim, p)\n",
    "\n",
    "def mean_pB_attributes(dimensions, train_grid_columns, dataO, tries):\n",
    "    p = []\n",
    "    rec_cols([],list(range(22)),dimensions, p)\n",
    "    pB_uniques = []\n",
    "    pB_unique_means = []\n",
    "    pB_means = []\n",
    "    pB_unique_zeroes = []\n",
    "    for i in range(tries):\n",
    "        choice = random.choice(p)\n",
    "        short_grid_snapshots = []\n",
    "        for j in choice:\n",
    "            short_grid_snapshots.append(train_grid_columns[j])\n",
    "\n",
    "        pB_dict, pBs = gridO.approximate_pB(np.transpose(short_grid_snapshots), dataO.train_labels, dataO.train_weights)\n",
    "        pB_uniques.append(len(pB_dict)/len(pBs))\n",
    "        pB_unique_means.append(np.mean([label for key, label in pB_dict.items()]))\n",
    "        pB_means.append(np.mean(pBs))\n",
    "        pB_unique_zeroes.append(len([label for key, label in pB_dict.items() if label == 0])/len(pBs))\n",
    "    return np.mean(pB_uniques), np.mean(pB_unique_means), np.mean(pB_means), np.mean(pB_unique_zeroes)\n",
    "over_list = []\n",
    "for i in range(1,23):\n",
    "    print(i)\n",
    "    over_list.append(list(mean_pB_attributes(i, train_grid_columns, dataO, 5)))\n",
    "\n",
    "print(over_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_list = [[1.4504122192394765e-05, 0.4575422359289151, 0.35037916491845283, 0.0], \n",
    "             [8.823341000373481e-05, 0.31197805032254855, 0.24989161958688205, 1.4987592932141258e-05], \n",
    "             [0.0011390570628427355, 0.254336987822713, 0.21065742919045424, 0.00041312574711337754], \n",
    "             [0.0031104090041590574, 0.18133452313340712, 0.2170178744045542, 0.0019471784043289969], \n",
    "             [0.019666622751407806, 0.14815356079849162, 0.1799223783174298, 0.016739932628352418], \n",
    "             [0.03723860852286394, 0.0766339470321534, 0.15529480428303702, 0.03335392112900087], \n",
    "             [0.10585688540897394, 0.04434703300207507, 0.12188629749425983, 0.10030736652279384], \n",
    "             [0.14584233293970758, 0.03858060089555661, 0.12155443609723238, 0.13996139486143125], \n",
    "             [0.2389070660457291, 0.030955704907381622, 0.08891275517831804, 0.2316479946238054], \n",
    "             [0.3461106592002669, 0.02109568034865818, 0.07299244764109925, 0.33821316466650786], \n",
    "             [0.40314715278037977, 0.023028707168210764, 0.06465460885978427, 0.3932804819236334], \n",
    "             [0.5168345720256579, 0.020024200833715332, 0.055430137658731636, 0.5062564739753744], \n",
    "             [0.6097532969682758, 0.01770556773880911, 0.04620075248938048, 0.5986738397608754], \n",
    "             [0.649903849756633, 0.019642215099454723, 0.040862941059977745, 0.6368426459870116], \n",
    "             [0.648240710411905, 0.018966941544334908, 0.04145155090339449, 0.6356276840180286], \n",
    "             [0.694312812820677, 0.019123793821338247, 0.037413562972544265, 0.6808039151460504], \n",
    "             [0.7417976167309884, 0.0178504232145612, 0.03518109760382279, 0.7283716342882285], \n",
    "             [0.7338169652299931, 0.018954433483644333, 0.035358386024052696, 0.7197073551612314], \n",
    "             [0.7123308003495493, 0.01780718726971691, 0.03705239598119732, 0.6994197142446192], \n",
    "             [0.7675257538819679, 0.01880599290810691, 0.03314824301337975, 0.7529500780200907], \n",
    "             [0.7685473275550522, 0.02063167015576706, 0.032021440958897376, 0.752516404766538], \n",
    "             [0.7835114721563158, 0.021141123648464055, 0.031053932448914478, 0.766812392805472]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[1])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[2])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "plt.figure()\n",
    "plt.scatter(list(range(1,23)),over_columns[0], label = \"Unique entries\")\n",
    "plt.scatter(list(range(1,23)),over_columns[3], label = \"Unique entries = 0\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.ylim(-0.1,1)\n",
    "plt.xlim(0,23)\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.title(\"Fraction of unique entries of all entries\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(list(range(1,23)),(over_columns[0]-over_columns[3])/over_columns[0])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.title(\"Fraction of non-zero entries of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridO.plot_distribution(train_grid_snapshots,6,20,var_names,\"untrimmed\")\n",
    "gridO.plot_distribution(trimmed_keys,6,20,var_names,\"trimmed_both\")\n",
    "gridO.plot_distribution(trimmed_back_keys,6,20,var_names,\"trimmed_back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(pB_dict))\n",
    "#print(max([label for key, label in pB_dict.items()]))\n",
    "#print(min([label for key, label in pB_dict.items()]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label == 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label < 0.25]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.25]))\n",
    "\n",
    "def broken_hist(xs, bins, y_lower_1, y_upper_1, y_lower_2, y_upper_2, filename):\n",
    "    f, (ax, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "    # ax.hist([label for key, label in pB_dict.items()], 100)\n",
    "    # ax2.hist([label for key, label in pB_dict.items()], 100)\n",
    "    ax.hist(xs, bins)\n",
    "    ax2.hist(xs, bins)\n",
    "    ax.set_ylim(y_lower_2, y_upper_2)  # outliers only\n",
    "    ax2.set_ylim(y_lower_1, y_upper_1)  # most of the data\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "    ax2.xaxis.tick_bottom()\n",
    "\n",
    "    d = .015  # how big to make the diagonal lines in axes coordinates\n",
    "    # arguments to pass to plot, just so we don't keep repeating them\n",
    "    kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax.plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\n",
    "    ax.plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "    kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.hist(train_pBs, 10)\n",
    "plt.savefig(\"pB_untrimmed.png\")\n",
    "plt.show()\n",
    "\n",
    "broken_hist(train_pBs, 10, 0, 20000, 100000, 850000, \"pBs.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(trimmed_labels, 10)\n",
    "plt.savefig(\"trimmed_labels.png\")\n",
    "plt.show()\n",
    "    \n",
    "broken_hist(trimmed_back_labels, 10, 0, 8000, 10000, 400000, \"Hist_scaled_labels.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pBsl_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_norm_snapshots}, \n",
    "    {c.output_name_1: train_pBs, \n",
    "    c.output_name_2: train_norm_snapshots})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBsl_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_norm_snapshots}, \n",
    "    {c.output_name_1: val_pBs, \n",
    "    c.output_name_2: val_norm_snapshots})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "for method in methods_used:\n",
    "    plot_with_different_settings(\n",
    "        train_pBsl_ds, val_pBsl_ds, \n",
    "        method, \n",
    "        \"pBsLabels_{}_\".format(function_to_str(method)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pBl_pBbw_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: train_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: train_pBs, \n",
    "    c.output_name_2: train_reduced_normed_snapshots},\n",
    "    {c.output_name_1: train_pB_balanced_weights, \n",
    "    c.output_name_2: train_hc_balanced_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "val_pBl_pBbw_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    ({c.input_name: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pBs, \n",
    "    c.output_name_2: val_reduced_normed_snapshots}, \n",
    "    {c.output_name_1: val_pB_balanced_weights, \n",
    "    c.output_name_2: val_hc_balanced_weights})) \\\n",
    "        .shuffle(pipeline.snapshot_cnt) \\\n",
    "        .batch(c.batch_size)\n",
    "for method in methods_used:\n",
    "    plot_with_different_settings(\n",
    "        train_pBl_pBbw_ds, val_pBl_pBbw_ds, \n",
    "        method, \n",
    "        \"pBsAsLabels_pBbAsWeights_{}_\".format(function_to_str(method)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
