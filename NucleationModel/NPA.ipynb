{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the future functions actually necessary?\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import imp\n",
    "\n",
    "from globalConstants import Const\n",
    "from dataset import Dataset\n",
    "from pipeline import Pipeline\n",
    "from autoEncoder import AutoEncoder \n",
    "from corrector import Corrector\n",
    "from stepper import Stepper\n",
    "\n",
    "from plotter import *\n",
    "from data_read import *\n",
    "from helperFunctions import *\n",
    "from losses import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# allows for automatic reloading of imports and makes it unncessecary to restart the kernel\n",
    "# whenever a function is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetType = \"DW\" #\"DW\", \"ZP\", or \"MH\"\n",
    "assert dataSetType == \"DW\" or dataSetType == \"ZP\" or dataSetType == \"MH\",\\\n",
    "    \"dataSetType needs to be set to 'DW', 'ZP' or 'MH'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataSetType == \"DW\" or dataSetType == \"ZP\":\n",
    "    train_val_test_function = make_train_val_test_from_toy\n",
    "    get_paths_function=get_toy_paths\n",
    "elif dataSetType == \"MH\":\n",
    "    train_val_test_function = make_train_val_test_from_TIS_and_TPS\n",
    "    get_paths_function=get_TPS_and_TIS_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainData = pickle.load(\n",
    "        open(\"datasets/{}_trainData_{}.p\".format(\n",
    "            dataSetType, c.used_dataset_fraction), \"rb\"))\n",
    "    valData = pickle.load(\n",
    "        open(\"datasets/{}_valData_{}.p\".format(\n",
    "            dataSetType, c.used_dataset_fraction), \"rb\"))\n",
    "except Exception:\n",
    "    print(\"Processed dataset files not found.\"\n",
    "          +\"\\nGenerating datasets from raw data.\")\n",
    "    trainData, valData, _ = Dataset\\\n",
    "        .initialize_train_val_test_datasets(\n",
    "            *train_val_test_function(c))\n",
    "    print(\"Saving datasets for future use.\")\n",
    "    pickle.dump(\n",
    "        trainData,\n",
    "        open(\"datasets/{}_trainData_{}.p\".format(\n",
    "            dataSetType, c.used_dataset_fraction), \"wb\"))\n",
    "    pickle.dump(\n",
    "        valData,\n",
    "        open(\"datasets/{}_valData_{}.p\".format(\n",
    "            dataSetType, c.used_dataset_fraction), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_coverage(list_var_names, trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(c, trainData.snapshots)\n",
    "print(get_size(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots, pBs, g_snapshots = pipeline.prepare_dataset_pickle(trainData)\n",
    "pickle.dump(\n",
    "    (snapshots, pBs),\n",
    "    open(\"dumps/{}_train_datasets_tuple_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n",
    "\n",
    "snapshots, pBs, _ = pipeline.prepare_dataset_pickle(valData)\n",
    "pickle.dump(\n",
    "    (snapshots, pBs),\n",
    "    open(\"dumps/{}_val_datasets_tuple_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n",
    "\n",
    "print(\"1D\")\n",
    "train_corrected_1D = pipeline.get_1D_means(g_snapshots)\n",
    "print(\"2D\")\n",
    "\n",
    "pickle.dump(\n",
    "    train_corrected_1D,\n",
    "    open(\"dumps/{}_train_corrected_1D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n",
    "train_corrected_2D = pipeline.get_2D_means(g_snapshots)\n",
    "del g_snapshots\n",
    "pickle.dump(\n",
    "    train_corrected_2D,\n",
    "    open(\"dumps/{}_train_corrected_2D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_ds, train_corrected_1D, train_corrected_2D = \\\n",
    "    pipeline.prepare_prediction_plotter(trainData)\n",
    "\n",
    "val_ds, _, _ = \\\n",
    "    pipeline.prepare_prediction_plotter(valData)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots, pBs = \\\n",
    "        pickle.load(open(\"dumps/{}_train_datasets_tuple_{}_{}_{}.p\"\\\n",
    "                     .format(\n",
    "                        dataSetType,\n",
    "                        len(c.used_variable_names),\n",
    "                        c.used_dataset_fraction,\n",
    "                        c.resolution),\n",
    "                     \"rb\"))\n",
    "\n",
    "train_ds = pipeline.pack_tf_dataset(\n",
    "            snapshots=snapshots,\n",
    "            labels=pBs,\n",
    "            prediction_weights=np.ones(len(snapshots)),\n",
    "            reconstruction_weights=np.ones(len(snapshots)))\n",
    "\n",
    "snapshots, pBs = \\\n",
    "    pickle.load(open(\"dumps/{}_val_datasets_tuple_{}_{}_{}.p\"\\\n",
    "                     .format(\n",
    "                        dataSetType,\n",
    "                        len(c.used_variable_names),\n",
    "                        c.used_dataset_fraction,\n",
    "                        c.resolution),\n",
    "                     \"rb\"))\n",
    "val_ds = pipeline.pack_tf_dataset(\n",
    "            snapshots=snapshots,\n",
    "            labels=pBs,\n",
    "            prediction_weights=np.ones(len(snapshots)),\n",
    "            reconstruction_weights=np.ones(len(snapshots)))\n",
    "\n",
    "del snapshots\n",
    "del pBs\n",
    "\n",
    "train_corrected_1D = pickle.load(\n",
    "    open(\"dumps/{}_train_corrected_1D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution),\n",
    "         \"rb\"))\n",
    "train_corrected_2D = pickle.load(\n",
    "    open(\"dumps/{}_train_corrected_2D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution),\n",
    "         \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)\n",
    "c.epochs = 10\n",
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "    AutoEncoder.make_models(c)\n",
    "history = autoencoder.fit(\n",
    "    x=train_ds,\n",
    "    epochs=c.epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model_weights(\n",
    "    \"results/{}_model_weights_{}\"\\\n",
    "        .format(dataSetType, c.model_stamp),\n",
    "    autoencoder, autoencoder_1,\n",
    "    autoencoder_2, encoder, decoder_1, decoder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "        load_model_weights(\n",
    "            \"results/model_weights\", \n",
    "            *AutoEncoder.make_models(c))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minmax_container=pipeline,\n",
    "    representations=train_corrected_2D,\n",
    "    stamp=\"x1x2_Prediction_\" + c.model_stamp + c.data_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_relative_importance_plot(encoder, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_projected_path_plot(\n",
    "    pipeline=pipeline, steps=20, pre_stamp=dataSetType, model=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "make_super_map_plot(\n",
    "    method=calc_represented_map_generated,\n",
    "    pipeline=pipeline,\n",
    "    pre_stamp=\"CorrelatedMean_map\",\n",
    "    model=autoencoder_1,\n",
    "    minmax_container=pipeline,\n",
    "    representations=train_corrected_2D)\n",
    "#\"\"\"\n",
    "#\"\"\"\n",
    "make_super_scatter_plot(\n",
    "    method=calc_represented_scatter_generated,\n",
    "    pipeline=pipeline,\n",
    "    pre_stamp=\"CorrelatedMean_scatter\",\n",
    "    model=autoencoder_2,\n",
    "    minmax_container=pipeline,\n",
    "    representations=train_corrected_1D,\n",
    "    max_row_len=4)\n",
    "#\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_minimum, latent_maximum = \\\n",
    "    get_projected_minimum_and_maximum(pipeline, model=encoder, steps=20)\n",
    "\n",
    "plot_reconstruction_from_latent_space(\n",
    "    const=c,\n",
    "    latent_minimum=latent_minimum,\n",
    "    latent_maximum=latent_maximum,\n",
    "    steps=11,\n",
    "    recon_decoder=decoder_2,\n",
    "    pre_stamp=dataSetType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_projected_path_plot(\n",
    "    pipeline=pipeline, steps=20, pre_stamp=dataSetType + \"_comm\", model=autoencoder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_snapshots, train_labels, train_weights = \\\n",
    "    pipeline.prepare_groundTruth(\n",
    "        trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    (train_grid_snapshots, train_labels, train_weights),\n",
    "    open(\"dumps/{}_train_groundtruth_tuple_{}_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            c.used_dataset_fraction,\n",
    "            c.resolution,\n",
    "            c.outlier_cutoff),\n",
    "         \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)\n",
    "train_grid_snapshots, train_labels, train_weights = \\\n",
    "        pickle.load(open(\"dumps/{}_train_groundtruth_tuple_{}_{}_{}_{}.p\"\\\n",
    "                     .format(\n",
    "                        dataSetType,\n",
    "                        len(c.used_variable_names),\n",
    "                        c.used_dataset_fraction,\n",
    "                        c.resolution,\n",
    "                        c.outlier_cutoff),\n",
    "                     \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    stamp=\"{}_x1x2_GroundTruth_\".format(c.dataSetType)\\\n",
    "           + c.data_stamp)\n",
    "\n",
    "\"\"\"\n",
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given_configurational_density,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    weights=train_weights,\n",
    "    stamp=\"{}_x1x2_ConfDensity_\".format(c.dataSetType)\\\n",
    "          + c.data_stamp)\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_super_map_plot(\n",
    "    method=calc_map_given_configurational_density,\n",
    "    pipeline=pipeline,\n",
    "    pre_stamp=\"{}_ConfDensity_Train\".format(c.dataSetType),\n",
    "    grid_snapshots=train_grid_snapshots,\n",
    "    weights=train_weights)\n",
    "pass\n",
    "\n",
    "make_super_map_plot(\n",
    "    method=calc_map_given,\n",
    "    pipeline=pipeline,\n",
    "    pre_stamp=f\"{c.dataSetType}_GroundTruth_Train\",\n",
    "    grid_snapshots=train_grid_snapshots,\n",
    "    labels=train_labels,\n",
    "    weights=train_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    stamp=\"MCG_BigCage_GroundTruth_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_MCG_BigCage)\n",
    "\n",
    "plot_single_map(\n",
    "    x_int=6,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    stamp=\"NoW_BigCage_GroundTruth_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_now_BigCage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minmax_container=pipeline,\n",
    "    representations=train_corrected_2D,\n",
    "    stamp=\"MCG_BigCage_Train_\" + c.model_stamp + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_MCG_BigCage)\n",
    "\n",
    "plot_single_map(\n",
    "    x_int=6,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minmax_container=pipeline,\n",
    "    representations=train_corrected_2D,\n",
    "    stamp=\"NoW_BigCage_Train_\" + c.model_stamp + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_now_BigCage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_points, shooting_labels = read_shooting_points(\n",
    "    \"total_data_till_982mc_280K.txt\")\n",
    "\n",
    "shootingData = Dataset(\n",
    "    shooting_points,\n",
    "    shooting_labels,\n",
    "    np.ones(len(shooting_labels)),\n",
    "    flag=\"Shooting\")\n",
    "\n",
    "shoot_grid_snapshots, shoot_labels, shoot_weights = \\\n",
    "    pipeline.prepare_groundTruth(shootingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=shoot_grid_snapshots, \n",
    "    labels=shoot_labels, \n",
    "    weights=shoot_weights,\n",
    "    stamp=\"MCG_BigCage_Shooting_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_MCG_BigCage)\n",
    "\n",
    "plot_single_map(\n",
    "    x_int=6,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=shoot_grid_snapshots, \n",
    "    labels=shoot_labels, \n",
    "    weights=shoot_weights,\n",
    "    stamp=\"NoW_BigCage_Shooting_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_now_BigCage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_super_map_plot(\n",
    "    method=calc_map_given,\n",
    "    pipeline=pipeline,\n",
    "    pre_stamp=f\"{c.dataSetType}_GroundTruth_Shoot\",\n",
    "    grid_snapshots=shoot_grid_snapshots,\n",
    "    labels=shoot_labels,\n",
    "    weights=shoot_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_input_distribution(c, train_grid_snapshots, 5, pipeline)\n",
    "plot_histogram_with_broken_axes(\n",
    "    train_pBs, 10, 0, 500, 1000, 250000, \"results/pB_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_range_retained(outlier_cutoff):\n",
    "    snapshots = trainData.snapshots\n",
    "    span = np.amax(snapshots, axis=0) - np.amin(snapshots, axis=0)\n",
    "    percentile_span = np.percentile(snapshots, 100 - outlier_cutoff, axis=0) \\\n",
    "        - np.percentile(snapshots, outlier_cutoff, axis=0)\n",
    "    print(np.mean(percentile_span/span))\n",
    "    \n",
    "def estimate_reduction_on_AA_and_AB():\n",
    "    reducer = Reducer(\n",
    "        reduced_list_var_names,\n",
    "        c.name_to_list_position)\n",
    "    reduced_snapshots = reducer.reduce_snapshots(trainData.snapshots)\n",
    "    bounder = Bounder(reduced_snapshots, c.outlier_cutoff)\n",
    "    bound_snapshots = bounder.bound_snapshots(reduced_snapshots)\n",
    "\n",
    "    all_AA_frames = len([1 for i, label in enumerate(trainData.labels) if label == 0])\n",
    "    all_AB_frames = len([1 for i, label in enumerate(trainData.labels) if label == 1])\n",
    "    bound_AA_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 0 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "    bound_AB_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 1 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "\n",
    "    print(bound_AA_frames/all_AA_frames)\n",
    "    print(bound_AB_frames/all_AB_frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
