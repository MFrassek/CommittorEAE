{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are the future functions actually necessary?\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import imp\n",
    "\n",
    "from globalConstants import Const\n",
    "from dataset import Dataset\n",
    "from pipeline import Pipeline\n",
    "from autoEncoder import AutoEncoder \n",
    "from corrector import Corrector\n",
    "from stepper import Stepper\n",
    "\n",
    "from plotter import *\n",
    "from data_read import *\n",
    "from helperFunctions import *\n",
    "from losses import *\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import Counter\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "# allows for automatic reloading of imports and makes it unncessecary to restart the kernel\n",
    "# whenever a function is changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetType = \"DW\" #\"DW\", \"ZP\", or \"MH\"\n",
    "assert dataSetType == \"DW\" or dataSetType == \"ZP\" or dataSetType == \"MH\",\\\n",
    "    \"dataSetType needs to be set to 'DW', 'ZP' or 'MH'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataSetType == \"DW\" or dataSetType == \"ZP\":\n",
    "    dataset_frac = c.used_toy_frac\n",
    "    train_val_test_function = make_train_val_test_from_toy\n",
    "    get_paths_function=get_toy_paths\n",
    "elif dataSetType == \"MH\":\n",
    "    dataset_frac = c.used_TIS_frac\n",
    "    train_val_test_function = make_train_val_test_from_TIS_and_TPS\n",
    "    get_paths_function=get_TPS_and_TIS_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    trainData = pickle.load(\n",
    "        open(\"datasets/{}_trainData_{}.p\".format(\n",
    "            dataSetType, dataset_frac), \"rb\"))\n",
    "    valData = pickle.load(\n",
    "        open(\"datasets/{}_valData_{}.p\".format(\n",
    "            dataSetType, dataset_frac), \"rb\"))\n",
    "except Exception:\n",
    "    print(\"Processed dataset files not found.\"\n",
    "          +\"\\nGenerating datasets from raw data.\")\n",
    "    trainData, valData, _ = Dataset\\\n",
    "        .initialize_train_val_test_datasets(\n",
    "            *train_val_test_function(c))\n",
    "    print(\"Saving datasets for future use.\")\n",
    "    pickle.dump(\n",
    "        trainData,\n",
    "        open(\"datasets/{}_trainData_{}.p\".format(\n",
    "            dataSetType, dataset_frac), \"wb\"))\n",
    "    pickle.dump(\n",
    "        valData,\n",
    "        open(\"datasets/{}_valData_{}.p\".format(\n",
    "            dataSetType, dataset_frac), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_coverage(list_var_names, trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(c, trainData.snapshots)\n",
    "print(get_size(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "c = Const(dataSetType)\n",
    "c.bottleneck_size = 1\n",
    "Stepper.iter_top_down(\n",
    "    pipeline=pipeline,\n",
    "    train_dataset=trainData,\n",
    "    val_dataset=valData,\n",
    "    used=reduced_list_var_names,\n",
    "    param_limit=1,\n",
    "    epochs=1,\n",
    "    repetitions=1,\n",
    "    const=c)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snapshots, pBs, hcb_weights, minima, maxima, g_snapshots = \\\n",
    "    pipeline.prepare_dataset_pickle(\n",
    "        c.used_variable_names, trainData)\n",
    "pickle.dump(\n",
    "    (snapshots, pBs, hcb_weights, minima, maxima),\n",
    "    open(\"dumps/{}_train_datasets_tuple_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n",
    "\n",
    "snapshots, pBs, hcb_weights, minima, maxima, _ = \\\n",
    "    pipeline.prepare_dataset_pickle(\n",
    "        c.used_variable_names, valData)\n",
    "pickle.dump(\n",
    "    (snapshots, pBs, hcb_weights, minima, maxima),\n",
    "    open(\"dumps/{}_val_datasets_tuple_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n",
    "\n",
    "print(\"1D\")\n",
    "train_corrected_1D = pipeline.get_1D_means(g_snapshots)\n",
    "print(\"2D\")\n",
    "\n",
    "pickle.dump(\n",
    "    train_corrected_1D,\n",
    "    open(\"dumps/{}_train_corrected_1D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n",
    "train_corrected_2D = pipeline.get_2D_means(g_snapshots)\n",
    "del g_snapshots\n",
    "pickle.dump(\n",
    "    train_corrected_2D,\n",
    "    open(\"dumps/{}_train_corrected_2D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution),\n",
    "         \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_ds, train_bnrn_minima, train_bnrn_maxima, \\\n",
    "    train_corrected_1D, train_corrected_2D = \\\n",
    "    pipeline.prepare_prediction_plotter(\n",
    "        c.used_variable_names,\n",
    "        trainData)\n",
    "\n",
    "val_ds, _, _, _, _ = \\\n",
    "    pipeline.prepare_prediction_plotter(\n",
    "        c.used_variable_names,\n",
    "        valData)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots, pBs, hcb_weights, \\\n",
    "    train_bnrn_minima, train_bnrn_maxima = \\\n",
    "        pickle.load(open(\"dumps/{}_train_datasets_tuple_{}_{}_{}.p\"\\\n",
    "                     .format(\n",
    "                        dataSetType,\n",
    "                        len(c.used_variable_names),\n",
    "                        dataset_frac,\n",
    "                        c.resolution),\n",
    "                     \"rb\"))\n",
    "\n",
    "train_ds = pipeline.pack_tf_dataset(\n",
    "            snapshots=snapshots,\n",
    "            labels=pBs,\n",
    "            prediction_weights=np.ones(len(snapshots)),\n",
    "            reconstruction_weights=np.ones(len(snapshots)))\n",
    "#            prediction_weights=hcb_weights,\n",
    "#            reconstruction_weights=hcb_weights)\n",
    "\n",
    "snapshots, pBs, hcb_weights, minima, maxima = \\\n",
    "    pickle.load(open(\"dumps/{}_val_datasets_tuple_{}_{}_{}.p\"\\\n",
    "                     .format(\n",
    "                        dataSetType,\n",
    "                        len(c.used_variable_names),\n",
    "                        dataset_frac,\n",
    "                        c.resolution),\n",
    "                     \"rb\"))\n",
    "val_ds = pipeline.pack_tf_dataset(\n",
    "            snapshots=snapshots,\n",
    "            labels=pBs,\n",
    "            prediction_weights=np.ones(len(snapshots)),\n",
    "            reconstruction_weights=np.ones(len(snapshots)))\n",
    "#            prediction_weights=hcb_weights,\n",
    "#            reconstruction_weights=hcb_weights)\n",
    "\n",
    "del snapshots\n",
    "del pBs\n",
    "del hcb_weights\n",
    "\n",
    "train_corrected_1D = pickle.load(\n",
    "    open(\"dumps/{}_train_corrected_1D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution),\n",
    "         \"rb\"))\n",
    "train_corrected_2D = pickle.load(\n",
    "    open(\"dumps/{}_train_corrected_2D_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution),\n",
    "         \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)\n",
    "c.bottleneck_size = 1\n",
    "c.epochs = 10\n",
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "    AutoEncoder.make_models(\n",
    "        len(c.used_variable_names),\n",
    "        c)\n",
    "history = autoencoder.fit(\n",
    "    x=train_ds,\n",
    "    epochs=c.epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_model_weights(\n",
    "    \"results/{}_model_weights_{}\"\\\n",
    "        .format(dataSetType, c.model_stamp),\n",
    "    autoencoder, autoencoder_1,\n",
    "    autoencoder_2, encoder, decoder_1, decoder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c.bottleneck_size = 1\n",
    "autoencoder, autoencoder_1, autoencoder_2, \\\n",
    "    encoder, decoder_1, decoder_2 = \\\n",
    "        load_model_weights(\n",
    "            \"results/model_weights\", \n",
    "            *AutoEncoder.make_models(\n",
    "                len(c.used_variable_names),\n",
    "                c))\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    representations=train_corrected_2D,\n",
    "    stamp=\"x1x2_Prediction_\" + c.model_stamp + c.data_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for component in get_relative_encoder_importances(encoder, c.used_variable_names):\n",
    "    print(\"{:8s}\\t{}\".format(component[0],\n",
    "        \"\\t\".join(list(map(lambda x: str(round(x, 4)),component[1:])))))\n",
    "\n",
    "plot_relative_importances(\n",
    "    *list(zip(*get_relative_encoder_importances(encoder, c.used_variable_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_minimum, latent_maximum = \\\n",
    "    plot_projected_example_paths(\n",
    "        get_paths_function=get_paths_function,\n",
    "        const=c,\n",
    "        pipeline=pipeline,\n",
    "        steps=20,\n",
    "        pre_stamp=dataSetType,\n",
    "        model=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "plot_super_map(\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMean_map\",\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1,\n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    representations=train_corrected_2D)\n",
    "#\"\"\"\n",
    "#\"\"\"\n",
    "plot_super_scatter(\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    pre_stamp=\"CorrelatedMean_scatter\",\n",
    "    model=autoencoder_2,\n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    method=calc_represented_scatter_generated,\n",
    "    representations=train_corrected_1D,\n",
    "    max_row_len=4)\n",
    "#\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction_from_latent_space(\n",
    "    reduced_list_var_names=c.used_variable_names,\n",
    "    latent_minimum=latent_minimum,\n",
    "    latent_maximum=latent_maximum,\n",
    "    steps=11,\n",
    "    recon_decoder=decoder_2,\n",
    "    pre_stamp=dataSetType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = plot_projected_example_paths(\n",
    "    get_paths_function=get_paths_function,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    steps=20,\n",
    "    pre_stamp=dataSetType + \"_Committor\",\n",
    "    model=autoencoder_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_snapshots, train_labels, train_weights = \\\n",
    "    pipeline.prepare_groundTruth(\n",
    "        c.used_variable_names,\n",
    "        trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(\n",
    "    (train_grid_snapshots, train_labels, train_weights),\n",
    "    open(\"dumps/{}_train_groundtruth_tuple_{}_{}_{}_{}.p\"\\\n",
    "         .format(\n",
    "            dataSetType,\n",
    "            len(c.used_variable_names),\n",
    "            dataset_frac,\n",
    "            c.resolution,\n",
    "            c.outlier_cutoff),\n",
    "         \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Const(dataSetType)\n",
    "train_grid_snapshots, train_labels, train_weights = \\\n",
    "        pickle.load(open(\"dumps/{}_train_groundtruth_tuple_{}_{}_{}_{}.p\"\\\n",
    "                     .format(\n",
    "                        dataSetType,\n",
    "                        len(c.used_variable_names),\n",
    "                        dataset_frac,\n",
    "                        c.resolution,\n",
    "                        c.outlier_cutoff),\n",
    "                     \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    stamp=\"{}_x1x2_GroundTruth_\".format(c.dataSetType)\\\n",
    "           + c.data_stamp)\n",
    "\n",
    "\"\"\"\n",
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given_configurational_density,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    weights=train_weights,\n",
    "    stamp=\"{}_x1x2_ConfDensity_\".format(c.dataSetType)\\\n",
    "          + c.data_stamp)\"\"\"\n",
    "\"\"\"\n",
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given_configurational_density,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    weights=train_weights,\n",
    "    stamp=\"{}_x1x2_ConfDensity_Overlay\".format(c.dataSetType)\\\n",
    "          + c.data_stamp,\n",
    "    PES_function=inject_PES)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_super_map(\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    pre_stamp=\"{}_ConfDensity_Train\".format(c.dataSetType),\n",
    "    method=calc_map_given_configurational_density,\n",
    "    grid_snapshots=train_grid_snapshots,\n",
    "    weights=train_weights)\n",
    "pass\n",
    "\n",
    "plot_ground_truth(\n",
    "    pipeline=pipeline,\n",
    "    const=c,\n",
    "    grid_snapshots=train_grid_snapshots,\n",
    "    labels=train_labels,\n",
    "    weights=train_weights,\n",
    "    pre_stamp=\"{}_GroundTruth_Train\".format(c.dataSetType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_encoder_decoder(\n",
    "    const=c,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    pipeline=pipeline)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    stamp=\"MCG_BigCage_GroundTruth_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_MCG_BigCage)\n",
    "\n",
    "plot_single_map(\n",
    "    x_int=6,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=train_grid_snapshots, \n",
    "    labels=train_labels, \n",
    "    weights=train_weights,\n",
    "    stamp=\"NoW_BigCage_GroundTruth_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_now_BigCage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    representations=train_corrected_2D,\n",
    "    stamp=\"MCG_BigCage_Train_\" + c.model_stamp + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_MCG_BigCage)\n",
    "\n",
    "plot_single_map(\n",
    "    x_int=6,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_represented_map_generated,\n",
    "    model=autoencoder_1, \n",
    "    minima=train_bnrn_minima,\n",
    "    maxima=train_bnrn_maxima,\n",
    "    representations=train_corrected_2D,\n",
    "    stamp=\"NoW_BigCage_Train_\" + c.model_stamp + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_now_BigCage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_points, shooting_labels = read_shooting_points(\n",
    "    \"total_data_till_982mc_280K.txt\")\n",
    "\n",
    "shootingData = Dataset(\n",
    "    shooting_points,\n",
    "    shooting_labels,\n",
    "    np.ones(len(shooting_labels)),\n",
    "    flag=\"Shooting\")\n",
    "\n",
    "shoot_grid_snapshots, shoot_labels, shoot_weights = \\\n",
    "    pipeline.prepare_groundTruth(\n",
    "        c.used_variable_names,\n",
    "        shootingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_map(\n",
    "    x_int=0,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=shoot_grid_snapshots, \n",
    "    labels=shoot_labels, \n",
    "    weights=shoot_weights,\n",
    "    stamp=\"MCG_BigCage_Shooting_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_MCG_BigCage)\n",
    "\n",
    "plot_single_map(\n",
    "    x_int=6,\n",
    "    y_int=1,\n",
    "    const=c,\n",
    "    pipeline=pipeline,\n",
    "    method=calc_map_given,\n",
    "    grid_snapshots=shoot_grid_snapshots, \n",
    "    labels=shoot_labels, \n",
    "    weights=shoot_weights,\n",
    "    stamp=\"NoW_BigCage_Shooting_\" + c.data_stamp,\n",
    "    line_function=inject_dividing_line,\n",
    "    line_formula=calculate_slope_now_BigCage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage_of_range_retained(outlier_cutoff):\n",
    "    snapshots = trainData.snapshots\n",
    "    span = np.amax(snapshots, axis=0) - np.amin(snapshots, axis=0)\n",
    "    percentile_span = np.percentile(snapshots, 100 - outlier_cutoff, axis=0) \\\n",
    "        - np.percentile(snapshots, outlier_cutoff, axis=0)\n",
    "    print(np.mean(percentile_span/span))\n",
    "    \n",
    "def estimate_reduction_on_AA_and_AB():\n",
    "    reducer = Reducer(\n",
    "        reduced_list_var_names,\n",
    "        c.name_to_list_position)\n",
    "    reduced_snapshots = reducer.reduce_snapshots(trainData.snapshots)\n",
    "    bounder = Bounder(reduced_snapshots, c.outlier_cutoff)\n",
    "    bound_snapshots = bounder.bound_snapshots(reduced_snapshots)\n",
    "\n",
    "    all_AA_frames = len([1 for i, label in enumerate(trainData.labels) if label == 0])\n",
    "    all_AB_frames = len([1 for i, label in enumerate(trainData.labels) if label == 1])\n",
    "    bound_AA_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 0 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "    bound_AB_frames = len([1 for i, label in enumerate(trainData.labels) if (label == 1 and bound_snapshots[i][0] == bounder.upper_bound[0])])\n",
    "\n",
    "    print(bound_AA_frames/all_AA_frames)\n",
    "    print(bound_AB_frames/all_AB_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder\n",
    "print(model.name)\n",
    "print(model.input_names)\n",
    "print(model.output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grid_columns = np.transpose(train_grid_snapshots)\n",
    "def rec_cols(used, unused, lim, p):\n",
    "    if len(used) == lim:\n",
    "        p.append(used)\n",
    "        return\n",
    "    else:\n",
    "        for i in range(len(unused)):\n",
    "            rec_cols(used+[unused[i]], unused[i+1:], lim, p)\n",
    "\n",
    "def mean_pB_attributes(dimensions, train_grid_columns, dataO, tries):\n",
    "    p = []\n",
    "    rec_cols([],list(range(22)),dimensions, p)\n",
    "    pB_uniques = []\n",
    "    pB_unique_means = []\n",
    "    pB_means = []\n",
    "    pB_unique_zeroes = []\n",
    "    for i in range(tries):\n",
    "        choice = random.choice(p)\n",
    "        short_grid_snapshots = []\n",
    "        for j in choice:\n",
    "            short_grid_snapshots.append(train_grid_columns[j])\n",
    "\n",
    "        pB_dict, pBs = gridO.approximate_pB(np.transpose(short_grid_snapshots), dataO.train_labels, dataO.train_weights)\n",
    "        pB_uniques.append(len(pB_dict)/len(pBs))\n",
    "        pB_unique_means.append(np.mean([label for key, label in pB_dict.items()]))\n",
    "        pB_means.append(np.mean(pBs))\n",
    "        pB_unique_zeroes.append(len([label for key, label in pB_dict.items() if label == 0])/len(pBs))\n",
    "    return np.mean(pB_uniques), np.mean(pB_unique_means), np.mean(pB_means), np.mean(pB_unique_zeroes)\n",
    "over_list = []\n",
    "for i in range(1,23):\n",
    "    print(i)\n",
    "    over_list.append(list(mean_pB_attributes(i, train_grid_columns, dataO, 5)))\n",
    "\n",
    "print(over_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_list = [[1.4504122192394765e-05, 0.4575422359289151, 0.35037916491845283, 0.0], \n",
    "             [8.823341000373481e-05, 0.31197805032254855, 0.24989161958688205, 1.4987592932141258e-05], \n",
    "             [0.0011390570628427355, 0.254336987822713, 0.21065742919045424, 0.00041312574711337754], \n",
    "             [0.0031104090041590574, 0.18133452313340712, 0.2170178744045542, 0.0019471784043289969], \n",
    "             [0.019666622751407806, 0.14815356079849162, 0.1799223783174298, 0.016739932628352418], \n",
    "             [0.03723860852286394, 0.0766339470321534, 0.15529480428303702, 0.03335392112900087], \n",
    "             [0.10585688540897394, 0.04434703300207507, 0.12188629749425983, 0.10030736652279384], \n",
    "             [0.14584233293970758, 0.03858060089555661, 0.12155443609723238, 0.13996139486143125], \n",
    "             [0.2389070660457291, 0.030955704907381622, 0.08891275517831804, 0.2316479946238054], \n",
    "             [0.3461106592002669, 0.02109568034865818, 0.07299244764109925, 0.33821316466650786], \n",
    "             [0.40314715278037977, 0.023028707168210764, 0.06465460885978427, 0.3932804819236334], \n",
    "             [0.5168345720256579, 0.020024200833715332, 0.055430137658731636, 0.5062564739753744], \n",
    "             [0.6097532969682758, 0.01770556773880911, 0.04620075248938048, 0.5986738397608754], \n",
    "             [0.649903849756633, 0.019642215099454723, 0.040862941059977745, 0.6368426459870116], \n",
    "             [0.648240710411905, 0.018966941544334908, 0.04145155090339449, 0.6356276840180286], \n",
    "             [0.694312812820677, 0.019123793821338247, 0.037413562972544265, 0.6808039151460504], \n",
    "             [0.7417976167309884, 0.0178504232145612, 0.03518109760382279, 0.7283716342882285], \n",
    "             [0.7338169652299931, 0.018954433483644333, 0.035358386024052696, 0.7197073551612314], \n",
    "             [0.7123308003495493, 0.01780718726971691, 0.03705239598119732, 0.6994197142446192], \n",
    "             [0.7675257538819679, 0.01880599290810691, 0.03314824301337975, 0.7529500780200907], \n",
    "             [0.7685473275550522, 0.02063167015576706, 0.032021440958897376, 0.752516404766538], \n",
    "             [0.7835114721563158, 0.021141123648464055, 0.031053932448914478, 0.766812392805472]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[1])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "over_columns = np.transpose(over_list)\n",
    "plt.scatter(list(range(1,23)),over_columns[2])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.title(\"Mean pB value of all entries\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "            \n",
    "plt.figure()\n",
    "plt.scatter(list(range(1,23)),over_columns[0], label = \"Unique entries\")\n",
    "plt.scatter(list(range(1,23)),over_columns[3], label = \"Unique entries = 0\")\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.ylim(-0.1,1)\n",
    "plt.xlim(0,23)\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.title(\"Fraction of unique entries of all entries\")\n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(list(range(1,23)),(over_columns[0]-over_columns[3])/over_columns[0])\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.xlim(0,23)\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.title(\"Fraction of non-zero entries of all unique entries\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridO.plot_distribution(train_grid_snapshots,6,20,var_names,\"untrimmed\")\n",
    "gridO.plot_distribution(trimmed_keys,6,20,var_names,\"trimmed_both\")\n",
    "gridO.plot_distribution(trimmed_back_keys,6,20,var_names,\"trimmed_back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(len(pB_dict))\n",
    "#print(max([label for key, label in pB_dict.items()]))\n",
    "#print(min([label for key, label in pB_dict.items()]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label == 0.0]))\n",
    "#print(len([label for key, label in pB_dict.items() if label < 0.25]))\n",
    "#print(len([label for key, label in pB_dict.items() if label > 0.25]))\n",
    "\n",
    "def broken_hist(xs, bins, y_lower_1, y_upper_1, y_lower_2, y_upper_2, filename):\n",
    "    f, (ax, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "    # ax.hist([label for key, label in pB_dict.items()], 100)\n",
    "    # ax2.hist([label for key, label in pB_dict.items()], 100)\n",
    "    ax.hist(xs, bins)\n",
    "    ax2.hist(xs, bins)\n",
    "    ax.set_ylim(y_lower_2, y_upper_2)  # outliers only\n",
    "    ax2.set_ylim(y_lower_1, y_upper_1)  # most of the data\n",
    "    # hide the spines between ax and ax2\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.tick_params(labeltop=False)  # don't put tick labels at the top\n",
    "    ax2.xaxis.tick_bottom()\n",
    "\n",
    "    d = .015  # how big to make the diagonal lines in axes coordinates\n",
    "    # arguments to pass to plot, just so we don't keep repeating them\n",
    "    kwargs = dict(transform=ax.transAxes, color='k', clip_on=False)\n",
    "    ax.plot((-d, +d), (-d, +d), **kwargs)        # top-left diagonal\n",
    "    ax.plot((1 - d, 1 + d), (-d, +d), **kwargs)  # top-right diagonal\n",
    "\n",
    "    kwargs.update(transform=ax2.transAxes)  # switch to the bottom axes\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)  # bottom-left diagonal\n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)  # bottom-right diagonal\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.hist(train_pBs, 10)\n",
    "plt.savefig(\"pB_untrimmed.png\")\n",
    "plt.show()\n",
    "\n",
    "broken_hist(train_pBs, 10, 0, 20000, 100000, 850000, \"pBs.png\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(trimmed_labels, 10)\n",
    "plt.savefig(\"trimmed_labels.png\")\n",
    "plt.show()\n",
    "    \n",
    "broken_hist(trimmed_back_labels, 10, 0, 8000, 10000, 400000, \"Hist_scaled_labels.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
